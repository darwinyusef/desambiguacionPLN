{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 120,
   "id": "90259aa6",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[nltk_data] Downloading package senseval to C:\\Users\\Aqui\n",
      "[nltk_data]     Creamos\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]   Package senseval is already up-to-date!\n",
      "[nltk_data] Downloading package punkt to C:\\Users\\Aqui\n",
      "[nltk_data]     Creamos\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]   Package punkt is already up-to-date!\n",
      "[nltk_data] Downloading package stopwords to C:\\Users\\Aqui\n",
      "[nltk_data]     Creamos\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]   Package stopwords is already up-to-date!\n",
      "[nltk_data] Downloading package wordnet to C:\\Users\\Aqui\n",
      "[nltk_data]     Creamos\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]   Package wordnet is already up-to-date!\n",
      "[nltk_data] Downloading package averaged_perceptron_tagger to\n",
      "[nltk_data]     C:\\Users\\Aqui Creamos\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]   Package averaged_perceptron_tagger is already up-to-\n",
      "[nltk_data]       date!\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "True"
      ]
     },
     "execution_count": 120,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# 游뱁 Iniciamos con la configuraci칩n y extracci칩n de las librer칤as senseval, punk stopwords wordnet,\n",
    "import os\n",
    "import nltk\n",
    "from nltk.corpus import senseval\n",
    "from nltk.corpus import stopwords\n",
    "from nltk.probability import FreqDist\n",
    "import string\n",
    "from collections import Counter\n",
    "import re\n",
    "import string\n",
    "\n",
    "nltk.download('senseval') # Para el corpus Senseval 2\n",
    "nltk.download('punkt')    # Para tokenizaci칩n\n",
    "nltk.download('stopwords') # Para palabras vac칤as (stop words)\n",
    "nltk.download('wordnet')  # Aunque no se use directamente para los sentidos, es 칰til para entender WordNet.\n",
    "nltk.download('averaged_perceptron_tagger') # Para etiquetado POS si se necesita, aunque Senseval ya viene con ello."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 121,
   "id": "c4bccbf3",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['hard.pos', 'interest.pos', 'line.pos', 'serve.pos']"
      ]
     },
     "execution_count": 121,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Listamos todas las instancias\n",
    "senseval.fileids()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 122,
   "id": "f093af6c",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Total de instancias con sentidos: 4333\n"
     ]
    }
   ],
   "source": [
    "hard_instances = senseval.instances('hard.pos')\n",
    "hard_instances = [instance for instance in hard_instances if instance.senses]  # Filtrar instancias con sentidos\n",
    "print(f\"Total de instancias con sentidos: {len(hard_instances)}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 51,
   "id": "5afa72ae",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Archivo 'corpus_etiquetado.txt' cargado exitosamente.\n"
     ]
    }
   ],
   "source": [
    "file_path = 'corpus_etiquetado.txt'\n",
    "\n",
    "try:\n",
    "    with open(file_path, 'r', encoding='utf-8') as f:\n",
    "        corpus_content = f.read()\n",
    "    print(f\"Archivo '{file_path}' cargado exitosamente.\")\n",
    "except FileNotFoundError:\n",
    "    print(f\"Error: El archivo '{file_path}' no se encontr칩. Aseg칰rate de que est치 en la misma carpeta que tu script.\")\n",
    "except Exception as e:\n",
    "    print(f\"Ocurri칩 un error al leer el archivo: {e}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4dd18ee8",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "## Procesa el contenido del corpus para extraer pares (palabra, etiqueta).\n",
    "## Maneja etiquetas de documento, l칤neas vac칤as y el formato espec칤fico.\n",
    "def parse_corpus_content(content):\n",
    "    corpus = []\n",
    "    \n",
    "    for linea in content.splitlines():\n",
    "        linea = linea.strip()\n",
    "        \n",
    "        # Saltamos etiquetas de documento y l칤neas vac칤as.\n",
    "        if linea.startswith(\"<doc\") or linea.startswith(\"</doc>\") or linea == \"\":\n",
    "            continue\n",
    "        \n",
    "        # Saltar 'Fp' si aparece en una l칤nea separada (inconsistencia del corpus).\n",
    "        if linea == \"Fp\":\n",
    "            continue\n",
    "        \n",
    "        datos = linea.split(\"\\t\")\n",
    "        \n",
    "        # Formato esperado: palabra \\t lema \\t etiqueta\n",
    "        if len(datos) == 3:\n",
    "            word = datos[0]\n",
    "            tag = datos[2]\n",
    "            corpus.append((word, tag))\n",
    "        # Caso espec칤fico: . \\t . \\t Fp\n",
    "        elif len(datos) == 2 and datos[0] == '.' and datos[1] == 'Fp':\n",
    "            word = datos[0]\n",
    "            tag = datos[1] \n",
    "            corpus.append((word, tag))\n",
    "        # Las l칤neas que no encajan se omiten.\n",
    "        else:\n",
    "            pass \n",
    "\n",
    "    return corpus\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 109,
   "id": "7f3718ab",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[('Habla', 'VMIP3S0'),\n",
       " ('con', 'SP'),\n",
       " ('el', 'DA'),\n",
       " ('enfermo', 'NCMS000'),\n",
       " ('grave', 'AQ0CS00'),\n",
       " ('de', 'SP'),\n",
       " ('trasplantes', 'NCMN000'),\n",
       " ('.', 'Fp'),\n",
       " ('El', 'DA'),\n",
       " ('enfermo', 'NCMS000'),\n",
       " ('grave', 'AQ0CS00'),\n",
       " ('habla', 'VMIP3S0'),\n",
       " ('de', 'SP'),\n",
       " ('trasplantes', 'NCMN000'),\n",
       " ('.', 'Fp'),\n",
       " ('La', 'DA'),\n",
       " ('pel칤cula', 'NCFS000'),\n",
       " ('fue', 'VAIP3S0'),\n",
       " ('nominada', 'VMP00SF'),\n",
       " ('al', 'SP+DA'),\n",
       " ('Oscar', 'NP00000'),\n",
       " ('.', 'Fp'),\n",
       " ('Luis', 'NP00000'),\n",
       " ('Bu침uel', 'NP00000'),\n",
       " ('es', 'VSIP3S0'),\n",
       " ('director', 'NCMS000'),\n",
       " ('espa침ol', 'AQ0MS00'),\n",
       " ('.', 'Fp'),\n",
       " ('El', 'DA'),\n",
       " ('ni침o', 'NCMS000'),\n",
       " ('corre', 'VMIP3S0'),\n",
       " ('en', 'SP'),\n",
       " ('el', 'DA'),\n",
       " ('parque', 'NCMS000'),\n",
       " ('.', 'Fp'),\n",
       " ('Una', 'DI'),\n",
       " ('flor', 'NCFS000'),\n",
       " ('roja', 'AQ0FS00'),\n",
       " ('decoraba', 'VIIIS0S'),\n",
       " ('la', 'DA'),\n",
       " ('mesa', 'NCFS000'),\n",
       " ('.', 'Fp'),\n",
       " ('Ella', 'PP3FS000'),\n",
       " ('canta', 'VMIP3S0'),\n",
       " ('una', 'DI'),\n",
       " ('canci칩n', 'NCFS000'),\n",
       " ('alegre', 'AQ0CS00'),\n",
       " ('.', 'Fp'),\n",
       " ('Ellos', 'PP3MP000'),\n",
       " ('juegan', 'VMIP3P0'),\n",
       " ('f칰tbol', 'NCMS000'),\n",
       " ('cada', 'DI'),\n",
       " ('d칤a', 'NCMS000'),\n",
       " ('.', 'Fp'),\n",
       " ('Hoy', 'NCMS000'),\n",
       " ('es', 'VSIP3S0'),\n",
       " ('un', 'DI'),\n",
       " ('d칤a', 'NCMS000'),\n",
       " ('especial', 'AQ0CS00'),\n",
       " ('.', 'Fp'),\n",
       " ('El', 'DA'),\n",
       " ('sol', 'NCMS000'),\n",
       " ('brilla', 'VMIP3S0'),\n",
       " ('intensamente', 'RG'),\n",
       " ('en', 'SP'),\n",
       " ('el', 'DA'),\n",
       " ('cielo', 'NCMS000'),\n",
       " ('.', 'Fp'),\n",
       " ('Nosotros', 'PP1MP000'),\n",
       " ('estudiamos', 'VMIP1P0'),\n",
       " ('procesamiento', 'NCMS000'),\n",
       " ('del', 'SP+DA'),\n",
       " ('lenguaje', 'NCMS000'),\n",
       " ('natural', 'AQ0CS00'),\n",
       " ('.', 'Fp'),\n",
       " ('Un', 'DI'),\n",
       " ('gato', 'NCMS000'),\n",
       " ('gris', 'AQ0CS00'),\n",
       " ('miraba', 'VIIIS3S'),\n",
       " ('por', 'SP'),\n",
       " ('la', 'DA'),\n",
       " ('ventana', 'NCFS000'),\n",
       " ('.', 'Fp'),\n",
       " ('El', 'DA'),\n",
       " ('profesor', 'NCMS000'),\n",
       " ('explic칩', 'VMIS3S0'),\n",
       " ('la', 'DA'),\n",
       " ('tarea', 'NCFS000'),\n",
       " ('.', 'Fp'),\n",
       " ('Mar칤a', 'NP00000'),\n",
       " ('cocinaba', 'VIIIS3S'),\n",
       " ('un', 'DI'),\n",
       " ('pastel', 'NCMS000'),\n",
       " ('de', 'SP'),\n",
       " ('chocolate', 'NCMS000'),\n",
       " ('.', 'Fp'),\n",
       " ('Ayer', 'RG'),\n",
       " ('llovi칩', 'VMIS3S0'),\n",
       " ('fuertemente', 'RG'),\n",
       " ('en', 'SP'),\n",
       " ('la', 'DA'),\n",
       " ('ciudad', 'NCFS000'),\n",
       " ('.', 'Fp'),\n",
       " ('Ellas', 'PP3FP000'),\n",
       " ('estudian', 'VMIP3P0'),\n",
       " ('arte', 'NCMS000'),\n",
       " ('en', 'SP'),\n",
       " ('la', 'DA'),\n",
       " ('universidad', 'NCFS000'),\n",
       " ('.', 'Fp'),\n",
       " ('Un', 'DI'),\n",
       " ('coche', 'NCMS000'),\n",
       " ('rojo', 'AQ0MS00'),\n",
       " ('avanza', 'VMIP3S0'),\n",
       " ('r치pido', 'RG'),\n",
       " ('por', 'SP'),\n",
       " ('la', 'DA'),\n",
       " ('carretera', 'NCFS000'),\n",
       " ('.', 'Fp'),\n",
       " ('Ella', 'PP3FS000'),\n",
       " ('pinta', 'VMIP3S0'),\n",
       " ('cuadros', 'NCMP000'),\n",
       " ('hermosos', 'AQ0MP00'),\n",
       " ('.', 'Fp'),\n",
       " ('Ellos', 'PP3MP000'),\n",
       " ('viajan', 'VMIP3P0'),\n",
       " ('a', 'SP'),\n",
       " ('diferentes', 'AQ0MP00'),\n",
       " ('pa칤ses', 'NCMP000'),\n",
       " ('.', 'Fp'),\n",
       " ('La', 'DA'),\n",
       " ('tarde', 'NCFS000'),\n",
       " ('es', 'VSIP3S0'),\n",
       " ('tranquila', 'AQ0FS00'),\n",
       " ('y', 'CC'),\n",
       " ('agradable', 'AQ0CS00'),\n",
       " ('.', 'Fp')]"
      ]
     },
     "execution_count": 109,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "parse_corpus_content(corpus_content) # procesamos el contenido del corpus de manera organizada\n",
    "# Procesamos el contenido del corpus para extraer pares (palabra, etiqueta)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 110,
   "id": "9ba99169",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Corpus cargado. N칰mero de tokens: 137\n",
      "Primeros 5 registros: [('Habla', 'VMIP3S0'), ('con', 'SP'), ('el', 'DA'), ('enfermo', 'NCMS000'), ('grave', 'AQ0CS00')]\n"
     ]
    }
   ],
   "source": [
    "corpus = parse_corpus_content(corpus_content)\n",
    "print(\"Corpus cargado. N칰mero de tokens:\", len(corpus))\n",
    "print(\"Primeros 5 registros:\", corpus[:5])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 111,
   "id": "44f75ccd",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Cargando instancias de 'hard' desde Senseval...\n",
      "Total Senseval 'hard' instances with senses: 4333\n"
     ]
    }
   ],
   "source": [
    "# Load Senseval instances for the word \"hard\"\n",
    "hard_instances = []\n",
    "try:\n",
    "    # Load Senseval instances for the word \"hard\"\n",
    "    print(\"Cargando instancias de 'hard' desde Senseval...\")\n",
    "    hard_instances = senseval.instances('hard.pos')\n",
    "    # Filter instances to ensure they have senses, as some might not\n",
    "    hard_instances = [instance for instance in hard_instances if instance.senses]\n",
    "    print(f\"Total Senseval 'hard' instances with senses: {len(hard_instances)}\")\n",
    "except Exception as e:\n",
    "    print(f\"Could not load 'hard.pos' from senseval, skipping 'hard' analysis for senses: {e}\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 112,
   "id": "02658947",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Total Senseval 'serve' instances with senses: 4378\n"
     ]
    }
   ],
   "source": [
    "# Inicializamos las instancias de \"serve\" de Senseval\n",
    "serve_instances = []\n",
    "try:\n",
    "    # Load instances for \"serve\" from Senseval\n",
    "    serve_instances = senseval.instances('serve.pos')\n",
    "    # Filter instances to ensure they have senses, as some might not\n",
    "    serve_instances = [instance for instance in serve_instances if instance.senses]\n",
    "    print(f\"Total Senseval 'serve' instances with senses: {len(serve_instances)}\")\n",
    "except Exception as e:\n",
    "    print(f\"Could not load 'serve.pos' from senseval, skipping 'serve' analysis for senses: {e}\")\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 113,
   "id": "faafb0e3",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "--- 1. Posibles sentidos y etiquetas para 'hard and 'serve' ---\n",
      "'hard' has 3 possible senses: {'HARD2', 'HARD1', 'HARD3'}\n",
      "'serve' has 4 possible senses: {'SERVE6', 'SERVE10', 'SERVE2', 'SERVE12'}\n"
     ]
    }
   ],
   "source": [
    "print(\"--- 1. Posibles sentidos y etiquetas para 'hard and 'serve' ---\")\n",
    "\n",
    "hard_senses = set()\n",
    "for instance in hard_instances:\n",
    "    for sense in instance.senses:\n",
    "        hard_senses.add(sense)\n",
    "print(f\"'hard' has {len(hard_senses)} possible senses: {hard_senses}\")\n",
    "\n",
    "serve_senses = set()\n",
    "for instance in serve_instances:\n",
    "    for sense in instance.senses:\n",
    "        serve_senses.add(sense)\n",
    "print(f\"'serve' has {len(serve_senses)} possible senses: {serve_senses}\")\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 114,
   "id": "396358d7",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "--- 2. Numero de instancias para cada sentido ---\n",
      "\n",
      "'hard' sense counts:\n",
      "  Sense 'HARD1': 3455 instances\n",
      "  Sense 'HARD2': 502 instances\n",
      "  Sense 'HARD3': 376 instances\n",
      "\n",
      "'serve' sense counts:\n",
      "  Sense 'SERVE10': 1814 instances\n",
      "  Sense 'SERVE12': 1272 instances\n",
      "  Sense 'SERVE2': 853 instances\n",
      "  Sense 'SERVE6': 439 instances\n"
     ]
    }
   ],
   "source": [
    "print(\"\\n--- 2. Numero de instancias para cada sentido ---\")\n",
    "\n",
    "hard_sense_counts = {}\n",
    "for instance in hard_instances:\n",
    "    for sense in instance.senses:\n",
    "        hard_sense_counts[sense] = hard_sense_counts.get(sense, 0) + 1\n",
    "print(\"\\n'hard' sense counts:\")\n",
    "for sense, count in hard_sense_counts.items():\n",
    "    print(f\"  Sense '{sense}': {count} instances\")\n",
    "\n",
    "serve_sense_counts = {}\n",
    "for instance in serve_instances:\n",
    "    for sense in instance.senses:\n",
    "        serve_sense_counts[sense] = serve_sense_counts.get(sense, 0) + 1\n",
    "print(\"\\n'serve' sense counts:\")\n",
    "for sense, count in serve_sense_counts.items():\n",
    "    print(f\"  Sense '{sense}': {count} instances\")\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 115,
   "id": "20a2e34b",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "--- 3. Grammatical forms for 'hard' and 'serve' ---\n",
      "Formatos gramaticales en 'hard' encontrados: {'hardest', 'harder', 'hard'}\n",
      "Formatos gramaticales en 'server' encontrados: {'serves', 'server', 'serve', 'served'}\n"
     ]
    }
   ],
   "source": [
    "# 3. Grammatical forms for \"hard\" and \"serve\"\n",
    "print(\"--- 3. Grammatical forms for 'hard' and 'serve' ---\")\n",
    "\n",
    "# For 'hard'\n",
    "hard_forms = set()\n",
    "for instance in hard_instances:\n",
    "    # instance.context might contain (word, tag) tuples or just words.\n",
    "    # We'll try to handle both cases by checking the type of 'item'\n",
    "    for item in instance.context:\n",
    "        word_to_check = ''\n",
    "        if isinstance(item, tuple):\n",
    "            word_to_check = item[0]  # Get the word from the tuple\n",
    "        elif isinstance(item, str):\n",
    "            word_to_check = item   # 'item' is already the word string\n",
    "        \n",
    "        if word_to_check: # Ensure it's not empty\n",
    "            if word_to_check.lower() == 'hard' or word_to_check.lower() == 'harder' or word_to_check.lower() == 'hardest':\n",
    "                hard_forms.add(word_to_check.lower())\n",
    "print(f\"Formatos gramaticales en 'hard' encontrados: {hard_forms}\")\n",
    "\n",
    "# For 'serve'\n",
    "serve_forms = set()\n",
    "for instance in serve_instances:\n",
    "    for item in instance.context:\n",
    "        word_to_check = ''\n",
    "        if isinstance(item, tuple):\n",
    "            word_to_check = item[0]\n",
    "        elif isinstance(item, str):\n",
    "            word_to_check = item\n",
    "            \n",
    "        if word_to_check:\n",
    "            if word_to_check.lower().startswith('serve'): # Catch serve, serves, served, serving, etc.\n",
    "                serve_forms.add(word_to_check.lower())\n",
    "print(f\"Formatos gramaticales en 'server' encontrados: {serve_forms}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 116,
   "id": "b46a868a",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "SensevalInstance(word='hard-a', position=10, context=[('clever', 'NNP'), ('white', 'NNP'), ('house', 'NNP'), ('``', '``'), ('spin', 'VB'), ('doctors', 'NNS'), (\"''\", \"''\"), ('are', 'VBP'), ('having', 'VBG'), ('a', 'DT'), ('hard', 'JJ'), ('time', 'NN'), ('helping', 'VBG'), ('president', 'NNP'), ('bush', 'NNP'), ('explain', 'VB'), ('away', 'RB'), ('the', 'DT'), ('economic', 'JJ'), ('bashing', 'NN'), ('that', 'IN'), ('low-and', 'JJ'), ('middle-income', 'JJ'), ('workers', 'NNS'), ('are', 'VBP'), ('taking', 'VBG'), ('these', 'DT'), ('days', 'NNS'), ('.', '.')], senses=('HARD1',))\n"
     ]
    }
   ],
   "source": [
    "inst = hard_instances[1]\n",
    "print(inst)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 117,
   "id": "ea503a91",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[('clever', 'NNP'),\n",
       " ('white', 'NNP'),\n",
       " ('house', 'NNP'),\n",
       " ('``', '``'),\n",
       " ('spin', 'VB'),\n",
       " ('doctors', 'NNS'),\n",
       " (\"''\", \"''\"),\n",
       " ('are', 'VBP'),\n",
       " ('having', 'VBG'),\n",
       " ('a', 'DT'),\n",
       " ('hard', 'JJ'),\n",
       " ('time', 'NN'),\n",
       " ('helping', 'VBG'),\n",
       " ('president', 'NNP'),\n",
       " ('bush', 'NNP'),\n",
       " ('explain', 'VB'),\n",
       " ('away', 'RB'),\n",
       " ('the', 'DT'),\n",
       " ('economic', 'JJ'),\n",
       " ('bashing', 'NN'),\n",
       " ('that', 'IN'),\n",
       " ('low-and', 'JJ'),\n",
       " ('middle-income', 'JJ'),\n",
       " ('workers', 'NNS'),\n",
       " ('are', 'VBP'),\n",
       " ('taking', 'VBG'),\n",
       " ('these', 'DT'),\n",
       " ('days', 'NNS'),\n",
       " ('.', '.')]"
      ]
     },
     "execution_count": 117,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "inst.context"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 118,
   "id": "dcf67e20",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'find', 'get', 'make', 'time', 'work', 'would'}"
      ]
     },
     "execution_count": 118,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "vocab = ['time', 'would', 'get', 'work', 'find', 'make']\n",
    "vocab = set(vocab) \n",
    "vocab# Convert to set for faster lookup"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 119,
   "id": "d8b46dac",
   "metadata": {},
   "outputs": [],
   "source": [
    "class Instance:\n",
    "    def __init__(self, context, ambiguous_word_index):\n",
    "        self.context = context\n",
    "        self.ambiguous_word_index = ambiguous_word_index # 칈ndice de la palabra ambigua en el contexto\n",
    "\n",
    "# Ejemplo de contexto para 'hard'\n",
    "# Asumimos que 'hard' es la palabra ambigua y est치 en alguna posici칩n\n",
    "# Para simplificar, aqu칤 el contexto es una lista de palabras.\n",
    "hard_instance_context = vocab"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "db784e42",
   "metadata": {},
   "source": [
    "## Extrae un vector de caracter칤sticas binario basado en la presencia de palabras del vocabulario\n",
    "    en el contexto dado.\n",
    "\n",
    "    + Args:\n",
    "        contexto (list): Una lista de palabras que forman el contexto de la palabra ambigua.\n",
    "                         Puede contener tuplas (palabra, tag) o solo palabras.\n",
    "        vocabulario (list): Una lista de palabras que conforman el vocabulario de caracter칤sticas.\n",
    "\n",
    "    + Returns:\n",
    "        list: Un vector binario donde cada elemento indica la presencia (1) o ausencia (0)\n",
    "              de la palabra correspondiente del vocabulario en el contexto.\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 64,
   "id": "82045073",
   "metadata": {},
   "outputs": [],
   "source": [
    "def extraer_caracteristicas_palabras_vecinas(contexto, vocabulario):\n",
    "\n",
    "    vector_caracteristicas = []\n",
    "    contexto_palabras_lower = []\n",
    "\n",
    "    # Normalizar el contexto a solo palabras en min칰sculas\n",
    "    for item in contexto:\n",
    "        word_to_add = ''\n",
    "        if isinstance(item, tuple):\n",
    "            word_to_add = item[0].lower()\n",
    "        elif isinstance(item, str):\n",
    "            word_to_add = item.lower()\n",
    "        if word_to_add:\n",
    "            contexto_palabras_lower.append(word_to_add)\n",
    "\n",
    "    # Construir el vector de caracter칤sticas\n",
    "    for palabra_vocabulario in vocabulario:\n",
    "        if palabra_vocabulario.lower() in contexto_palabras_lower:\n",
    "            vector_caracteristicas.append(1)\n",
    "        else:\n",
    "            vector_caracteristicas.append(0)\n",
    "            \n",
    "    return vector_caracteristicas"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a4179bf7",
   "metadata": {},
   "source": [
    "# Construcci칩n del vocabulario o bags of words.\n",
    "\"Este vector de caracter칤sticas indica que en el contexto de la palabra\n",
    "ambigua aparece la palabra 춺time췉 y no aparecen las palabras\n",
    "춺would췉, 춺get췉, 춺work췉, 춺find췉 y 춺make췉.\"\n",
    "Esto se traducir칤a a [1, 0, 0, 0, 0, 0] para el vocabulario dado.\n",
    "Si en el contexto_hard las palabras \"would\", \"get\", \"work\", \"find\", \"make\" no estuvieran presentes,\n",
    "el resultado ser칤a exactamente [1, 0, 0, 0, 0, 0].\n",
    "En el contexto_hard de ejemplo, las palabras \"would\", \"get\", \"work\", \"find\", \"make\" s칤 aparecen.\n",
    "Por lo tanto, el vector resultante ser칤a:\n",
    "Palabra 'time': 1 (aparece)\n",
    "Palabra 'would': 1 (aparece)\n",
    "Palabra 'get': 1 (aparece)\n",
    "Palabra 'work': 1 (aparece)\n",
    "Palabra 'find': 1 (aparece)\n",
    "Palabra 'make': 1 (aparece)\n",
    "--> [1, 1, 1, 1, 1, 1]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "97926c98",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Vocabulario: {'get', 'make', 'time', 'would', 'find', 'work'}\n",
      "Contexto de 'hard': este vector de caracter칤sticas indica que en el contexto de la palabra ambigua aparece la palabra time y no aparecen las palabras would get work find y make\n",
      "Vector de caracter칤sticas para 'hard': [1, 1, 1, 1, 1, 1]\n",
      "Contexto de 'hard' (ejemplo solicitado): este es un ejemplo con time pero sin would get work find make\n",
      "Vector de caracter칤sticas para 'hard' (ejemplo solicitado): [1, 1, 1, 1, 1, 1]\n"
     ]
    }
   ],
   "source": [
    "    # Contexto de la instancia de \"hard\"\n",
    "# Simulamos un contexto donde \"time\" aparece y el resto no.\n",
    "contexto_hard = [\"este\", \"vector\", \"de\", \"caracter칤sticas\", \"indica\", \"que\", \"en\", \"el\", \"contexto\", \"de\", \"la\", \"palabra\",\n",
    "                 \"ambigua\", \"aparece\", \"la\", \"palabra\", \"time\", \"y\", \"no\", \"aparecen\", \"las\", \"palabras\",\n",
    "                 \"would\", \"get\", \"work\", \"find\", \"y\", \"make\"]\n",
    "\n",
    "# Extraer el vector de caracter칤sticas\n",
    "vector_caracteristicas_hard = extraer_caracteristicas_palabras_vecinas(contexto_hard, vocab)\n",
    "\n",
    "print(f\"Vocabulario: {vocab}\")\n",
    "print(f\"Contexto de 'hard': {' '.join(contexto_hard)}\")\n",
    "print(f\"Vector de caracter칤sticas para 'hard': {vector_caracteristicas_hard}\")\n",
    "\n",
    "\n",
    "\n",
    "# Si el contexto_hard fuera el que describiste:\n",
    "contexto_hard_solicitado = [\"este\", \"es\", \"un\", \"ejemplo\", \"con\", \"time\", \"pero\", \"sin\", \"would\", \"get\", \"work\", \"find\", \"make\"]\n",
    "vector_caracteristicas_hard_solicitado = extraer_caracteristicas_palabras_vecinas(contexto_hard_solicitado, vocab)\n",
    "print(f\"Contexto de 'hard' (ejemplo solicitado): {' '.join(contexto_hard_solicitado)}\")\n",
    "print(f\"Vector de caracter칤sticas para 'hard' (ejemplo solicitado): {vector_caracteristicas_hard_solicitado}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a21ecfb9",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[<__main__.Instance at 0x20bbca44610>,\n",
       " <__main__.Instance at 0x20bbca47b90>,\n",
       " <__main__.Instance at 0x20bbca45990>,\n",
       " <__main__.Instance at 0x20bbca440d0>,\n",
       " <__main__.Instance at 0x20bbca47350>,\n",
       " <__main__.Instance at 0x20bbca47b50>,\n",
       " <__main__.Instance at 0x20bbca46990>,\n",
       " <__main__.Instance at 0x20bbca45110>,\n",
       " <__main__.Instance at 0x20bbca45310>,\n",
       " <__main__.Instance at 0x20bbca44110>,\n",
       " <__main__.Instance at 0x20bbca41150>,\n",
       " <__main__.Instance at 0x20bbca42910>]"
      ]
     },
     "execution_count": 81,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Aseg칰rate de haber descargado los recursos necesarios de NLTK\n",
    "try:\n",
    "    nltk.data.find('corpora/stopwords')\n",
    "except nltk.downloader.DownloadError:\n",
    "    nltk.download('stopwords')\n",
    "try:\n",
    "    nltk.data.find('tokenizers/punkt')\n",
    "except nltk.downloader.DownloadError:\n",
    "    nltk.download('punkt')\n",
    "\n",
    "\n",
    "# Simulaci칩n de las instancias del corpus\n",
    "# En un escenario real, 'hard_instances' y 'serve_instances'\n",
    "# vendr칤an de tu corpus cargado.\n",
    "class Instance:\n",
    "    def __init__(self, context, ambiguous_word):\n",
    "        self.context = context # Lista de palabras o (palabra, tag) tuplas\n",
    "        self.ambiguous_word = ambiguous_word # La palabra ambigua asociada a esta instancia\n",
    "\n",
    "        \n",
    "# Ejemplos de instancias (simuladas para 'hard' y 'serve')\n",
    "# Aqu칤 usaremos solo strings en el contexto para simplificar, pero el c칩digo manejar치 tuplas tambi칠n.\n",
    "hard_instances = [\n",
    "    Instance([\"It\", \"was\", \"a\", \"hard\", \"time\", \"for\", \"everyone\", \"involved\", \".\"], \"hard\"),\n",
    "    Instance([\"He\", \"worked\", \"hard\", \"to\", \"get\", \"the\", \"job\", \"done\", \".\"], \"hard\"),\n",
    "    Instance([\"The\", \"decision\", \"was\", \"harder\", \"than\", \"I\", \"thought\", \".\"], \"hard\"),\n",
    "    Instance([\"This\", \"is\", \"the\", \"hardest\", \"challenge\", \"I've\", \"faced\", \".\"], \"hard\"),\n",
    "    Instance([\"You\", \"would\", \"find\", \"it\", \"hard\", \"to\", \"make\", \"a\", \"living\", \".\"], \"hard\"),\n",
    "    Instance([\"It's\", \"hard\", \"work\", \"but\", \"it's\", \"rewarding\", \".\"], \"hard\"),\n",
    "    Instance([\"They\", \"get\", \"through\", \"hard\", \"times\", \".\"], \"hard\"),\n",
    "    Instance([\"It's\", \"hard\", \"to\", \"get\", \"enough\", \"sleep\", \".\"], \"hard\"),\n",
    "]\n",
    "\n",
    "serve_instances = [\n",
    "    Instance([\"He\", \"will\", \"serve\", \"his\", \"country\", \".\"], \"serve\"),\n",
    "    Instance([\"The\", \"waiter\", \"serves\", \"food\", \".\"], \"serve\"),\n",
    "    Instance([\"She\", \"served\", \"in\", \"the\", \"military\", \".\"], \"serve\"),\n",
    "    Instance([\"Serving\", \"the\", \"community\", \"is\", \"important\", \".\"], \"serve\"),\n",
    "]\n",
    "\n",
    "# Recopilar todas las instancias en una sola lista para procesar todo el corpus\n",
    "all_instances = hard_instances + serve_instances\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 83,
   "id": "bdf14f46",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Total de palabras a excluir: 268\n"
     ]
    }
   ],
   "source": [
    "# Stopwords y otros signos a excluir\n",
    "OTHER_WORDS = [\"''\",\"'d\",\"'ll\",\"'m\",\"'re\",\"'s\",\"'t\",\"'ve\",'--','000','1','2','3','4','5','6','8','10','15','30','I','F','``',\n",
    "               'also',\"don'\",'n','one','said','say','says','u','us']\n",
    "\n",
    "# Combinar stopwords de NLTK, puntuaci칩n y OTHER_WORDS\n",
    "STOPWORDS_SET = set(stopwords.words('english')).union(set(string.punctuation), set(OTHER_WORDS))\n",
    "\n",
    "# Identificar las formas gramaticales de tus palabras ambiguas\n",
    "# Aqu칤 es donde debes a침adir las formas que identificaste en tu laboratorio.\n",
    "# Por ejemplo, para 'hard': hard, harder, hardest\n",
    "# Para 'serve': serve, serves, served, serving, etc.\n",
    "# Basado en tu c칩digo anterior:\n",
    "hard_forms = {'hard', 'harder', 'hardest'}\n",
    "serve_forms = set()\n",
    "for instance in serve_instances:\n",
    "    for item in instance.context:\n",
    "        word_to_check = ''\n",
    "        if isinstance(item, tuple):\n",
    "            word_to_check = item[0]\n",
    "        elif isinstance(item, str):\n",
    "            word_to_check = item\n",
    "        if word_to_check and word_to_check.lower().startswith('serve'):\n",
    "            serve_forms.add(word_to_check.lower())\n",
    "\n",
    "# Combinar todas las palabras a excluir\n",
    "WORDS_TO_EXCLUDE = STOPWORDS_SET.union(hard_forms).union(serve_forms)\n",
    "\n",
    "print(f\"Total de palabras a excluir: {len(WORDS_TO_EXCLUDE)}\")\n",
    "# print(f\"Ejemplo de palabras a excluir: {list(WORDS_TO_EXCLUDE)[:20]}\") # Descomenta para ver algunas"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 90,
   "id": "8081eeed",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Distribuci칩n de frecuencias inicial (top 10): [('.', 12), ('hard', 6), ('the', 6), ('to', 3), ('get', 3), (\"it's\", 3), ('it', 2), ('was', 2), ('a', 2), ('he', 2)]\n"
     ]
    }
   ],
   "source": [
    "all_words = []\n",
    "for instance in all_instances:\n",
    "    for item in instance.context:\n",
    "        word = ''\n",
    "        if isinstance(item, tuple):\n",
    "            word = item[0]\n",
    "        elif isinstance(item, str):\n",
    "            word = item\n",
    "        \n",
    "        # Procesar solo si la palabra no est치 vac칤a y no es una forma gramatical de la palabra ambigua (inicialmente)\n",
    "        # La exclusi칩n de otras palabras se har치 despu칠s de FreqDist\n",
    "        if word:\n",
    "            all_words.append(word.lower())\n",
    "\n",
    "# Calcular la distribuci칩n de frecuencias de todas las palabras\n",
    "freq_dist = FreqDist(all_words)\n",
    "\n",
    "print(f\"\\nDistribuci칩n de frecuencias inicial (top 10): {freq_dist.most_common(10)}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 91,
   "id": "d008f6be",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "--- Vocabulario Construido ---\n",
      "N칰mero de palabras m치s frecuentes (m): 6\n",
      "Vocabulario final: ['get', 'time', 'everyone', 'involved', 'worked', 'job']\n"
     ]
    }
   ],
   "source": [
    "def construir_vocabulario_frecuente(freq_dist, words_to_exclude, m):\n",
    "    \"\"\"\n",
    "    Construye un vocabulario con las m palabras m치s frecuentes,\n",
    "    excluyendo las palabras especificadas.\n",
    "\n",
    "    Args:\n",
    "        freq_dist (nltk.probability.FreqDist): Distribuci칩n de frecuencias de las palabras.\n",
    "        words_to_exclude (set): Conjunto de palabras (en min칰sculas) a excluir.\n",
    "        m (int): El n칰mero de palabras m치s frecuentes que queremos en el vocabulario.\n",
    "\n",
    "    Returns:\n",
    "        list: Una lista de las m palabras m치s frecuentes y filtradas, que formar치n el vocabulario.\n",
    "    \"\"\"\n",
    "    filtered_vocab = []\n",
    "    \n",
    "    # Itera sobre las palabras m치s comunes en orden descendente de frecuencia\n",
    "    for word, frequency in freq_dist.most_common():\n",
    "        if word not in words_to_exclude:\n",
    "            filtered_vocab.append(word)\n",
    "        \n",
    "        # Detener una vez que hemos alcanzado el tama침o de vocabulario deseado (m)\n",
    "        if len(filtered_vocab) >= m:\n",
    "            break\n",
    "            \n",
    "    return filtered_vocab\n",
    "\n",
    "# Definir el tama침o del vocabulario deseado (m)\n",
    "m_palabras = 6 # Para el ejemplo de 'hard'\n",
    "\n",
    "# Construir el vocabulario final\n",
    "vocabulario_final = construir_vocabulario_frecuente(freq_dist, WORDS_TO_EXCLUDE, m_palabras)\n",
    "\n",
    "print(f\"\\n--- Vocabulario Construido ---\")\n",
    "print(f\"N칰mero de palabras m치s frecuentes (m): {m_palabras}\")\n",
    "print(f\"Vocabulario final: {vocabulario_final}\")\n",
    "\n",
    "# Comprobaci칩n con el ejemplo de 'hard'\n",
    "# Si las instancias de 'hard' contienen las palabras 'time', 'would', 'get', 'work', 'find', 'make'\n",
    "# y estas son las 6 m치s frecuentes despu칠s del filtrado, entonces coincidir치n.\n",
    "# Con mis datos simulados, el resultado ser치 diferente al ejemplo dado inicialmente,\n",
    "# ya que el ejemplo de 'hard' ten칤a un vocabulario predefinido.\n",
    "# Para que coincida con ['time', 'would', 'get', 'work', 'find', 'make'],\n",
    "# esas palabras deber칤an ser las 6 m치s frecuentes en TU corpus real despu칠s de la limpieza."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 95,
   "id": "7eb83562",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "N칰mero de palabras en el vocabulario deseado (m): 6\n",
      "Vocabulario construido: ['time', 'get', 'find', 'work', 'make', 'everyone']\n"
     ]
    }
   ],
   "source": [
    "# nltk.download('stopwords')\n",
    "# nltk.download('punkt') # Para tokenizaci칩n si la usas expl칤citamente\n",
    "\n",
    "# --- Definici칩n de Stop Words y Palabras Adicionales a Eliminar ---\n",
    "# Tu conjunto de palabras a eliminar\n",
    "OTHER_WORDS = [\"''\", \"'d\", \"'ll\", \"'m\", \"'re\", \"'s\", \"'t\", \"'ve\", '--', '000', '1', '2', '3', '4', '5', '6', '8', '10', '15', '30', 'I', 'F', '``',\n",
    "               'also', \"don'\", 'n', 'one', 'said', 'say', 'says', 'u', 'us']\n",
    "\n",
    "# Conjunto combinado de stopwords de NLTK (ingl칠s), puntuaci칩n y tus palabras adicionales\n",
    "# Nota: NLTK stopwords por defecto son en ingl칠s. Si tu corpus est치 en espa침ol,\n",
    "# necesitar치s cambiar 'english' a 'spanish' en 'stopwords.words()'.\n",
    "# Para este ejemplo, mantendr칠 'english' como en tu referencia, pero tenlo en cuenta.\n",
    "STOPWORDS_SET = set(stopwords.words('english')).union(set(string.punctuation), set(OTHER_WORDS))\n",
    "\n",
    "# Clase dummy para simular las instancias del corpus\n",
    "class Instance:\n",
    "    def __init__(self, context, ambiguous_word):\n",
    "        # context puede ser una lista de palabras o de tuplas (palabra, tag)\n",
    "        self.context = context\n",
    "        self.ambiguous_word = ambiguous_word\n",
    "\n",
    "def construir_vocabulario(instances, m, palabras_ambiguas_forms):\n",
    "    \"\"\"\n",
    "    Construye un vocabulario de las 'm' palabras m치s frecuentes del corpus,\n",
    "    excluyendo puntuaci칩n, stopwords y formas gramaticales de las palabras ambiguas.\n",
    "\n",
    "    Args:\n",
    "        instances (list): Lista de objetos Instance, cada uno con un contexto.\n",
    "        m (int): El n칰mero de palabras m치s frecuentes a incluir en el vocabulario.\n",
    "        palabras_ambiguas_forms (dict): Un diccionario donde la clave es la palabra ambigua\n",
    "                                        y el valor es un conjunto de sus formas gramaticales\n",
    "                                        a excluir (ej: {'hard': {'hard', 'harder', 'hardest'}}).\n",
    "\n",
    "    Returns:\n",
    "        list: El vocabulario construido (lista de palabras).\n",
    "    \"\"\"\n",
    "    all_words = []\n",
    "    \n",
    "    # Recopilar todas las palabras de todos los contextos\n",
    "    for instance in instances:\n",
    "        for item in instance.context:\n",
    "            word = ''\n",
    "            if isinstance(item, tuple):\n",
    "                word = item[0] # Extraer la palabra de la tupla (si hay POS tags)\n",
    "            elif isinstance(item, str):\n",
    "                word = item # La palabra ya es un string\n",
    "            \n",
    "            # Normalizar a min칰sculas\n",
    "            word_lower = word.lower()\n",
    "            all_words.append(word_lower)\n",
    "            \n",
    "    # Usar FreqDist para calcular las frecuencias\n",
    "    fdist = nltk.FreqDist(all_words)\n",
    "    \n",
    "    # Filtrar palabras: eliminar puntuaci칩n, stopwords y formas de palabras ambiguas\n",
    "    # Creamos un conjunto de todas las palabras a excluir\n",
    "    words_to_exclude = set()\n",
    "    words_to_exclude.update(STOPWORDS_SET)\n",
    "    \n",
    "    for amb_word_forms in palabras_ambiguas_forms.values():\n",
    "        words_to_exclude.update(amb_word_forms)\n",
    "\n",
    "    filtered_words_freq = Counter()\n",
    "    for word, freq in fdist.items():\n",
    "        if word not in words_to_exclude:\n",
    "            filtered_words_freq[word] = freq\n",
    "            \n",
    "    # Obtener las 'm' palabras m치s frecuentes\n",
    "    vocabulario = [word for word, freq in filtered_words_freq.most_common(m)]\n",
    "    \n",
    "    return vocabulario\n",
    "\n",
    "# --- Ejemplo de Uso ---\n",
    "\n",
    "# 1. Simular datos del corpus (tus instancias de \"hard\" y \"serve\")\n",
    "# Debes reemplazar esto con tus datos reales (hard_instances, serve_instances)\n",
    "# Ejemplo de contextos\n",
    "corpus_instances = [\n",
    "    Instance(context=[\"this\", \"is\", \"a\", \"hard\", \"time\", \"for\", \"everyone\", \"and\", \"we\", \"will\", \"get\", \"through\", \"it\"], ambiguous_word=\"hard\"),\n",
    "    Instance(context=[\"it's\", \"hard\", \"to\", \"find\", \"a\", \"job\", \"these\", \"days\"], ambiguous_word=\"hard\"),\n",
    "    Instance(context=[\"the\", \"machine\", \"will\", \"serve\", \"you\", \"well\"], ambiguous_word=\"serve\"),\n",
    "    Instance(context=[\"they\", \"serve\", \"delicious\", \"food\", \"at\", \"that\", \"workplace\"], ambiguous_word=\"serve\"),\n",
    "    Instance(context=[\"a\", \"time\", \"to\", \"work\", \"harder\", \"than\", \"ever\"], ambiguous_word=\"hard\"),\n",
    "    Instance(context=[\"he\", \"would\", \"like\", \"to\", \"make\", \"a\", \"difference\"], ambiguous_word=\"would\"), # Una instancia extra para 'would'\n",
    "    Instance(context=[\"you\", \"should\", \"get\", \"up\", \"and\", \"make\", \"it\", \"happen\"], ambiguous_word=\"get\"), # Instancia para 'get', 'make'\n",
    "    Instance(context=[\"find\", \"your\", \"own\", \"way\", \"to\", \"work\", \"it\", \"out\"], ambiguous_word=\"find\") # Instancia para 'find', 'work'\n",
    "]\n",
    "\n",
    "# 2. Definir las formas gramaticales de las palabras ambiguas\n",
    "# Aqu칤 es donde a침adir칤as las formas que identificaste en la Parte 1\n",
    "palabras_ambiguas_forms = {\n",
    "    'hard': {'hard', 'harder', 'hardest'},\n",
    "    'serve': {'serve', 'serves', 'served', 'serving'} # Ejemplos para 'serve'\n",
    "}\n",
    "\n",
    "# 3. Construir el vocabulario con un tama침o 'm' (por ejemplo, m=6)\n",
    "m_size = 6\n",
    "vocab_construido = construir_vocabulario(corpus_instances, m_size, palabras_ambiguas_forms)\n",
    "\n",
    "print(f\"N칰mero de palabras en el vocabulario deseado (m): {m_size}\")\n",
    "print(f\"Vocabulario construido: {vocab_construido}\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "87681088",
   "metadata": {},
   "source": [
    "\n",
    "---\n",
    "\n",
    "### Explicaci칩n del C칩digo:\n",
    "\n",
    "1.  **Importaciones:** `nltk` para `FreqDist`, `stopwords` y `string` para la puntuaci칩n, `collections.Counter` (aunque `FreqDist` es m치s espec칤fico para este caso, `Counter` es una alternativa general para contar).\n",
    "2.  **`OTHER_WORDS` y `STOPWORDS_SET`:** Se mantienen tus definiciones para las palabras a excluir. Es crucial que si tu corpus es en espa침ol, cambies `stopwords.words('english')` a `stopwords.words('spanish')`.\n",
    "3.  **Clase `Instance` (Simulaci칩n):** Esta es una clase simplificada para demostrar c칩mo manejar칤as tus datos. En tu entorno real, `hard_instances` y `serve_instances` probablemente ya son listas de objetos con un atributo `context`.\n",
    "4.  **`construir_vocabulario(instances, m, palabras_ambiguas_forms)`:**\n",
    "    * **Recopilaci칩n de palabras:** Itera sobre todas las `instances` y sus `contextos`. Para cada palabra, maneja si viene como tupla `(palabra, tag)` o como cadena simple, y la convierte a min칰sculas.\n",
    "    * **`nltk.FreqDist(all_words)`:** Calcula la frecuencia de cada palabra en la lista `all_words`.\n",
    "    * **`words_to_exclude`:** Se crea un conjunto combinado de todas las palabras que queremos eliminar: las stop words, la puntuaci칩n y *todas las formas gramaticales de tus palabras ambiguas* (ej: 'hard', 'harder', 'hardest', 'serve', 'serves', etc.).\n",
    "    * **Filtrado:** Se crea un nuevo `Counter` (`filtered_words_freq`) que solo incluye las palabras que *no* est치n en `words_to_exclude`.\n",
    "    * **`filtered_words_freq.most_common(m)`:** Este m칠todo es clave. Devuelve una lista de tuplas `(palabra, frecuencia)` de las `m` palabras m치s comunes del diccionario de frecuencias filtrado.\n",
    "    * **Construcci칩n final del vocabulario:** Se extraen solo las palabras (el primer elemento de cada tupla) para formar el vocabulario final.\n",
    "5.  **`palabras_ambiguas_forms`:** Este diccionario es fundamental. **Debes poblarlo con las formas gramaticales espec칤ficas que identificaste para \"hard\", \"serve\" y cualquier otra palabra ambigua que est칠s analizando.**\n",
    "\n",
    "Este enfoque te permite crear un vocabulario robusto y relevante, que es un requisito fundamental para luego construir tus vectores de caracter칤sticas.\n",
    "\n",
    "쮿ay alguna parte de este proceso que te gustar칤a explorar con m치s detalle, o alguna otra consideraci칩n para tu corpus?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 97,
   "id": "dc97d71c",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Vocabulario generado para m=6: ['time', 'get', 'find', 'work', 'make', 'everyone']\n",
      "---\n",
      "Instancia de ejemplo - Palabra ambigua: 'hard'\n",
      "Contexto (oraci칩n completa): this is a hard time for everyone and we will get through it\n",
      "Vector de caracter칤sticas para la instancia (diccionario):\n",
      "{'contains(time)': True, 'contains(get)': True, 'contains(find)': False, 'contains(work)': False, 'contains(make)': False, 'contains(everyone)': True}\n",
      "---\n",
      "--- Ejemplo espec칤fico del enunciado ---\n",
      "Vocabulario de ejemplo del enunciado: ['time', 'would', 'get', 'work', 'find', 'make']\n",
      "Contexto del enunciado: este vector de caracter칤sticas indica que en el contexto de la palabra ambigua aparece la palabra time y no aparecen las palabras would get work find y make\n",
      "Vector de caracter칤sticas (si solo 'time' aparece en el contexto): {'contains(time)': True, 'contains(would)': False, 'contains(get)': False, 'contains(work)': False, 'contains(find)': False, 'contains(make)': False}\n"
     ]
    }
   ],
   "source": [
    "\n",
    "# Tu conjunto de palabras a eliminar (igual al anterior)\n",
    "OTHER_WORDS = [\"''\", \"'d\", \"'ll\", \"'m\", \"'re\", \"'s\", \"'t\", \"'ve\", '--', '000', '1', '2', '3', '4', '5', '6', '8', '10', '15', '30', 'I', 'F', '``',\n",
    "               'also', \"don'\", 'n', 'one', 'said', 'say', 'says', 'u', 'us']\n",
    "\n",
    "# Conjunto combinado de stopwords de NLTK (ingl칠s), puntuaci칩n y tus palabras adicionales\n",
    "STOPWORDS_SET = set(stopwords.words('english')).union(set(string.punctuation), set(OTHER_WORDS))\n",
    "\n",
    "def construir_vocabulario(instances, m, palabras_ambiguas_forms):\n",
    "    \"\"\"\n",
    "    Construye un vocabulario de las 'm' palabras m치s frecuentes del corpus,\n",
    "    excluyendo puntuaci칩n, stopwords y formas gramaticales de las palabras ambiguas.\n",
    "    (Esta funci칩n es la misma que la proporcionada anteriormente)\n",
    "    \"\"\"\n",
    "    all_words = []\n",
    "    for instance in instances:\n",
    "        for item in instance.context:\n",
    "            word = ''\n",
    "            if isinstance(item, tuple):\n",
    "                word = item[0]\n",
    "            elif isinstance(item, str):\n",
    "                word = item\n",
    "            word_lower = word.lower()\n",
    "            all_words.append(word_lower)\n",
    "            \n",
    "    fdist = nltk.FreqDist(all_words)\n",
    "    \n",
    "    words_to_exclude = set()\n",
    "    words_to_exclude.update(STOPWORDS_SET)\n",
    "    \n",
    "    for amb_word_forms in palabras_ambiguas_forms.values():\n",
    "        words_to_exclude.update(amb_word_forms)\n",
    "\n",
    "    filtered_words_freq = Counter()\n",
    "    for word, freq in fdist.items():\n",
    "        if word not in words_to_exclude:\n",
    "            filtered_words_freq[word] = freq\n",
    "            \n",
    "    vocabulario = [word for word, freq in filtered_words_freq.most_common(m)]\n",
    "    \n",
    "    return vocabulario\n",
    "\n",
    "# --- Nueva funci칩n para extraer el vector de caracter칤sticas como diccionario ---\n",
    "\n",
    "def extraer_caracteristicas_diccionario(instance_context, vocabulario):\n",
    "    \"\"\"\n",
    "    Extrae un diccionario de caracter칤sticas binarias basado en la presencia\n",
    "    de palabras del vocabulario en el contexto dado.\n",
    "\n",
    "    Args:\n",
    "        instance_context (list): Una lista de palabras que forman la oraci칩n completa\n",
    "                                 donde aparece la palabra ambigua.\n",
    "                                 Puede contener tuplas (palabra, tag) o solo palabras.\n",
    "        vocabulario (list): Una lista de palabras que conforman el vocabulario de caracter칤sticas.\n",
    "\n",
    "    Returns:\n",
    "        dict: Un diccionario donde la clave es 'contains(palabra)' y el valor es True/False.\n",
    "    \"\"\"\n",
    "    vector_caracteristicas = {}\n",
    "    contexto_palabras_lower = []\n",
    "\n",
    "    # Normalizar el contexto a solo palabras en min칰sculas\n",
    "    for item in instance_context:\n",
    "        word_to_add = ''\n",
    "        if isinstance(item, tuple):\n",
    "            word_to_add = item[0]\n",
    "        elif isinstance(item, str):\n",
    "            word_to_add = item\n",
    "        \n",
    "        if word_to_add:\n",
    "            contexto_palabras_lower.append(word_to_add.lower())\n",
    "\n",
    "    # Construir el diccionario de caracter칤sticas\n",
    "    for palabra_vocabulario in vocabulario:\n",
    "        clave = f'contains({palabra_vocabulario})'\n",
    "        if palabra_vocabulario.lower() in contexto_palabras_lower:\n",
    "            vector_caracteristicas[clave] = True\n",
    "        else:\n",
    "            vector_caracteristicas[clave] = False\n",
    "            \n",
    "    return vector_caracteristicas\n",
    "\n",
    "# --- Ejemplo Completo ---\n",
    "\n",
    "# 1. Simular datos del corpus (usando los mismos datos de ejemplo que antes)\n",
    "corpus_instances = [\n",
    "    Instance(context=[\"this\", \"is\", \"a\", \"hard\", \"time\", \"for\", \"everyone\", \"and\", \"we\", \"will\", \"get\", \"through\", \"it\"], ambiguous_word=\"hard\"),\n",
    "    Instance(context=[\"it's\", \"hard\", \"to\", \"find\", \"a\", \"job\", \"these\", \"days\"], ambiguous_word=\"hard\"),\n",
    "    Instance(context=[\"the\", \"machine\", \"will\", \"serve\", \"you\", \"well\"], ambiguous_word=\"serve\"),\n",
    "    Instance(context=[\"they\", \"serve\", \"delicious\", \"food\", \"at\", \"that\", \"workplace\"], ambiguous_word=\"serve\"),\n",
    "    Instance(context=[\"a\", \"time\", \"to\", \"work\", \"harder\", \"than\", \"ever\"], ambiguous_word=\"hard\"),\n",
    "    Instance(context=[\"he\", \"would\", \"like\", \"to\", \"make\", \"a\", \"difference\"], ambiguous_word=\"would\"),\n",
    "    Instance(context=[\"you\", \"should\", \"get\", \"up\", \"and\", \"make\", \"it\", \"happen\"], ambiguous_word=\"get\"),\n",
    "    Instance(context=[\"find\", \"your\", \"own\", \"way\", \"to\", \"work\", \"it\", \"out\"], ambiguous_word=\"find\")\n",
    "]\n",
    "\n",
    "# 2. Definir las formas gramaticales de las palabras ambiguas\n",
    "palabras_ambiguas_forms = {\n",
    "    'hard': {'hard', 'harder', 'hardest'},\n",
    "    'serve': {'serve', 'serves', 'served', 'serving'}\n",
    "}\n",
    "\n",
    "# 3. Construir el vocabulario (ejemplo con m=6)\n",
    "m_size = 6\n",
    "vocabulario_generado = construir_vocabulario(corpus_instances, m_size, palabras_ambiguas_forms)\n",
    "\n",
    "print(f\"Vocabulario generado para m={m_size}: {vocabulario_generado}\")\n",
    "print(\"---\")\n",
    "\n",
    "# 4. Mostrar el vector de caracter칤sticas para una instancia del corpus\n",
    "# Tomemos la primera instancia como ejemplo:\n",
    "# \"this is a hard time for everyone and we will get through it\"\n",
    "\n",
    "instancia_ejemplo = corpus_instances[0]\n",
    "print(f\"Instancia de ejemplo - Palabra ambigua: '{instancia_ejemplo.ambiguous_word}'\")\n",
    "print(f\"Contexto (oraci칩n completa): {' '.join(instancia_ejemplo.context)}\")\n",
    "\n",
    "# Extraer el vector de caracter칤sticas para esta instancia\n",
    "vector_caracteristicas_instancia = extraer_caracteristicas_diccionario(instancia_ejemplo.context, vocabulario_generado)\n",
    "\n",
    "print(f\"Vector de caracter칤sticas para la instancia (diccionario):\\n{vector_caracteristicas_instancia}\")\n",
    "\n",
    "print(\"---\")\n",
    "\n",
    "# Para el ejemplo solicitado: {'contains(time)': True, 'contains(would)': False, ...}\n",
    "# Usaremos un vocabulario fijo para ilustrar ese caso espec칤fico.\n",
    "vocabulario_fijo_ejemplo = ['time', 'would', 'get', 'work', 'find', 'make']\n",
    "contexto_ejemplo_solicitado = [\"este\", \"vector\", \"de\", \"caracter칤sticas\", \"indica\", \"que\", \"en\", \"el\", \"contexto\", \"de\", \"la\", \"palabra\",\n",
    "                               \"ambigua\", \"aparece\", \"la\", \"palabra\", \"time\", \"y\", \"no\", \"aparecen\", \"las\", \"palabras\",\n",
    "                               \"would\", \"get\", \"work\", \"find\", \"y\", \"make\"] # Aqu칤, todas aparecen en el ejemplo de texto.\n",
    "\n",
    "print(\"--- Ejemplo espec칤fico del enunciado ---\")\n",
    "print(f\"Vocabulario de ejemplo del enunciado: {vocabulario_fijo_ejemplo}\")\n",
    "print(f\"Contexto del enunciado: {' '.join(contexto_ejemplo_solicitado)}\")\n",
    "\n",
    "# Si el contexto del ejemplo del enunciado solo contuviera 'time' y NO el resto:\n",
    "contexto_solo_time = [\"este\", \"vector\", \"de\", \"caracter칤sticas\", \"indica\", \"que\", \"en\", \"el\", \"contexto\", \"de\", \"la\", \"palabra\",\n",
    "                      \"ambigua\", \"aparece\", \"la\", \"palabra\", \"time\"]\n",
    "\n",
    "vector_caracteristicas_solicitado = extraer_caracteristicas_diccionario(contexto_solo_time, vocabulario_fijo_ejemplo)\n",
    "print(f\"Vector de caracter칤sticas (si solo 'time' aparece en el contexto): {vector_caracteristicas_solicitado}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 98,
   "id": "f66a8e03",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[('``', '``'),\n",
       " ('it', 'PRP'),\n",
       " (\"'s\", 'VBZ'),\n",
       " ('a', 'DT'),\n",
       " ('very', 'RB'),\n",
       " ('interesting', 'JJ'),\n",
       " ('place', 'NN'),\n",
       " ('to', 'TO'),\n",
       " ('work', 'VB'),\n",
       " (',', ','),\n",
       " ('but', 'CC'),\n",
       " ('i', 'PRP'),\n",
       " ('can', 'MD'),\n",
       " ('see', 'VB'),\n",
       " ('why', 'WRB'),\n",
       " ('some', 'DT'),\n",
       " ('people', 'NNS'),\n",
       " ('have', 'VBP'),\n",
       " ('a', 'DT'),\n",
       " ('hard', 'JJ'),\n",
       " ('time', 'NN'),\n",
       " ('imagining', 'VBG'),\n",
       " ('what', 'WP'),\n",
       " ('it', 'PRP'),\n",
       " (\"'s\", 'VBZ'),\n",
       " ('like', 'IN'),\n",
       " (',', ','),\n",
       " ('\"', '\"'),\n",
       " ('says', 'VBZ'),\n",
       " ('nate', 'NNP'),\n",
       " ('gossett', 'NNP'),\n",
       " ('.', '.')]"
      ]
     },
     "execution_count": 98,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "senseval.instances('hard.pos')[2737].context"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 96,
   "id": "56c66c20",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "--- 4. Checking corpus_etiquetado.txt format consistency ---\n",
      "Por si se encuentra alguna inconsistencia 'corpus_etiquetado.txt':\n",
      "  1. Invalid tag format: .\t.\tFp (Doc ID: 1)\n",
      "  2. Invalid tag format: .\t.\tFp (Doc ID: 2)\n",
      "  3. Invalid tag format: .\t.\tFp (Doc ID: 3)\n",
      "  4. Invalid tag format: .\t.\tFp (Doc ID: 4)\n",
      "  5. Invalid tag format: .\t.\tFp (Doc ID: 5)\n",
      "  6. Invalid tag format: .\t.\tFp (Doc ID: 6)\n",
      "  7. Invalid tag format: .\t.\tFp (Doc ID: 7)\n",
      "  8. Invalid tag format: .\t.\tFp (Doc ID: 8)\n",
      "  9. Invalid tag format: .\t.\tFp (Doc ID: 9)\n",
      "  10. Invalid tag format: .\t.\tFp (Doc ID: 10)\n",
      "  ...and 10 more inconsistencies.\n",
      "\n",
      "Ejemplo como lo pide la guia con (word\\tlemma\\ttag):\n",
      "Habla\\thablar\\tVMIP3S0\n",
      "con\\tcon\\tSP\n",
      "el\\tel\\tDA\n"
     ]
    }
   ],
   "source": [
    "\n",
    "# Check if all instances in the provided corpus follow the format\n",
    "print(\"\\n--- 4. Checking corpus_etiquetado.txt format consistency ---\")\n",
    "\n",
    "if not inconsistent_lines:\n",
    "    print(\"Todas las inconsistencias en el modulo 'corpus_etiquetado.txt' que aparecen el  formato (word\\\\tlemma\\\\ttag).\")\n",
    "else:\n",
    "    print(\"Por si se encuentra alguna inconsistencia 'corpus_etiquetado.txt':\")\n",
    "    for i, problem in enumerate(inconsistent_lines[:10]): # Show first 10 examples\n",
    "        print(f\"  {i+1}. {problem}\")\n",
    "    if len(inconsistent_lines) > 10:\n",
    "        print(f\"  ...and {len(inconsistent_lines) - 10} more inconsistencies.\")\n",
    "\n",
    "# Example of a well-formatted line from your corpus for reference\n",
    "print(\"\\nEjemplo como lo pide la guia con (word\\\\tlemma\\\\ttag):\")\n",
    "print(\"Habla\\\\thablar\\\\tVMIP3S0\")\n",
    "print(\"con\\\\tcon\\\\tSP\")\n",
    "print(\"el\\\\tel\\\\tDA\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 68,
   "id": "fbe8b067",
   "metadata": {},
   "outputs": [],
   "source": [
    "def calcular_probabilidades_emision(corpus):\n",
    "    emision = {}\n",
    "    total_etiquetas = {}\n",
    "    for token, etiqueta in corpus:\n",
    "        if etiqueta not in emision:\n",
    "            emision[etiqueta] = {}\n",
    "            total_etiquetas[etiqueta] = 0\n",
    "        emision[etiqueta][token] = emision[etiqueta].get(token, 0) + 1\n",
    "        total_etiquetas[etiqueta] += 1\n",
    "    for etiqueta in emision:\n",
    "        for token in emision[etiqueta]:\n",
    "            emision[etiqueta][token] /= total_etiquetas[etiqueta]\n",
    "    return emision\n",
    "\n",
    "def calcular_probabilidades_transicion(corpus):\n",
    "    transicion = {}\n",
    "    total_transiciones = {}\n",
    "    anterior = None\n",
    "    for _, etiqueta in corpus:\n",
    "        if anterior is not None:\n",
    "            if anterior not in transicion:\n",
    "                transicion[anterior] = {}\n",
    "                total_transiciones[anterior] = 0\n",
    "            transicion[anterior][etiqueta] = transicion[anterior].get(etiqueta, 0) + 1\n",
    "            total_transiciones[anterior] += 1\n",
    "        anterior = etiqueta\n",
    "    for etiqueta in transicion:\n",
    "        for siguiente in transicion[etiqueta]:\n",
    "            transicion[etiqueta][siguiente] /= total_transiciones[etiqueta]\n",
    "    return transicion\n",
    "\n",
    "def calcular_probabilidades_iniciales(corpus):\n",
    "    iniciales = {}\n",
    "    total = 0\n",
    "    if corpus:\n",
    "        iniciales[corpus[0][1]] = 1\n",
    "        total = 1\n",
    "    for etiqueta in iniciales:\n",
    "        iniciales[etiqueta] /= total\n",
    "    return iniciales\n",
    "\n",
    "emision = calcular_probabilidades_emision(corpus)\n",
    "transicion = calcular_probabilidades_transicion(corpus)\n",
    "estado_inicial = calcular_probabilidades_iniciales(corpus)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 69,
   "id": "cbf97199",
   "metadata": {},
   "outputs": [],
   "source": [
    "def viterbi(frase, estados, transicion, emision, estado_inicial):\n",
    "    V = [{}]\n",
    "    path = {}\n",
    "\n",
    "    for estado in estados:\n",
    "        V[0][estado] = estado_inicial.get(estado, 1e-6) * emision.get(estado, {}).get(frase[0], 1e-6)\n",
    "        path[estado] = [estado]\n",
    "\n",
    "    for t in range(1, len(frase)):\n",
    "        V.append({})\n",
    "        new_path = {}\n",
    "        for y in estados:\n",
    "            (prob, estado_max) = max(\n",
    "                [(V[t-1][y0] * transicion.get(y0, {}).get(y, 1e-6) * emision.get(y, {}).get(frase[t], 1e-6), y0)\n",
    "                 for y0 in estados], key=lambda x: x[0])\n",
    "            V[t][y] = prob\n",
    "            new_path[y] = path[estado_max] + [y]\n",
    "        path = new_path\n",
    "\n",
    "    n = len(frase) - 1\n",
    "    (prob, estado_max) = max([(V[n][y], y) for y in estados], key=lambda x: x[0])\n",
    "    return (prob, path[estado_max])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 70,
   "id": "16dc22b6",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Frase: ['Habla', 'con', 'el', 'enfermo', 'grave', 'de', 'trasplantes', '.']\n",
      "Ruta m치s probable: ['VMIP3S0', 'SP', 'DA', 'NCMS000', 'AQ0CS00', 'SP', 'NCMN000', 'Fp']\n",
      "Probabilidad total: 1.8533917043090838e-08\n"
     ]
    }
   ],
   "source": [
    "frase_prueba = ['Habla', 'con', 'el', 'enfermo', 'grave', 'de', 'trasplantes', '.']\n",
    "estados = list(set([etiqueta for _, etiqueta in corpus]))\n",
    "\n",
    "probabilidad, ruta = viterbi(frase_prueba, estados, transicion, emision, estado_inicial)\n",
    "print(\"Frase:\", frase_prueba)\n",
    "print(\"Ruta m치s probable:\", ruta)\n",
    "print(\"Probabilidad total:\", probabilidad)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 71,
   "id": "1e2dd194",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "DataFrame de Emisi칩n:\n",
      "             Habla     habla     corre     canta    brilla    avanza  \\\n",
      "VMIP3S0   0.142857  0.142857  0.142857  0.142857  0.142857  0.142857   \n",
      "SP        0.000000  0.000000  0.000000  0.000000  0.000000  0.000000   \n",
      "DA        0.000000  0.000000  0.000000  0.000000  0.000000  0.000000   \n",
      "NCMS000   0.000000  0.000000  0.000000  0.000000  0.000000  0.000000   \n",
      "AQ0CS00   0.000000  0.000000  0.000000  0.000000  0.000000  0.000000   \n",
      "NCMN000   0.000000  0.000000  0.000000  0.000000  0.000000  0.000000   \n",
      "Fp        0.000000  0.000000  0.000000  0.000000  0.000000  0.000000   \n",
      "NCFS000   0.000000  0.000000  0.000000  0.000000  0.000000  0.000000   \n",
      "VAIP3S0   0.000000  0.000000  0.000000  0.000000  0.000000  0.000000   \n",
      "VMP00SF   0.000000  0.000000  0.000000  0.000000  0.000000  0.000000   \n",
      "SP+DA     0.000000  0.000000  0.000000  0.000000  0.000000  0.000000   \n",
      "NP00000   0.000000  0.000000  0.000000  0.000000  0.000000  0.000000   \n",
      "VSIP3S0   0.000000  0.000000  0.000000  0.000000  0.000000  0.000000   \n",
      "AQ0MS00   0.000000  0.000000  0.000000  0.000000  0.000000  0.000000   \n",
      "DI        0.000000  0.000000  0.000000  0.000000  0.000000  0.000000   \n",
      "AQ0FS00   0.000000  0.000000  0.000000  0.000000  0.000000  0.000000   \n",
      "VIIIS0S   0.000000  0.000000  0.000000  0.000000  0.000000  0.000000   \n",
      "PP3FS000  0.000000  0.000000  0.000000  0.000000  0.000000  0.000000   \n",
      "PP3MP000  0.000000  0.000000  0.000000  0.000000  0.000000  0.000000   \n",
      "VMIP3P0   0.000000  0.000000  0.000000  0.000000  0.000000  0.000000   \n",
      "RG        0.000000  0.000000  0.000000  0.000000  0.000000  0.000000   \n",
      "PP1MP000  0.000000  0.000000  0.000000  0.000000  0.000000  0.000000   \n",
      "VMIP1P0   0.000000  0.000000  0.000000  0.000000  0.000000  0.000000   \n",
      "VIIIS3S   0.000000  0.000000  0.000000  0.000000  0.000000  0.000000   \n",
      "VMIS3S0   0.000000  0.000000  0.000000  0.000000  0.000000  0.000000   \n",
      "PP3FP000  0.000000  0.000000  0.000000  0.000000  0.000000  0.000000   \n",
      "NCMP000   0.000000  0.000000  0.000000  0.000000  0.000000  0.000000   \n",
      "AQ0MP00   0.000000  0.000000  0.000000  0.000000  0.000000  0.000000   \n",
      "CC        0.000000  0.000000  0.000000  0.000000  0.000000  0.000000   \n",
      "\n",
      "             pinta       con        de        en  ...  miraba  cocinaba  \\\n",
      "VMIP3S0   0.142857  0.000000  0.000000  0.000000  ...     0.0       0.0   \n",
      "SP        0.000000  0.090909  0.272727  0.363636  ...     0.0       0.0   \n",
      "DA        0.000000  0.000000  0.000000  0.000000  ...     0.0       0.0   \n",
      "NCMS000   0.000000  0.000000  0.000000  0.000000  ...     0.0       0.0   \n",
      "AQ0CS00   0.000000  0.000000  0.000000  0.000000  ...     0.0       0.0   \n",
      "NCMN000   0.000000  0.000000  0.000000  0.000000  ...     0.0       0.0   \n",
      "Fp        0.000000  0.000000  0.000000  0.000000  ...     0.0       0.0   \n",
      "NCFS000   0.000000  0.000000  0.000000  0.000000  ...     0.0       0.0   \n",
      "VAIP3S0   0.000000  0.000000  0.000000  0.000000  ...     0.0       0.0   \n",
      "VMP00SF   0.000000  0.000000  0.000000  0.000000  ...     0.0       0.0   \n",
      "SP+DA     0.000000  0.000000  0.000000  0.000000  ...     0.0       0.0   \n",
      "NP00000   0.000000  0.000000  0.000000  0.000000  ...     0.0       0.0   \n",
      "VSIP3S0   0.000000  0.000000  0.000000  0.000000  ...     0.0       0.0   \n",
      "AQ0MS00   0.000000  0.000000  0.000000  0.000000  ...     0.0       0.0   \n",
      "DI        0.000000  0.000000  0.000000  0.000000  ...     0.0       0.0   \n",
      "AQ0FS00   0.000000  0.000000  0.000000  0.000000  ...     0.0       0.0   \n",
      "VIIIS0S   0.000000  0.000000  0.000000  0.000000  ...     0.0       0.0   \n",
      "PP3FS000  0.000000  0.000000  0.000000  0.000000  ...     0.0       0.0   \n",
      "PP3MP000  0.000000  0.000000  0.000000  0.000000  ...     0.0       0.0   \n",
      "VMIP3P0   0.000000  0.000000  0.000000  0.000000  ...     0.0       0.0   \n",
      "RG        0.000000  0.000000  0.000000  0.000000  ...     0.0       0.0   \n",
      "PP1MP000  0.000000  0.000000  0.000000  0.000000  ...     0.0       0.0   \n",
      "VMIP1P0   0.000000  0.000000  0.000000  0.000000  ...     0.0       0.0   \n",
      "VIIIS3S   0.000000  0.000000  0.000000  0.000000  ...     0.5       0.5   \n",
      "VMIS3S0   0.000000  0.000000  0.000000  0.000000  ...     0.0       0.0   \n",
      "PP3FP000  0.000000  0.000000  0.000000  0.000000  ...     0.0       0.0   \n",
      "NCMP000   0.000000  0.000000  0.000000  0.000000  ...     0.0       0.0   \n",
      "AQ0MP00   0.000000  0.000000  0.000000  0.000000  ...     0.0       0.0   \n",
      "CC        0.000000  0.000000  0.000000  0.000000  ...     0.0       0.0   \n",
      "\n",
      "          explic칩  llovi칩  Ellas  cuadros  pa칤ses  hermosos  diferentes    y  \n",
      "VMIP3S0       0.0     0.0    0.0      0.0     0.0       0.0         0.0  0.0  \n",
      "SP            0.0     0.0    0.0      0.0     0.0       0.0         0.0  0.0  \n",
      "DA            0.0     0.0    0.0      0.0     0.0       0.0         0.0  0.0  \n",
      "NCMS000       0.0     0.0    0.0      0.0     0.0       0.0         0.0  0.0  \n",
      "AQ0CS00       0.0     0.0    0.0      0.0     0.0       0.0         0.0  0.0  \n",
      "NCMN000       0.0     0.0    0.0      0.0     0.0       0.0         0.0  0.0  \n",
      "Fp            0.0     0.0    0.0      0.0     0.0       0.0         0.0  0.0  \n",
      "NCFS000       0.0     0.0    0.0      0.0     0.0       0.0         0.0  0.0  \n",
      "VAIP3S0       0.0     0.0    0.0      0.0     0.0       0.0         0.0  0.0  \n",
      "VMP00SF       0.0     0.0    0.0      0.0     0.0       0.0         0.0  0.0  \n",
      "SP+DA         0.0     0.0    0.0      0.0     0.0       0.0         0.0  0.0  \n",
      "NP00000       0.0     0.0    0.0      0.0     0.0       0.0         0.0  0.0  \n",
      "VSIP3S0       0.0     0.0    0.0      0.0     0.0       0.0         0.0  0.0  \n",
      "AQ0MS00       0.0     0.0    0.0      0.0     0.0       0.0         0.0  0.0  \n",
      "DI            0.0     0.0    0.0      0.0     0.0       0.0         0.0  0.0  \n",
      "AQ0FS00       0.0     0.0    0.0      0.0     0.0       0.0         0.0  0.0  \n",
      "VIIIS0S       0.0     0.0    0.0      0.0     0.0       0.0         0.0  0.0  \n",
      "PP3FS000      0.0     0.0    0.0      0.0     0.0       0.0         0.0  0.0  \n",
      "PP3MP000      0.0     0.0    0.0      0.0     0.0       0.0         0.0  0.0  \n",
      "VMIP3P0       0.0     0.0    0.0      0.0     0.0       0.0         0.0  0.0  \n",
      "RG            0.0     0.0    0.0      0.0     0.0       0.0         0.0  0.0  \n",
      "PP1MP000      0.0     0.0    0.0      0.0     0.0       0.0         0.0  0.0  \n",
      "VMIP1P0       0.0     0.0    0.0      0.0     0.0       0.0         0.0  0.0  \n",
      "VIIIS3S       0.0     0.0    0.0      0.0     0.0       0.0         0.0  0.0  \n",
      "VMIS3S0       0.5     0.5    0.0      0.0     0.0       0.0         0.0  0.0  \n",
      "PP3FP000      0.0     0.0    1.0      0.0     0.0       0.0         0.0  0.0  \n",
      "NCMP000       0.0     0.0    0.0      0.5     0.5       0.0         0.0  0.0  \n",
      "AQ0MP00       0.0     0.0    0.0      0.0     0.0       0.5         0.5  0.0  \n",
      "CC            0.0     0.0    0.0      0.0     0.0       0.0         0.0  1.0  \n",
      "\n",
      "[29 rows x 91 columns]\n",
      "\n",
      "==================================================\n",
      "\n"
     ]
    }
   ],
   "source": [
    "import pandas as pd\n",
    "\n",
    "# (Assuming 'emision', 'transicion', 'frase_prueba', and 'ruta' are defined as above or loaded elsewhere)\n",
    "\n",
    "# Create DataFrame for Emisi칩n (Emission Probabilities)\n",
    "# Rows will be words, columns will be tags\n",
    "df_emision = pd.DataFrame.from_dict(emision, orient='index').fillna(0)\n",
    "print(\"DataFrame de Emisi칩n:\")\n",
    "print(df_emision)\n",
    "print(\"\\n\" + \"=\"*50 + \"\\n\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 101,
   "id": "1fd08cd0",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "DataFrame de Transici칩n:\n",
      "                SP        DI        RG   NCMP000        DA   NCMN000  \\\n",
      "VMIP3S0   0.428571  0.142857  0.285714  0.142857  0.000000  0.000000   \n",
      "NCMS000   0.105263  0.052632  0.000000  0.000000  0.000000  0.000000   \n",
      "AQ0CS00   0.142857  0.000000  0.000000  0.000000  0.000000  0.000000   \n",
      "VMIP3P0   0.333333  0.000000  0.000000  0.000000  0.000000  0.000000   \n",
      "RG        0.750000  0.000000  0.000000  0.000000  0.000000  0.000000   \n",
      "VIIIS3S   0.500000  0.500000  0.000000  0.000000  0.000000  0.000000   \n",
      "Fp        0.000000  0.157895  0.052632  0.000000  0.315789  0.000000   \n",
      "VSIP3S0   0.000000  0.333333  0.000000  0.000000  0.000000  0.000000   \n",
      "VMIS3S0   0.000000  0.000000  0.500000  0.000000  0.500000  0.000000   \n",
      "AQ0MP00   0.000000  0.000000  0.000000  0.500000  0.000000  0.000000   \n",
      "SP        0.000000  0.000000  0.000000  0.000000  0.636364  0.181818   \n",
      "VIIIS0S   0.000000  0.000000  0.000000  0.000000  1.000000  0.000000   \n",
      "DA        0.000000  0.000000  0.000000  0.000000  0.000000  0.000000   \n",
      "SP+DA     0.000000  0.000000  0.000000  0.000000  0.000000  0.000000   \n",
      "DI        0.000000  0.000000  0.000000  0.000000  0.000000  0.000000   \n",
      "VMIP1P0   0.000000  0.000000  0.000000  0.000000  0.000000  0.000000   \n",
      "NCMP000   0.000000  0.000000  0.000000  0.000000  0.000000  0.000000   \n",
      "NCFS000   0.000000  0.000000  0.000000  0.000000  0.000000  0.000000   \n",
      "CC        0.000000  0.000000  0.000000  0.000000  0.000000  0.000000   \n",
      "AQ0MS00   0.000000  0.000000  0.000000  0.000000  0.000000  0.000000   \n",
      "PP3FS000  0.000000  0.000000  0.000000  0.000000  0.000000  0.000000   \n",
      "NCMN000   0.000000  0.000000  0.000000  0.000000  0.000000  0.000000   \n",
      "NP00000   0.000000  0.000000  0.000000  0.000000  0.000000  0.000000   \n",
      "VMP00SF   0.000000  0.000000  0.000000  0.000000  0.000000  0.000000   \n",
      "VAIP3S0   0.000000  0.000000  0.000000  0.000000  0.000000  0.000000   \n",
      "AQ0FS00   0.000000  0.000000  0.000000  0.000000  0.000000  0.000000   \n",
      "PP3MP000  0.000000  0.000000  0.000000  0.000000  0.000000  0.000000   \n",
      "PP3FP000  0.000000  0.000000  0.000000  0.000000  0.000000  0.000000   \n",
      "PP1MP000  0.000000  0.000000  0.000000  0.000000  0.000000  0.000000   \n",
      "\n",
      "           NCMS000   AQ0MP00   NCFS000   AQ0CS00  ...  PP3MP000  PP1MP000  \\\n",
      "VMIP3S0   0.000000  0.000000  0.000000  0.000000  ...  0.000000  0.000000   \n",
      "NCMS000   0.000000  0.000000  0.000000  0.263158  ...  0.000000  0.000000   \n",
      "AQ0CS00   0.000000  0.000000  0.000000  0.000000  ...  0.000000  0.000000   \n",
      "VMIP3P0   0.666667  0.000000  0.000000  0.000000  ...  0.000000  0.000000   \n",
      "RG        0.000000  0.000000  0.000000  0.000000  ...  0.000000  0.000000   \n",
      "VIIIS3S   0.000000  0.000000  0.000000  0.000000  ...  0.000000  0.000000   \n",
      "Fp        0.052632  0.000000  0.000000  0.000000  ...  0.105263  0.052632   \n",
      "VSIP3S0   0.333333  0.000000  0.000000  0.000000  ...  0.000000  0.000000   \n",
      "VMIS3S0   0.000000  0.000000  0.000000  0.000000  ...  0.000000  0.000000   \n",
      "AQ0MP00   0.000000  0.000000  0.000000  0.000000  ...  0.000000  0.000000   \n",
      "SP        0.090909  0.090909  0.000000  0.000000  ...  0.000000  0.000000   \n",
      "VIIIS0S   0.000000  0.000000  0.000000  0.000000  ...  0.000000  0.000000   \n",
      "DA        0.466667  0.000000  0.533333  0.000000  ...  0.000000  0.000000   \n",
      "SP+DA     0.500000  0.000000  0.000000  0.000000  ...  0.000000  0.000000   \n",
      "DI        0.714286  0.000000  0.285714  0.000000  ...  0.000000  0.000000   \n",
      "VMIP1P0   1.000000  0.000000  0.000000  0.000000  ...  0.000000  0.000000   \n",
      "NCMP000   0.000000  0.500000  0.000000  0.000000  ...  0.000000  0.000000   \n",
      "NCFS000   0.000000  0.000000  0.000000  0.100000  ...  0.000000  0.000000   \n",
      "CC        0.000000  0.000000  0.000000  1.000000  ...  0.000000  0.000000   \n",
      "AQ0MS00   0.000000  0.000000  0.000000  0.000000  ...  0.000000  0.000000   \n",
      "PP3FS000  0.000000  0.000000  0.000000  0.000000  ...  0.000000  0.000000   \n",
      "NCMN000   0.000000  0.000000  0.000000  0.000000  ...  0.000000  0.000000   \n",
      "NP00000   0.000000  0.000000  0.000000  0.000000  ...  0.000000  0.000000   \n",
      "VMP00SF   0.000000  0.000000  0.000000  0.000000  ...  0.000000  0.000000   \n",
      "VAIP3S0   0.000000  0.000000  0.000000  0.000000  ...  0.000000  0.000000   \n",
      "AQ0FS00   0.000000  0.000000  0.000000  0.000000  ...  0.000000  0.000000   \n",
      "PP3MP000  0.000000  0.000000  0.000000  0.000000  ...  0.000000  0.000000   \n",
      "PP3FP000  0.000000  0.000000  0.000000  0.000000  ...  0.000000  0.000000   \n",
      "PP1MP000  0.000000  0.000000  0.000000  0.000000  ...  0.000000  0.000000   \n",
      "\n",
      "          PP3FP000  VAIP3S0   AQ0FS00  VMP00SF  VIIIS0S   CC  VMIP3P0  VMIP1P0  \n",
      "VMIP3S0   0.000000      0.0  0.000000      0.0      0.0  0.0      0.0      0.0  \n",
      "NCMS000   0.000000      0.0  0.000000      0.0      0.0  0.0      0.0      0.0  \n",
      "AQ0CS00   0.000000      0.0  0.000000      0.0      0.0  0.0      0.0      0.0  \n",
      "VMIP3P0   0.000000      0.0  0.000000      0.0      0.0  0.0      0.0      0.0  \n",
      "RG        0.000000      0.0  0.000000      0.0      0.0  0.0      0.0      0.0  \n",
      "VIIIS3S   0.000000      0.0  0.000000      0.0      0.0  0.0      0.0      0.0  \n",
      "Fp        0.052632      0.0  0.000000      0.0      0.0  0.0      0.0      0.0  \n",
      "VSIP3S0   0.000000      0.0  0.333333      0.0      0.0  0.0      0.0      0.0  \n",
      "VMIS3S0   0.000000      0.0  0.000000      0.0      0.0  0.0      0.0      0.0  \n",
      "AQ0MP00   0.000000      0.0  0.000000      0.0      0.0  0.0      0.0      0.0  \n",
      "SP        0.000000      0.0  0.000000      0.0      0.0  0.0      0.0      0.0  \n",
      "VIIIS0S   0.000000      0.0  0.000000      0.0      0.0  0.0      0.0      0.0  \n",
      "DA        0.000000      0.0  0.000000      0.0      0.0  0.0      0.0      0.0  \n",
      "SP+DA     0.000000      0.0  0.000000      0.0      0.0  0.0      0.0      0.0  \n",
      "DI        0.000000      0.0  0.000000      0.0      0.0  0.0      0.0      0.0  \n",
      "VMIP1P0   0.000000      0.0  0.000000      0.0      0.0  0.0      0.0      0.0  \n",
      "NCMP000   0.000000      0.0  0.000000      0.0      0.0  0.0      0.0      0.0  \n",
      "NCFS000   0.000000      0.1  0.100000      0.0      0.0  0.0      0.0      0.0  \n",
      "CC        0.000000      0.0  0.000000      0.0      0.0  0.0      0.0      0.0  \n",
      "AQ0MS00   0.000000      0.0  0.000000      0.0      0.0  0.0      0.0      0.0  \n",
      "PP3FS000  0.000000      0.0  0.000000      0.0      0.0  0.0      0.0      0.0  \n",
      "NCMN000   0.000000      0.0  0.000000      0.0      0.0  0.0      0.0      0.0  \n",
      "NP00000   0.000000      0.0  0.000000      0.0      0.0  0.0      0.0      0.0  \n",
      "VMP00SF   0.000000      0.0  0.000000      0.0      0.0  0.0      0.0      0.0  \n",
      "VAIP3S0   0.000000      0.0  0.000000      1.0      0.0  0.0      0.0      0.0  \n",
      "AQ0FS00   0.000000      0.0  0.000000      0.0      0.5  0.5      0.0      0.0  \n",
      "PP3MP000  0.000000      0.0  0.000000      0.0      0.0  0.0      1.0      0.0  \n",
      "PP3FP000  0.000000      0.0  0.000000      0.0      0.0  0.0      1.0      0.0  \n",
      "PP1MP000  0.000000      0.0  0.000000      0.0      0.0  0.0      0.0      1.0  \n",
      "\n",
      "[29 rows x 29 columns]\n",
      "\n",
      "==================================================\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# Create DataFrame for Transici칩n (Transition Probabilities)\n",
    "# Rows will be current tags, columns will be next tags\n",
    "df_transicion = pd.DataFrame.from_dict(transicion, orient='index').fillna(0)\n",
    "print(\"DataFrame de Transici칩n:\")\n",
    "print(df_transicion)\n",
    "print(\"\\n\" + \"=\"*50 + \"\\n\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 104,
   "id": "7ef67f22",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "DataFrame Viterbi (Palabra y Etiqueta):\n",
      "       Palabra Etiqueta\n",
      "0        Habla  VMIP3S0\n",
      "1          con       SP\n",
      "2           el       DA\n",
      "3      enfermo  NCMS000\n",
      "4        grave  AQ0CS00\n",
      "5           de       SP\n",
      "6  trasplantes  NCMN000\n",
      "7            .       Fp\n"
     ]
    }
   ],
   "source": [
    "\n",
    "# Create DataFrame for Viterbi result\n",
    "# Shows the input phrase words and their predicted tags\n",
    "df_viterbi = pd.DataFrame({\n",
    "    \"Palabra\": frase_prueba,\n",
    "    \"Etiqueta\": ruta\n",
    "})\n",
    "print(\"DataFrame Viterbi (Palabra y Etiqueta):\")\n",
    "print(df_viterbi)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 74,
   "id": "be605a77",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<a href='tabla_emision.xlsx' target='_blank'>tabla_emision.xlsx</a><br>"
      ],
      "text/plain": [
       "i:\\unirIA\\procesamientoLenguaje\\senseval\\tabla_emision.xlsx"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "<a href='tabla_transicion.xlsx' target='_blank'>tabla_transicion.xlsx</a><br>"
      ],
      "text/plain": [
       "i:\\unirIA\\procesamientoLenguaje\\senseval\\tabla_transicion.xlsx"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "<a href='ruta_viterbi.xlsx' target='_blank'>ruta_viterbi.xlsx</a><br>"
      ],
      "text/plain": [
       "i:\\unirIA\\procesamientoLenguaje\\senseval\\ruta_viterbi.xlsx"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "df_emision.to_excel(\"tabla_emision.xlsx\", engine='openpyxl')\n",
    "df_transicion.to_excel(\"tabla_transicion.xlsx\", engine='openpyxl')\n",
    "df_viterbi.to_excel(\"ruta_viterbi.xlsx\", engine='openpyxl')\n",
    "\n",
    "from IPython.display import FileLink, display\n",
    "display(FileLink(\"tabla_emision.xlsx\"))\n",
    "display(FileLink(\"tabla_transicion.xlsx\"))\n",
    "display(FileLink(\"ruta_viterbi.xlsx\"))"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
