{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 120,
   "id": "90259aa6",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[nltk_data] Downloading package senseval to C:\\Users\\Aqui\n",
      "[nltk_data]     Creamos\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]   Package senseval is already up-to-date!\n",
      "[nltk_data] Downloading package punkt to C:\\Users\\Aqui\n",
      "[nltk_data]     Creamos\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]   Package punkt is already up-to-date!\n",
      "[nltk_data] Downloading package stopwords to C:\\Users\\Aqui\n",
      "[nltk_data]     Creamos\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]   Package stopwords is already up-to-date!\n",
      "[nltk_data] Downloading package wordnet to C:\\Users\\Aqui\n",
      "[nltk_data]     Creamos\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]   Package wordnet is already up-to-date!\n",
      "[nltk_data] Downloading package averaged_perceptron_tagger to\n",
      "[nltk_data]     C:\\Users\\Aqui Creamos\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]   Package averaged_perceptron_tagger is already up-to-\n",
      "[nltk_data]       date!\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "True"
      ]
     },
     "execution_count": 120,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# 🤓 Iniciamos con la configuración y extracción de las librerías senseval, punk stopwords wordnet,\n",
    "import os\n",
    "import nltk\n",
    "from nltk.corpus import senseval\n",
    "from nltk.corpus import stopwords\n",
    "from nltk.probability import FreqDist\n",
    "import string\n",
    "from collections import Counter\n",
    "import re\n",
    "import string\n",
    "\n",
    "nltk.download('senseval') # Para el corpus Senseval 2\n",
    "nltk.download('punkt')    # Para tokenización\n",
    "nltk.download('stopwords') # Para palabras vacías (stop words)\n",
    "nltk.download('wordnet')  # Aunque no se use directamente para los sentidos, es útil para entender WordNet.\n",
    "nltk.download('averaged_perceptron_tagger') # Para etiquetado POS si se necesita, aunque Senseval ya viene con ello."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 121,
   "id": "c4bccbf3",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['hard.pos', 'interest.pos', 'line.pos', 'serve.pos']"
      ]
     },
     "execution_count": 121,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Listamos todas las instancias\n",
    "senseval.fileids()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 122,
   "id": "f093af6c",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Total de instancias con sentidos: 4333\n"
     ]
    }
   ],
   "source": [
    "hard_instances = senseval.instances('hard.pos')\n",
    "hard_instances = [instance for instance in hard_instances if instance.senses]  # Filtrar instancias con sentidos\n",
    "print(f\"Total de instancias con sentidos: {len(hard_instances)}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 51,
   "id": "5afa72ae",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Archivo 'corpus_etiquetado.txt' cargado exitosamente.\n"
     ]
    }
   ],
   "source": [
    "file_path = 'corpus_etiquetado.txt'\n",
    "\n",
    "try:\n",
    "    with open(file_path, 'r', encoding='utf-8') as f:\n",
    "        corpus_content = f.read()\n",
    "    print(f\"Archivo '{file_path}' cargado exitosamente.\")\n",
    "except FileNotFoundError:\n",
    "    print(f\"Error: El archivo '{file_path}' no se encontró. Asegúrate de que está en la misma carpeta que tu script.\")\n",
    "except Exception as e:\n",
    "    print(f\"Ocurrió un error al leer el archivo: {e}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4dd18ee8",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "## Procesa el contenido del corpus para extraer pares (palabra, etiqueta).\n",
    "## Maneja etiquetas de documento, líneas vacías y el formato específico.\n",
    "def parse_corpus_content(content):\n",
    "    corpus = []\n",
    "    \n",
    "    for linea in content.splitlines():\n",
    "        linea = linea.strip()\n",
    "        \n",
    "        # Saltamos etiquetas de documento y líneas vacías.\n",
    "        if linea.startswith(\"<doc\") or linea.startswith(\"</doc>\") or linea == \"\":\n",
    "            continue\n",
    "        \n",
    "        # Saltar 'Fp' si aparece en una línea separada (inconsistencia del corpus).\n",
    "        if linea == \"Fp\":\n",
    "            continue\n",
    "        \n",
    "        datos = linea.split(\"\\t\")\n",
    "        \n",
    "        # Formato esperado: palabra \\t lema \\t etiqueta\n",
    "        if len(datos) == 3:\n",
    "            word = datos[0]\n",
    "            tag = datos[2]\n",
    "            corpus.append((word, tag))\n",
    "        # Caso específico: . \\t . \\t Fp\n",
    "        elif len(datos) == 2 and datos[0] == '.' and datos[1] == 'Fp':\n",
    "            word = datos[0]\n",
    "            tag = datos[1] \n",
    "            corpus.append((word, tag))\n",
    "        # Las líneas que no encajan se omiten.\n",
    "        else:\n",
    "            pass \n",
    "\n",
    "    return corpus\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 109,
   "id": "7f3718ab",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[('Habla', 'VMIP3S0'),\n",
       " ('con', 'SP'),\n",
       " ('el', 'DA'),\n",
       " ('enfermo', 'NCMS000'),\n",
       " ('grave', 'AQ0CS00'),\n",
       " ('de', 'SP'),\n",
       " ('trasplantes', 'NCMN000'),\n",
       " ('.', 'Fp'),\n",
       " ('El', 'DA'),\n",
       " ('enfermo', 'NCMS000'),\n",
       " ('grave', 'AQ0CS00'),\n",
       " ('habla', 'VMIP3S0'),\n",
       " ('de', 'SP'),\n",
       " ('trasplantes', 'NCMN000'),\n",
       " ('.', 'Fp'),\n",
       " ('La', 'DA'),\n",
       " ('película', 'NCFS000'),\n",
       " ('fue', 'VAIP3S0'),\n",
       " ('nominada', 'VMP00SF'),\n",
       " ('al', 'SP+DA'),\n",
       " ('Oscar', 'NP00000'),\n",
       " ('.', 'Fp'),\n",
       " ('Luis', 'NP00000'),\n",
       " ('Buñuel', 'NP00000'),\n",
       " ('es', 'VSIP3S0'),\n",
       " ('director', 'NCMS000'),\n",
       " ('español', 'AQ0MS00'),\n",
       " ('.', 'Fp'),\n",
       " ('El', 'DA'),\n",
       " ('niño', 'NCMS000'),\n",
       " ('corre', 'VMIP3S0'),\n",
       " ('en', 'SP'),\n",
       " ('el', 'DA'),\n",
       " ('parque', 'NCMS000'),\n",
       " ('.', 'Fp'),\n",
       " ('Una', 'DI'),\n",
       " ('flor', 'NCFS000'),\n",
       " ('roja', 'AQ0FS00'),\n",
       " ('decoraba', 'VIIIS0S'),\n",
       " ('la', 'DA'),\n",
       " ('mesa', 'NCFS000'),\n",
       " ('.', 'Fp'),\n",
       " ('Ella', 'PP3FS000'),\n",
       " ('canta', 'VMIP3S0'),\n",
       " ('una', 'DI'),\n",
       " ('canción', 'NCFS000'),\n",
       " ('alegre', 'AQ0CS00'),\n",
       " ('.', 'Fp'),\n",
       " ('Ellos', 'PP3MP000'),\n",
       " ('juegan', 'VMIP3P0'),\n",
       " ('fútbol', 'NCMS000'),\n",
       " ('cada', 'DI'),\n",
       " ('día', 'NCMS000'),\n",
       " ('.', 'Fp'),\n",
       " ('Hoy', 'NCMS000'),\n",
       " ('es', 'VSIP3S0'),\n",
       " ('un', 'DI'),\n",
       " ('día', 'NCMS000'),\n",
       " ('especial', 'AQ0CS00'),\n",
       " ('.', 'Fp'),\n",
       " ('El', 'DA'),\n",
       " ('sol', 'NCMS000'),\n",
       " ('brilla', 'VMIP3S0'),\n",
       " ('intensamente', 'RG'),\n",
       " ('en', 'SP'),\n",
       " ('el', 'DA'),\n",
       " ('cielo', 'NCMS000'),\n",
       " ('.', 'Fp'),\n",
       " ('Nosotros', 'PP1MP000'),\n",
       " ('estudiamos', 'VMIP1P0'),\n",
       " ('procesamiento', 'NCMS000'),\n",
       " ('del', 'SP+DA'),\n",
       " ('lenguaje', 'NCMS000'),\n",
       " ('natural', 'AQ0CS00'),\n",
       " ('.', 'Fp'),\n",
       " ('Un', 'DI'),\n",
       " ('gato', 'NCMS000'),\n",
       " ('gris', 'AQ0CS00'),\n",
       " ('miraba', 'VIIIS3S'),\n",
       " ('por', 'SP'),\n",
       " ('la', 'DA'),\n",
       " ('ventana', 'NCFS000'),\n",
       " ('.', 'Fp'),\n",
       " ('El', 'DA'),\n",
       " ('profesor', 'NCMS000'),\n",
       " ('explicó', 'VMIS3S0'),\n",
       " ('la', 'DA'),\n",
       " ('tarea', 'NCFS000'),\n",
       " ('.', 'Fp'),\n",
       " ('María', 'NP00000'),\n",
       " ('cocinaba', 'VIIIS3S'),\n",
       " ('un', 'DI'),\n",
       " ('pastel', 'NCMS000'),\n",
       " ('de', 'SP'),\n",
       " ('chocolate', 'NCMS000'),\n",
       " ('.', 'Fp'),\n",
       " ('Ayer', 'RG'),\n",
       " ('llovió', 'VMIS3S0'),\n",
       " ('fuertemente', 'RG'),\n",
       " ('en', 'SP'),\n",
       " ('la', 'DA'),\n",
       " ('ciudad', 'NCFS000'),\n",
       " ('.', 'Fp'),\n",
       " ('Ellas', 'PP3FP000'),\n",
       " ('estudian', 'VMIP3P0'),\n",
       " ('arte', 'NCMS000'),\n",
       " ('en', 'SP'),\n",
       " ('la', 'DA'),\n",
       " ('universidad', 'NCFS000'),\n",
       " ('.', 'Fp'),\n",
       " ('Un', 'DI'),\n",
       " ('coche', 'NCMS000'),\n",
       " ('rojo', 'AQ0MS00'),\n",
       " ('avanza', 'VMIP3S0'),\n",
       " ('rápido', 'RG'),\n",
       " ('por', 'SP'),\n",
       " ('la', 'DA'),\n",
       " ('carretera', 'NCFS000'),\n",
       " ('.', 'Fp'),\n",
       " ('Ella', 'PP3FS000'),\n",
       " ('pinta', 'VMIP3S0'),\n",
       " ('cuadros', 'NCMP000'),\n",
       " ('hermosos', 'AQ0MP00'),\n",
       " ('.', 'Fp'),\n",
       " ('Ellos', 'PP3MP000'),\n",
       " ('viajan', 'VMIP3P0'),\n",
       " ('a', 'SP'),\n",
       " ('diferentes', 'AQ0MP00'),\n",
       " ('países', 'NCMP000'),\n",
       " ('.', 'Fp'),\n",
       " ('La', 'DA'),\n",
       " ('tarde', 'NCFS000'),\n",
       " ('es', 'VSIP3S0'),\n",
       " ('tranquila', 'AQ0FS00'),\n",
       " ('y', 'CC'),\n",
       " ('agradable', 'AQ0CS00'),\n",
       " ('.', 'Fp')]"
      ]
     },
     "execution_count": 109,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "parse_corpus_content(corpus_content) # procesamos el contenido del corpus de manera organizada\n",
    "# Procesamos el contenido del corpus para extraer pares (palabra, etiqueta)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 110,
   "id": "9ba99169",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Corpus cargado. Número de tokens: 137\n",
      "Primeros 5 registros: [('Habla', 'VMIP3S0'), ('con', 'SP'), ('el', 'DA'), ('enfermo', 'NCMS000'), ('grave', 'AQ0CS00')]\n"
     ]
    }
   ],
   "source": [
    "corpus = parse_corpus_content(corpus_content)\n",
    "print(\"Corpus cargado. Número de tokens:\", len(corpus))\n",
    "print(\"Primeros 5 registros:\", corpus[:5])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 111,
   "id": "44f75ccd",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Cargando instancias de 'hard' desde Senseval...\n",
      "Total Senseval 'hard' instances with senses: 4333\n"
     ]
    }
   ],
   "source": [
    "# Load Senseval instances for the word \"hard\"\n",
    "hard_instances = []\n",
    "try:\n",
    "    # Load Senseval instances for the word \"hard\"\n",
    "    print(\"Cargando instancias de 'hard' desde Senseval...\")\n",
    "    hard_instances = senseval.instances('hard.pos')\n",
    "    # Filter instances to ensure they have senses, as some might not\n",
    "    hard_instances = [instance for instance in hard_instances if instance.senses]\n",
    "    print(f\"Total Senseval 'hard' instances with senses: {len(hard_instances)}\")\n",
    "except Exception as e:\n",
    "    print(f\"Could not load 'hard.pos' from senseval, skipping 'hard' analysis for senses: {e}\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 112,
   "id": "02658947",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Total Senseval 'serve' instances with senses: 4378\n"
     ]
    }
   ],
   "source": [
    "# Inicializamos las instancias de \"serve\" de Senseval\n",
    "serve_instances = []\n",
    "try:\n",
    "    # Load instances for \"serve\" from Senseval\n",
    "    serve_instances = senseval.instances('serve.pos')\n",
    "    # Filter instances to ensure they have senses, as some might not\n",
    "    serve_instances = [instance for instance in serve_instances if instance.senses]\n",
    "    print(f\"Total Senseval 'serve' instances with senses: {len(serve_instances)}\")\n",
    "except Exception as e:\n",
    "    print(f\"Could not load 'serve.pos' from senseval, skipping 'serve' analysis for senses: {e}\")\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 113,
   "id": "faafb0e3",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "--- 1. Posibles sentidos y etiquetas para 'hard and 'serve' ---\n",
      "'hard' has 3 possible senses: {'HARD2', 'HARD1', 'HARD3'}\n",
      "'serve' has 4 possible senses: {'SERVE6', 'SERVE10', 'SERVE2', 'SERVE12'}\n"
     ]
    }
   ],
   "source": [
    "print(\"--- 1. Posibles sentidos y etiquetas para 'hard and 'serve' ---\")\n",
    "\n",
    "hard_senses = set()\n",
    "for instance in hard_instances:\n",
    "    for sense in instance.senses:\n",
    "        hard_senses.add(sense)\n",
    "print(f\"'hard' has {len(hard_senses)} possible senses: {hard_senses}\")\n",
    "\n",
    "serve_senses = set()\n",
    "for instance in serve_instances:\n",
    "    for sense in instance.senses:\n",
    "        serve_senses.add(sense)\n",
    "print(f\"'serve' has {len(serve_senses)} possible senses: {serve_senses}\")\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 114,
   "id": "396358d7",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "--- 2. Numero de instancias para cada sentido ---\n",
      "\n",
      "'hard' sense counts:\n",
      "  Sense 'HARD1': 3455 instances\n",
      "  Sense 'HARD2': 502 instances\n",
      "  Sense 'HARD3': 376 instances\n",
      "\n",
      "'serve' sense counts:\n",
      "  Sense 'SERVE10': 1814 instances\n",
      "  Sense 'SERVE12': 1272 instances\n",
      "  Sense 'SERVE2': 853 instances\n",
      "  Sense 'SERVE6': 439 instances\n"
     ]
    }
   ],
   "source": [
    "print(\"\\n--- 2. Numero de instancias para cada sentido ---\")\n",
    "\n",
    "hard_sense_counts = {}\n",
    "for instance in hard_instances:\n",
    "    for sense in instance.senses:\n",
    "        hard_sense_counts[sense] = hard_sense_counts.get(sense, 0) + 1\n",
    "print(\"\\n'hard' sense counts:\")\n",
    "for sense, count in hard_sense_counts.items():\n",
    "    print(f\"  Sense '{sense}': {count} instances\")\n",
    "\n",
    "serve_sense_counts = {}\n",
    "for instance in serve_instances:\n",
    "    for sense in instance.senses:\n",
    "        serve_sense_counts[sense] = serve_sense_counts.get(sense, 0) + 1\n",
    "print(\"\\n'serve' sense counts:\")\n",
    "for sense, count in serve_sense_counts.items():\n",
    "    print(f\"  Sense '{sense}': {count} instances\")\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 115,
   "id": "20a2e34b",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "--- 3. Grammatical forms for 'hard' and 'serve' ---\n",
      "Formatos gramaticales en 'hard' encontrados: {'hardest', 'harder', 'hard'}\n",
      "Formatos gramaticales en 'server' encontrados: {'serves', 'server', 'serve', 'served'}\n"
     ]
    }
   ],
   "source": [
    "# 3. Grammatical forms for \"hard\" and \"serve\"\n",
    "print(\"--- 3. Grammatical forms for 'hard' and 'serve' ---\")\n",
    "\n",
    "# For 'hard'\n",
    "hard_forms = set()\n",
    "for instance in hard_instances:\n",
    "    # instance.context might contain (word, tag) tuples or just words.\n",
    "    # We'll try to handle both cases by checking the type of 'item'\n",
    "    for item in instance.context:\n",
    "        word_to_check = ''\n",
    "        if isinstance(item, tuple):\n",
    "            word_to_check = item[0]  # Get the word from the tuple\n",
    "        elif isinstance(item, str):\n",
    "            word_to_check = item   # 'item' is already the word string\n",
    "        \n",
    "        if word_to_check: # Ensure it's not empty\n",
    "            if word_to_check.lower() == 'hard' or word_to_check.lower() == 'harder' or word_to_check.lower() == 'hardest':\n",
    "                hard_forms.add(word_to_check.lower())\n",
    "print(f\"Formatos gramaticales en 'hard' encontrados: {hard_forms}\")\n",
    "\n",
    "# For 'serve'\n",
    "serve_forms = set()\n",
    "for instance in serve_instances:\n",
    "    for item in instance.context:\n",
    "        word_to_check = ''\n",
    "        if isinstance(item, tuple):\n",
    "            word_to_check = item[0]\n",
    "        elif isinstance(item, str):\n",
    "            word_to_check = item\n",
    "            \n",
    "        if word_to_check:\n",
    "            if word_to_check.lower().startswith('serve'): # Catch serve, serves, served, serving, etc.\n",
    "                serve_forms.add(word_to_check.lower())\n",
    "print(f\"Formatos gramaticales en 'server' encontrados: {serve_forms}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 116,
   "id": "b46a868a",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "SensevalInstance(word='hard-a', position=10, context=[('clever', 'NNP'), ('white', 'NNP'), ('house', 'NNP'), ('``', '``'), ('spin', 'VB'), ('doctors', 'NNS'), (\"''\", \"''\"), ('are', 'VBP'), ('having', 'VBG'), ('a', 'DT'), ('hard', 'JJ'), ('time', 'NN'), ('helping', 'VBG'), ('president', 'NNP'), ('bush', 'NNP'), ('explain', 'VB'), ('away', 'RB'), ('the', 'DT'), ('economic', 'JJ'), ('bashing', 'NN'), ('that', 'IN'), ('low-and', 'JJ'), ('middle-income', 'JJ'), ('workers', 'NNS'), ('are', 'VBP'), ('taking', 'VBG'), ('these', 'DT'), ('days', 'NNS'), ('.', '.')], senses=('HARD1',))\n"
     ]
    }
   ],
   "source": [
    "inst = hard_instances[1]\n",
    "print(inst)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 117,
   "id": "ea503a91",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[('clever', 'NNP'),\n",
       " ('white', 'NNP'),\n",
       " ('house', 'NNP'),\n",
       " ('``', '``'),\n",
       " ('spin', 'VB'),\n",
       " ('doctors', 'NNS'),\n",
       " (\"''\", \"''\"),\n",
       " ('are', 'VBP'),\n",
       " ('having', 'VBG'),\n",
       " ('a', 'DT'),\n",
       " ('hard', 'JJ'),\n",
       " ('time', 'NN'),\n",
       " ('helping', 'VBG'),\n",
       " ('president', 'NNP'),\n",
       " ('bush', 'NNP'),\n",
       " ('explain', 'VB'),\n",
       " ('away', 'RB'),\n",
       " ('the', 'DT'),\n",
       " ('economic', 'JJ'),\n",
       " ('bashing', 'NN'),\n",
       " ('that', 'IN'),\n",
       " ('low-and', 'JJ'),\n",
       " ('middle-income', 'JJ'),\n",
       " ('workers', 'NNS'),\n",
       " ('are', 'VBP'),\n",
       " ('taking', 'VBG'),\n",
       " ('these', 'DT'),\n",
       " ('days', 'NNS'),\n",
       " ('.', '.')]"
      ]
     },
     "execution_count": 117,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "inst.context"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 118,
   "id": "dcf67e20",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'find', 'get', 'make', 'time', 'work', 'would'}"
      ]
     },
     "execution_count": 118,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "vocab = ['time', 'would', 'get', 'work', 'find', 'make']\n",
    "vocab = set(vocab) \n",
    "vocab# Convert to set for faster lookup"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 119,
   "id": "d8b46dac",
   "metadata": {},
   "outputs": [],
   "source": [
    "class Instance:\n",
    "    def __init__(self, context, ambiguous_word_index):\n",
    "        self.context = context\n",
    "        self.ambiguous_word_index = ambiguous_word_index # Índice de la palabra ambigua en el contexto\n",
    "\n",
    "# Ejemplo de contexto para 'hard'\n",
    "# Asumimos que 'hard' es la palabra ambigua y está en alguna posición\n",
    "# Para simplificar, aquí el contexto es una lista de palabras.\n",
    "hard_instance_context = vocab"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "db784e42",
   "metadata": {},
   "source": [
    "## Extrae un vector de características binario basado en la presencia de palabras del vocabulario\n",
    "    en el contexto dado.\n",
    "\n",
    "    + Args:\n",
    "        contexto (list): Una lista de palabras que forman el contexto de la palabra ambigua.\n",
    "                         Puede contener tuplas (palabra, tag) o solo palabras.\n",
    "        vocabulario (list): Una lista de palabras que conforman el vocabulario de características.\n",
    "\n",
    "    + Returns:\n",
    "        list: Un vector binario donde cada elemento indica la presencia (1) o ausencia (0)\n",
    "              de la palabra correspondiente del vocabulario en el contexto.\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 64,
   "id": "82045073",
   "metadata": {},
   "outputs": [],
   "source": [
    "def extraer_caracteristicas_palabras_vecinas(contexto, vocabulario):\n",
    "\n",
    "    vector_caracteristicas = []\n",
    "    contexto_palabras_lower = []\n",
    "\n",
    "    # Normalizar el contexto a solo palabras en minúsculas\n",
    "    for item in contexto:\n",
    "        word_to_add = ''\n",
    "        if isinstance(item, tuple):\n",
    "            word_to_add = item[0].lower()\n",
    "        elif isinstance(item, str):\n",
    "            word_to_add = item.lower()\n",
    "        if word_to_add:\n",
    "            contexto_palabras_lower.append(word_to_add)\n",
    "\n",
    "    # Construir el vector de características\n",
    "    for palabra_vocabulario in vocabulario:\n",
    "        if palabra_vocabulario.lower() in contexto_palabras_lower:\n",
    "            vector_caracteristicas.append(1)\n",
    "        else:\n",
    "            vector_caracteristicas.append(0)\n",
    "            \n",
    "    return vector_caracteristicas"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a4179bf7",
   "metadata": {},
   "source": [
    "# Construcción del vocabulario o bags of words.\n",
    "\"Este vector de características indica que en el contexto de la palabra\n",
    "ambigua aparece la palabra «time» y no aparecen las palabras\n",
    "«would», «get», «work», «find» y «make».\"\n",
    "Esto se traduciría a [1, 0, 0, 0, 0, 0] para el vocabulario dado.\n",
    "Si en el contexto_hard las palabras \"would\", \"get\", \"work\", \"find\", \"make\" no estuvieran presentes,\n",
    "el resultado sería exactamente [1, 0, 0, 0, 0, 0].\n",
    "En el contexto_hard de ejemplo, las palabras \"would\", \"get\", \"work\", \"find\", \"make\" sí aparecen.\n",
    "Por lo tanto, el vector resultante sería:\n",
    "Palabra 'time': 1 (aparece)\n",
    "Palabra 'would': 1 (aparece)\n",
    "Palabra 'get': 1 (aparece)\n",
    "Palabra 'work': 1 (aparece)\n",
    "Palabra 'find': 1 (aparece)\n",
    "Palabra 'make': 1 (aparece)\n",
    "--> [1, 1, 1, 1, 1, 1]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "97926c98",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Vocabulario: {'get', 'make', 'time', 'would', 'find', 'work'}\n",
      "Contexto de 'hard': este vector de características indica que en el contexto de la palabra ambigua aparece la palabra time y no aparecen las palabras would get work find y make\n",
      "Vector de características para 'hard': [1, 1, 1, 1, 1, 1]\n",
      "Contexto de 'hard' (ejemplo solicitado): este es un ejemplo con time pero sin would get work find make\n",
      "Vector de características para 'hard' (ejemplo solicitado): [1, 1, 1, 1, 1, 1]\n"
     ]
    }
   ],
   "source": [
    "    # Contexto de la instancia de \"hard\"\n",
    "# Simulamos un contexto donde \"time\" aparece y el resto no.\n",
    "contexto_hard = [\"este\", \"vector\", \"de\", \"características\", \"indica\", \"que\", \"en\", \"el\", \"contexto\", \"de\", \"la\", \"palabra\",\n",
    "                 \"ambigua\", \"aparece\", \"la\", \"palabra\", \"time\", \"y\", \"no\", \"aparecen\", \"las\", \"palabras\",\n",
    "                 \"would\", \"get\", \"work\", \"find\", \"y\", \"make\"]\n",
    "\n",
    "# Extraer el vector de características\n",
    "vector_caracteristicas_hard = extraer_caracteristicas_palabras_vecinas(contexto_hard, vocab)\n",
    "\n",
    "print(f\"Vocabulario: {vocab}\")\n",
    "print(f\"Contexto de 'hard': {' '.join(contexto_hard)}\")\n",
    "print(f\"Vector de características para 'hard': {vector_caracteristicas_hard}\")\n",
    "\n",
    "\n",
    "\n",
    "# Si el contexto_hard fuera el que describiste:\n",
    "contexto_hard_solicitado = [\"este\", \"es\", \"un\", \"ejemplo\", \"con\", \"time\", \"pero\", \"sin\", \"would\", \"get\", \"work\", \"find\", \"make\"]\n",
    "vector_caracteristicas_hard_solicitado = extraer_caracteristicas_palabras_vecinas(contexto_hard_solicitado, vocab)\n",
    "print(f\"Contexto de 'hard' (ejemplo solicitado): {' '.join(contexto_hard_solicitado)}\")\n",
    "print(f\"Vector de características para 'hard' (ejemplo solicitado): {vector_caracteristicas_hard_solicitado}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a21ecfb9",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[<__main__.Instance at 0x20bbca44610>,\n",
       " <__main__.Instance at 0x20bbca47b90>,\n",
       " <__main__.Instance at 0x20bbca45990>,\n",
       " <__main__.Instance at 0x20bbca440d0>,\n",
       " <__main__.Instance at 0x20bbca47350>,\n",
       " <__main__.Instance at 0x20bbca47b50>,\n",
       " <__main__.Instance at 0x20bbca46990>,\n",
       " <__main__.Instance at 0x20bbca45110>,\n",
       " <__main__.Instance at 0x20bbca45310>,\n",
       " <__main__.Instance at 0x20bbca44110>,\n",
       " <__main__.Instance at 0x20bbca41150>,\n",
       " <__main__.Instance at 0x20bbca42910>]"
      ]
     },
     "execution_count": 81,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Asegúrate de haber descargado los recursos necesarios de NLTK\n",
    "try:\n",
    "    nltk.data.find('corpora/stopwords')\n",
    "except nltk.downloader.DownloadError:\n",
    "    nltk.download('stopwords')\n",
    "try:\n",
    "    nltk.data.find('tokenizers/punkt')\n",
    "except nltk.downloader.DownloadError:\n",
    "    nltk.download('punkt')\n",
    "\n",
    "\n",
    "# Simulación de las instancias del corpus\n",
    "# En un escenario real, 'hard_instances' y 'serve_instances'\n",
    "# vendrían de tu corpus cargado.\n",
    "class Instance:\n",
    "    def __init__(self, context, ambiguous_word):\n",
    "        self.context = context # Lista de palabras o (palabra, tag) tuplas\n",
    "        self.ambiguous_word = ambiguous_word # La palabra ambigua asociada a esta instancia\n",
    "\n",
    "        \n",
    "# Ejemplos de instancias (simuladas para 'hard' y 'serve')\n",
    "# Aquí usaremos solo strings en el contexto para simplificar, pero el código manejará tuplas también.\n",
    "hard_instances = [\n",
    "    Instance([\"It\", \"was\", \"a\", \"hard\", \"time\", \"for\", \"everyone\", \"involved\", \".\"], \"hard\"),\n",
    "    Instance([\"He\", \"worked\", \"hard\", \"to\", \"get\", \"the\", \"job\", \"done\", \".\"], \"hard\"),\n",
    "    Instance([\"The\", \"decision\", \"was\", \"harder\", \"than\", \"I\", \"thought\", \".\"], \"hard\"),\n",
    "    Instance([\"This\", \"is\", \"the\", \"hardest\", \"challenge\", \"I've\", \"faced\", \".\"], \"hard\"),\n",
    "    Instance([\"You\", \"would\", \"find\", \"it\", \"hard\", \"to\", \"make\", \"a\", \"living\", \".\"], \"hard\"),\n",
    "    Instance([\"It's\", \"hard\", \"work\", \"but\", \"it's\", \"rewarding\", \".\"], \"hard\"),\n",
    "    Instance([\"They\", \"get\", \"through\", \"hard\", \"times\", \".\"], \"hard\"),\n",
    "    Instance([\"It's\", \"hard\", \"to\", \"get\", \"enough\", \"sleep\", \".\"], \"hard\"),\n",
    "]\n",
    "\n",
    "serve_instances = [\n",
    "    Instance([\"He\", \"will\", \"serve\", \"his\", \"country\", \".\"], \"serve\"),\n",
    "    Instance([\"The\", \"waiter\", \"serves\", \"food\", \".\"], \"serve\"),\n",
    "    Instance([\"She\", \"served\", \"in\", \"the\", \"military\", \".\"], \"serve\"),\n",
    "    Instance([\"Serving\", \"the\", \"community\", \"is\", \"important\", \".\"], \"serve\"),\n",
    "]\n",
    "\n",
    "# Recopilar todas las instancias en una sola lista para procesar todo el corpus\n",
    "all_instances = hard_instances + serve_instances\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 83,
   "id": "bdf14f46",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Total de palabras a excluir: 268\n"
     ]
    }
   ],
   "source": [
    "# Stopwords y otros signos a excluir\n",
    "OTHER_WORDS = [\"''\",\"'d\",\"'ll\",\"'m\",\"'re\",\"'s\",\"'t\",\"'ve\",'--','000','1','2','3','4','5','6','8','10','15','30','I','F','``',\n",
    "               'also',\"don'\",'n','one','said','say','says','u','us']\n",
    "\n",
    "# Combinar stopwords de NLTK, puntuación y OTHER_WORDS\n",
    "STOPWORDS_SET = set(stopwords.words('english')).union(set(string.punctuation), set(OTHER_WORDS))\n",
    "\n",
    "# Identificar las formas gramaticales de tus palabras ambiguas\n",
    "# Aquí es donde debes añadir las formas que identificaste en tu laboratorio.\n",
    "# Por ejemplo, para 'hard': hard, harder, hardest\n",
    "# Para 'serve': serve, serves, served, serving, etc.\n",
    "# Basado en tu código anterior:\n",
    "hard_forms = {'hard', 'harder', 'hardest'}\n",
    "serve_forms = set()\n",
    "for instance in serve_instances:\n",
    "    for item in instance.context:\n",
    "        word_to_check = ''\n",
    "        if isinstance(item, tuple):\n",
    "            word_to_check = item[0]\n",
    "        elif isinstance(item, str):\n",
    "            word_to_check = item\n",
    "        if word_to_check and word_to_check.lower().startswith('serve'):\n",
    "            serve_forms.add(word_to_check.lower())\n",
    "\n",
    "# Combinar todas las palabras a excluir\n",
    "WORDS_TO_EXCLUDE = STOPWORDS_SET.union(hard_forms).union(serve_forms)\n",
    "\n",
    "print(f\"Total de palabras a excluir: {len(WORDS_TO_EXCLUDE)}\")\n",
    "# print(f\"Ejemplo de palabras a excluir: {list(WORDS_TO_EXCLUDE)[:20]}\") # Descomenta para ver algunas"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 90,
   "id": "8081eeed",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Distribución de frecuencias inicial (top 10): [('.', 12), ('hard', 6), ('the', 6), ('to', 3), ('get', 3), (\"it's\", 3), ('it', 2), ('was', 2), ('a', 2), ('he', 2)]\n"
     ]
    }
   ],
   "source": [
    "all_words = []\n",
    "for instance in all_instances:\n",
    "    for item in instance.context:\n",
    "        word = ''\n",
    "        if isinstance(item, tuple):\n",
    "            word = item[0]\n",
    "        elif isinstance(item, str):\n",
    "            word = item\n",
    "        \n",
    "        # Procesar solo si la palabra no está vacía y no es una forma gramatical de la palabra ambigua (inicialmente)\n",
    "        # La exclusión de otras palabras se hará después de FreqDist\n",
    "        if word:\n",
    "            all_words.append(word.lower())\n",
    "\n",
    "# Calcular la distribución de frecuencias de todas las palabras\n",
    "freq_dist = FreqDist(all_words)\n",
    "\n",
    "print(f\"\\nDistribución de frecuencias inicial (top 10): {freq_dist.most_common(10)}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 91,
   "id": "d008f6be",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "--- Vocabulario Construido ---\n",
      "Número de palabras más frecuentes (m): 6\n",
      "Vocabulario final: ['get', 'time', 'everyone', 'involved', 'worked', 'job']\n"
     ]
    }
   ],
   "source": [
    "def construir_vocabulario_frecuente(freq_dist, words_to_exclude, m):\n",
    "    \"\"\"\n",
    "    Construye un vocabulario con las m palabras más frecuentes,\n",
    "    excluyendo las palabras especificadas.\n",
    "\n",
    "    Args:\n",
    "        freq_dist (nltk.probability.FreqDist): Distribución de frecuencias de las palabras.\n",
    "        words_to_exclude (set): Conjunto de palabras (en minúsculas) a excluir.\n",
    "        m (int): El número de palabras más frecuentes que queremos en el vocabulario.\n",
    "\n",
    "    Returns:\n",
    "        list: Una lista de las m palabras más frecuentes y filtradas, que formarán el vocabulario.\n",
    "    \"\"\"\n",
    "    filtered_vocab = []\n",
    "    \n",
    "    # Itera sobre las palabras más comunes en orden descendente de frecuencia\n",
    "    for word, frequency in freq_dist.most_common():\n",
    "        if word not in words_to_exclude:\n",
    "            filtered_vocab.append(word)\n",
    "        \n",
    "        # Detener una vez que hemos alcanzado el tamaño de vocabulario deseado (m)\n",
    "        if len(filtered_vocab) >= m:\n",
    "            break\n",
    "            \n",
    "    return filtered_vocab\n",
    "\n",
    "# Definir el tamaño del vocabulario deseado (m)\n",
    "m_palabras = 6 # Para el ejemplo de 'hard'\n",
    "\n",
    "# Construir el vocabulario final\n",
    "vocabulario_final = construir_vocabulario_frecuente(freq_dist, WORDS_TO_EXCLUDE, m_palabras)\n",
    "\n",
    "print(f\"\\n--- Vocabulario Construido ---\")\n",
    "print(f\"Número de palabras más frecuentes (m): {m_palabras}\")\n",
    "print(f\"Vocabulario final: {vocabulario_final}\")\n",
    "\n",
    "# Comprobación con el ejemplo de 'hard'\n",
    "# Si las instancias de 'hard' contienen las palabras 'time', 'would', 'get', 'work', 'find', 'make'\n",
    "# y estas son las 6 más frecuentes después del filtrado, entonces coincidirán.\n",
    "# Con mis datos simulados, el resultado será diferente al ejemplo dado inicialmente,\n",
    "# ya que el ejemplo de 'hard' tenía un vocabulario predefinido.\n",
    "# Para que coincida con ['time', 'would', 'get', 'work', 'find', 'make'],\n",
    "# esas palabras deberían ser las 6 más frecuentes en TU corpus real después de la limpieza."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 95,
   "id": "7eb83562",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Número de palabras en el vocabulario deseado (m): 6\n",
      "Vocabulario construido: ['time', 'get', 'find', 'work', 'make', 'everyone']\n"
     ]
    }
   ],
   "source": [
    "# nltk.download('stopwords')\n",
    "# nltk.download('punkt') # Para tokenización si la usas explícitamente\n",
    "\n",
    "# --- Definición de Stop Words y Palabras Adicionales a Eliminar ---\n",
    "# Tu conjunto de palabras a eliminar\n",
    "OTHER_WORDS = [\"''\", \"'d\", \"'ll\", \"'m\", \"'re\", \"'s\", \"'t\", \"'ve\", '--', '000', '1', '2', '3', '4', '5', '6', '8', '10', '15', '30', 'I', 'F', '``',\n",
    "               'also', \"don'\", 'n', 'one', 'said', 'say', 'says', 'u', 'us']\n",
    "\n",
    "# Conjunto combinado de stopwords de NLTK (inglés), puntuación y tus palabras adicionales\n",
    "# Nota: NLTK stopwords por defecto son en inglés. Si tu corpus está en español,\n",
    "# necesitarás cambiar 'english' a 'spanish' en 'stopwords.words()'.\n",
    "# Para este ejemplo, mantendré 'english' como en tu referencia, pero tenlo en cuenta.\n",
    "STOPWORDS_SET = set(stopwords.words('english')).union(set(string.punctuation), set(OTHER_WORDS))\n",
    "\n",
    "# Clase dummy para simular las instancias del corpus\n",
    "class Instance:\n",
    "    def __init__(self, context, ambiguous_word):\n",
    "        # context puede ser una lista de palabras o de tuplas (palabra, tag)\n",
    "        self.context = context\n",
    "        self.ambiguous_word = ambiguous_word\n",
    "\n",
    "def construir_vocabulario(instances, m, palabras_ambiguas_forms):\n",
    "    \"\"\"\n",
    "    Construye un vocabulario de las 'm' palabras más frecuentes del corpus,\n",
    "    excluyendo puntuación, stopwords y formas gramaticales de las palabras ambiguas.\n",
    "\n",
    "    Args:\n",
    "        instances (list): Lista de objetos Instance, cada uno con un contexto.\n",
    "        m (int): El número de palabras más frecuentes a incluir en el vocabulario.\n",
    "        palabras_ambiguas_forms (dict): Un diccionario donde la clave es la palabra ambigua\n",
    "                                        y el valor es un conjunto de sus formas gramaticales\n",
    "                                        a excluir (ej: {'hard': {'hard', 'harder', 'hardest'}}).\n",
    "\n",
    "    Returns:\n",
    "        list: El vocabulario construido (lista de palabras).\n",
    "    \"\"\"\n",
    "    all_words = []\n",
    "    \n",
    "    # Recopilar todas las palabras de todos los contextos\n",
    "    for instance in instances:\n",
    "        for item in instance.context:\n",
    "            word = ''\n",
    "            if isinstance(item, tuple):\n",
    "                word = item[0] # Extraer la palabra de la tupla (si hay POS tags)\n",
    "            elif isinstance(item, str):\n",
    "                word = item # La palabra ya es un string\n",
    "            \n",
    "            # Normalizar a minúsculas\n",
    "            word_lower = word.lower()\n",
    "            all_words.append(word_lower)\n",
    "            \n",
    "    # Usar FreqDist para calcular las frecuencias\n",
    "    fdist = nltk.FreqDist(all_words)\n",
    "    \n",
    "    # Filtrar palabras: eliminar puntuación, stopwords y formas de palabras ambiguas\n",
    "    # Creamos un conjunto de todas las palabras a excluir\n",
    "    words_to_exclude = set()\n",
    "    words_to_exclude.update(STOPWORDS_SET)\n",
    "    \n",
    "    for amb_word_forms in palabras_ambiguas_forms.values():\n",
    "        words_to_exclude.update(amb_word_forms)\n",
    "\n",
    "    filtered_words_freq = Counter()\n",
    "    for word, freq in fdist.items():\n",
    "        if word not in words_to_exclude:\n",
    "            filtered_words_freq[word] = freq\n",
    "            \n",
    "    # Obtener las 'm' palabras más frecuentes\n",
    "    vocabulario = [word for word, freq in filtered_words_freq.most_common(m)]\n",
    "    \n",
    "    return vocabulario\n",
    "\n",
    "# --- Ejemplo de Uso ---\n",
    "\n",
    "# 1. Simular datos del corpus (tus instancias de \"hard\" y \"serve\")\n",
    "# Debes reemplazar esto con tus datos reales (hard_instances, serve_instances)\n",
    "# Ejemplo de contextos\n",
    "corpus_instances = [\n",
    "    Instance(context=[\"this\", \"is\", \"a\", \"hard\", \"time\", \"for\", \"everyone\", \"and\", \"we\", \"will\", \"get\", \"through\", \"it\"], ambiguous_word=\"hard\"),\n",
    "    Instance(context=[\"it's\", \"hard\", \"to\", \"find\", \"a\", \"job\", \"these\", \"days\"], ambiguous_word=\"hard\"),\n",
    "    Instance(context=[\"the\", \"machine\", \"will\", \"serve\", \"you\", \"well\"], ambiguous_word=\"serve\"),\n",
    "    Instance(context=[\"they\", \"serve\", \"delicious\", \"food\", \"at\", \"that\", \"workplace\"], ambiguous_word=\"serve\"),\n",
    "    Instance(context=[\"a\", \"time\", \"to\", \"work\", \"harder\", \"than\", \"ever\"], ambiguous_word=\"hard\"),\n",
    "    Instance(context=[\"he\", \"would\", \"like\", \"to\", \"make\", \"a\", \"difference\"], ambiguous_word=\"would\"), # Una instancia extra para 'would'\n",
    "    Instance(context=[\"you\", \"should\", \"get\", \"up\", \"and\", \"make\", \"it\", \"happen\"], ambiguous_word=\"get\"), # Instancia para 'get', 'make'\n",
    "    Instance(context=[\"find\", \"your\", \"own\", \"way\", \"to\", \"work\", \"it\", \"out\"], ambiguous_word=\"find\") # Instancia para 'find', 'work'\n",
    "]\n",
    "\n",
    "# 2. Definir las formas gramaticales de las palabras ambiguas\n",
    "# Aquí es donde añadirías las formas que identificaste en la Parte 1\n",
    "palabras_ambiguas_forms = {\n",
    "    'hard': {'hard', 'harder', 'hardest'},\n",
    "    'serve': {'serve', 'serves', 'served', 'serving'} # Ejemplos para 'serve'\n",
    "}\n",
    "\n",
    "# 3. Construir el vocabulario con un tamaño 'm' (por ejemplo, m=6)\n",
    "m_size = 6\n",
    "vocab_construido = construir_vocabulario(corpus_instances, m_size, palabras_ambiguas_forms)\n",
    "\n",
    "print(f\"Número de palabras en el vocabulario deseado (m): {m_size}\")\n",
    "print(f\"Vocabulario construido: {vocab_construido}\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "87681088",
   "metadata": {},
   "source": [
    "\n",
    "---\n",
    "\n",
    "### Explicación del Código:\n",
    "\n",
    "1.  **Importaciones:** `nltk` para `FreqDist`, `stopwords` y `string` para la puntuación, `collections.Counter` (aunque `FreqDist` es más específico para este caso, `Counter` es una alternativa general para contar).\n",
    "2.  **`OTHER_WORDS` y `STOPWORDS_SET`:** Se mantienen tus definiciones para las palabras a excluir. Es crucial que si tu corpus es en español, cambies `stopwords.words('english')` a `stopwords.words('spanish')`.\n",
    "3.  **Clase `Instance` (Simulación):** Esta es una clase simplificada para demostrar cómo manejarías tus datos. En tu entorno real, `hard_instances` y `serve_instances` probablemente ya son listas de objetos con un atributo `context`.\n",
    "4.  **`construir_vocabulario(instances, m, palabras_ambiguas_forms)`:**\n",
    "    * **Recopilación de palabras:** Itera sobre todas las `instances` y sus `contextos`. Para cada palabra, maneja si viene como tupla `(palabra, tag)` o como cadena simple, y la convierte a minúsculas.\n",
    "    * **`nltk.FreqDist(all_words)`:** Calcula la frecuencia de cada palabra en la lista `all_words`.\n",
    "    * **`words_to_exclude`:** Se crea un conjunto combinado de todas las palabras que queremos eliminar: las stop words, la puntuación y *todas las formas gramaticales de tus palabras ambiguas* (ej: 'hard', 'harder', 'hardest', 'serve', 'serves', etc.).\n",
    "    * **Filtrado:** Se crea un nuevo `Counter` (`filtered_words_freq`) que solo incluye las palabras que *no* están en `words_to_exclude`.\n",
    "    * **`filtered_words_freq.most_common(m)`:** Este método es clave. Devuelve una lista de tuplas `(palabra, frecuencia)` de las `m` palabras más comunes del diccionario de frecuencias filtrado.\n",
    "    * **Construcción final del vocabulario:** Se extraen solo las palabras (el primer elemento de cada tupla) para formar el vocabulario final.\n",
    "5.  **`palabras_ambiguas_forms`:** Este diccionario es fundamental. **Debes poblarlo con las formas gramaticales específicas que identificaste para \"hard\", \"serve\" y cualquier otra palabra ambigua que estés analizando.**\n",
    "\n",
    "Este enfoque te permite crear un vocabulario robusto y relevante, que es un requisito fundamental para luego construir tus vectores de características.\n",
    "\n",
    "¿Hay alguna parte de este proceso que te gustaría explorar con más detalle, o alguna otra consideración para tu corpus?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 97,
   "id": "dc97d71c",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Vocabulario generado para m=6: ['time', 'get', 'find', 'work', 'make', 'everyone']\n",
      "---\n",
      "Instancia de ejemplo - Palabra ambigua: 'hard'\n",
      "Contexto (oración completa): this is a hard time for everyone and we will get through it\n",
      "Vector de características para la instancia (diccionario):\n",
      "{'contains(time)': True, 'contains(get)': True, 'contains(find)': False, 'contains(work)': False, 'contains(make)': False, 'contains(everyone)': True}\n",
      "---\n",
      "--- Ejemplo específico del enunciado ---\n",
      "Vocabulario de ejemplo del enunciado: ['time', 'would', 'get', 'work', 'find', 'make']\n",
      "Contexto del enunciado: este vector de características indica que en el contexto de la palabra ambigua aparece la palabra time y no aparecen las palabras would get work find y make\n",
      "Vector de características (si solo 'time' aparece en el contexto): {'contains(time)': True, 'contains(would)': False, 'contains(get)': False, 'contains(work)': False, 'contains(find)': False, 'contains(make)': False}\n"
     ]
    }
   ],
   "source": [
    "\n",
    "# Tu conjunto de palabras a eliminar (igual al anterior)\n",
    "OTHER_WORDS = [\"''\", \"'d\", \"'ll\", \"'m\", \"'re\", \"'s\", \"'t\", \"'ve\", '--', '000', '1', '2', '3', '4', '5', '6', '8', '10', '15', '30', 'I', 'F', '``',\n",
    "               'also', \"don'\", 'n', 'one', 'said', 'say', 'says', 'u', 'us']\n",
    "\n",
    "# Conjunto combinado de stopwords de NLTK (inglés), puntuación y tus palabras adicionales\n",
    "STOPWORDS_SET = set(stopwords.words('english')).union(set(string.punctuation), set(OTHER_WORDS))\n",
    "\n",
    "def construir_vocabulario(instances, m, palabras_ambiguas_forms):\n",
    "    \"\"\"\n",
    "    Construye un vocabulario de las 'm' palabras más frecuentes del corpus,\n",
    "    excluyendo puntuación, stopwords y formas gramaticales de las palabras ambiguas.\n",
    "    (Esta función es la misma que la proporcionada anteriormente)\n",
    "    \"\"\"\n",
    "    all_words = []\n",
    "    for instance in instances:\n",
    "        for item in instance.context:\n",
    "            word = ''\n",
    "            if isinstance(item, tuple):\n",
    "                word = item[0]\n",
    "            elif isinstance(item, str):\n",
    "                word = item\n",
    "            word_lower = word.lower()\n",
    "            all_words.append(word_lower)\n",
    "            \n",
    "    fdist = nltk.FreqDist(all_words)\n",
    "    \n",
    "    words_to_exclude = set()\n",
    "    words_to_exclude.update(STOPWORDS_SET)\n",
    "    \n",
    "    for amb_word_forms in palabras_ambiguas_forms.values():\n",
    "        words_to_exclude.update(amb_word_forms)\n",
    "\n",
    "    filtered_words_freq = Counter()\n",
    "    for word, freq in fdist.items():\n",
    "        if word not in words_to_exclude:\n",
    "            filtered_words_freq[word] = freq\n",
    "            \n",
    "    vocabulario = [word for word, freq in filtered_words_freq.most_common(m)]\n",
    "    \n",
    "    return vocabulario\n",
    "\n",
    "# --- Nueva función para extraer el vector de características como diccionario ---\n",
    "\n",
    "def extraer_caracteristicas_diccionario(instance_context, vocabulario):\n",
    "    \"\"\"\n",
    "    Extrae un diccionario de características binarias basado en la presencia\n",
    "    de palabras del vocabulario en el contexto dado.\n",
    "\n",
    "    Args:\n",
    "        instance_context (list): Una lista de palabras que forman la oración completa\n",
    "                                 donde aparece la palabra ambigua.\n",
    "                                 Puede contener tuplas (palabra, tag) o solo palabras.\n",
    "        vocabulario (list): Una lista de palabras que conforman el vocabulario de características.\n",
    "\n",
    "    Returns:\n",
    "        dict: Un diccionario donde la clave es 'contains(palabra)' y el valor es True/False.\n",
    "    \"\"\"\n",
    "    vector_caracteristicas = {}\n",
    "    contexto_palabras_lower = []\n",
    "\n",
    "    # Normalizar el contexto a solo palabras en minúsculas\n",
    "    for item in instance_context:\n",
    "        word_to_add = ''\n",
    "        if isinstance(item, tuple):\n",
    "            word_to_add = item[0]\n",
    "        elif isinstance(item, str):\n",
    "            word_to_add = item\n",
    "        \n",
    "        if word_to_add:\n",
    "            contexto_palabras_lower.append(word_to_add.lower())\n",
    "\n",
    "    # Construir el diccionario de características\n",
    "    for palabra_vocabulario in vocabulario:\n",
    "        clave = f'contains({palabra_vocabulario})'\n",
    "        if palabra_vocabulario.lower() in contexto_palabras_lower:\n",
    "            vector_caracteristicas[clave] = True\n",
    "        else:\n",
    "            vector_caracteristicas[clave] = False\n",
    "            \n",
    "    return vector_caracteristicas\n",
    "\n",
    "# --- Ejemplo Completo ---\n",
    "\n",
    "# 1. Simular datos del corpus (usando los mismos datos de ejemplo que antes)\n",
    "corpus_instances = [\n",
    "    Instance(context=[\"this\", \"is\", \"a\", \"hard\", \"time\", \"for\", \"everyone\", \"and\", \"we\", \"will\", \"get\", \"through\", \"it\"], ambiguous_word=\"hard\"),\n",
    "    Instance(context=[\"it's\", \"hard\", \"to\", \"find\", \"a\", \"job\", \"these\", \"days\"], ambiguous_word=\"hard\"),\n",
    "    Instance(context=[\"the\", \"machine\", \"will\", \"serve\", \"you\", \"well\"], ambiguous_word=\"serve\"),\n",
    "    Instance(context=[\"they\", \"serve\", \"delicious\", \"food\", \"at\", \"that\", \"workplace\"], ambiguous_word=\"serve\"),\n",
    "    Instance(context=[\"a\", \"time\", \"to\", \"work\", \"harder\", \"than\", \"ever\"], ambiguous_word=\"hard\"),\n",
    "    Instance(context=[\"he\", \"would\", \"like\", \"to\", \"make\", \"a\", \"difference\"], ambiguous_word=\"would\"),\n",
    "    Instance(context=[\"you\", \"should\", \"get\", \"up\", \"and\", \"make\", \"it\", \"happen\"], ambiguous_word=\"get\"),\n",
    "    Instance(context=[\"find\", \"your\", \"own\", \"way\", \"to\", \"work\", \"it\", \"out\"], ambiguous_word=\"find\")\n",
    "]\n",
    "\n",
    "# 2. Definir las formas gramaticales de las palabras ambiguas\n",
    "palabras_ambiguas_forms = {\n",
    "    'hard': {'hard', 'harder', 'hardest'},\n",
    "    'serve': {'serve', 'serves', 'served', 'serving'}\n",
    "}\n",
    "\n",
    "# 3. Construir el vocabulario (ejemplo con m=6)\n",
    "m_size = 6\n",
    "vocabulario_generado = construir_vocabulario(corpus_instances, m_size, palabras_ambiguas_forms)\n",
    "\n",
    "print(f\"Vocabulario generado para m={m_size}: {vocabulario_generado}\")\n",
    "print(\"---\")\n",
    "\n",
    "# 4. Mostrar el vector de características para una instancia del corpus\n",
    "# Tomemos la primera instancia como ejemplo:\n",
    "# \"this is a hard time for everyone and we will get through it\"\n",
    "\n",
    "instancia_ejemplo = corpus_instances[0]\n",
    "print(f\"Instancia de ejemplo - Palabra ambigua: '{instancia_ejemplo.ambiguous_word}'\")\n",
    "print(f\"Contexto (oración completa): {' '.join(instancia_ejemplo.context)}\")\n",
    "\n",
    "# Extraer el vector de características para esta instancia\n",
    "vector_caracteristicas_instancia = extraer_caracteristicas_diccionario(instancia_ejemplo.context, vocabulario_generado)\n",
    "\n",
    "print(f\"Vector de características para la instancia (diccionario):\\n{vector_caracteristicas_instancia}\")\n",
    "\n",
    "print(\"---\")\n",
    "\n",
    "# Para el ejemplo solicitado: {'contains(time)': True, 'contains(would)': False, ...}\n",
    "# Usaremos un vocabulario fijo para ilustrar ese caso específico.\n",
    "vocabulario_fijo_ejemplo = ['time', 'would', 'get', 'work', 'find', 'make']\n",
    "contexto_ejemplo_solicitado = [\"este\", \"vector\", \"de\", \"características\", \"indica\", \"que\", \"en\", \"el\", \"contexto\", \"de\", \"la\", \"palabra\",\n",
    "                               \"ambigua\", \"aparece\", \"la\", \"palabra\", \"time\", \"y\", \"no\", \"aparecen\", \"las\", \"palabras\",\n",
    "                               \"would\", \"get\", \"work\", \"find\", \"y\", \"make\"] # Aquí, todas aparecen en el ejemplo de texto.\n",
    "\n",
    "print(\"--- Ejemplo específico del enunciado ---\")\n",
    "print(f\"Vocabulario de ejemplo del enunciado: {vocabulario_fijo_ejemplo}\")\n",
    "print(f\"Contexto del enunciado: {' '.join(contexto_ejemplo_solicitado)}\")\n",
    "\n",
    "# Si el contexto del ejemplo del enunciado solo contuviera 'time' y NO el resto:\n",
    "contexto_solo_time = [\"este\", \"vector\", \"de\", \"características\", \"indica\", \"que\", \"en\", \"el\", \"contexto\", \"de\", \"la\", \"palabra\",\n",
    "                      \"ambigua\", \"aparece\", \"la\", \"palabra\", \"time\"]\n",
    "\n",
    "vector_caracteristicas_solicitado = extraer_caracteristicas_diccionario(contexto_solo_time, vocabulario_fijo_ejemplo)\n",
    "print(f\"Vector de características (si solo 'time' aparece en el contexto): {vector_caracteristicas_solicitado}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 98,
   "id": "f66a8e03",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[('``', '``'),\n",
       " ('it', 'PRP'),\n",
       " (\"'s\", 'VBZ'),\n",
       " ('a', 'DT'),\n",
       " ('very', 'RB'),\n",
       " ('interesting', 'JJ'),\n",
       " ('place', 'NN'),\n",
       " ('to', 'TO'),\n",
       " ('work', 'VB'),\n",
       " (',', ','),\n",
       " ('but', 'CC'),\n",
       " ('i', 'PRP'),\n",
       " ('can', 'MD'),\n",
       " ('see', 'VB'),\n",
       " ('why', 'WRB'),\n",
       " ('some', 'DT'),\n",
       " ('people', 'NNS'),\n",
       " ('have', 'VBP'),\n",
       " ('a', 'DT'),\n",
       " ('hard', 'JJ'),\n",
       " ('time', 'NN'),\n",
       " ('imagining', 'VBG'),\n",
       " ('what', 'WP'),\n",
       " ('it', 'PRP'),\n",
       " (\"'s\", 'VBZ'),\n",
       " ('like', 'IN'),\n",
       " (',', ','),\n",
       " ('\"', '\"'),\n",
       " ('says', 'VBZ'),\n",
       " ('nate', 'NNP'),\n",
       " ('gossett', 'NNP'),\n",
       " ('.', '.')]"
      ]
     },
     "execution_count": 98,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "senseval.instances('hard.pos')[2737].context"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 96,
   "id": "56c66c20",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "--- 4. Checking corpus_etiquetado.txt format consistency ---\n",
      "Por si se encuentra alguna inconsistencia 'corpus_etiquetado.txt':\n",
      "  1. Invalid tag format: .\t.\tFp (Doc ID: 1)\n",
      "  2. Invalid tag format: .\t.\tFp (Doc ID: 2)\n",
      "  3. Invalid tag format: .\t.\tFp (Doc ID: 3)\n",
      "  4. Invalid tag format: .\t.\tFp (Doc ID: 4)\n",
      "  5. Invalid tag format: .\t.\tFp (Doc ID: 5)\n",
      "  6. Invalid tag format: .\t.\tFp (Doc ID: 6)\n",
      "  7. Invalid tag format: .\t.\tFp (Doc ID: 7)\n",
      "  8. Invalid tag format: .\t.\tFp (Doc ID: 8)\n",
      "  9. Invalid tag format: .\t.\tFp (Doc ID: 9)\n",
      "  10. Invalid tag format: .\t.\tFp (Doc ID: 10)\n",
      "  ...and 10 more inconsistencies.\n",
      "\n",
      "Ejemplo como lo pide la guia con (word\\tlemma\\ttag):\n",
      "Habla\\thablar\\tVMIP3S0\n",
      "con\\tcon\\tSP\n",
      "el\\tel\\tDA\n"
     ]
    }
   ],
   "source": [
    "\n",
    "# Check if all instances in the provided corpus follow the format\n",
    "print(\"\\n--- 4. Checking corpus_etiquetado.txt format consistency ---\")\n",
    "\n",
    "if not inconsistent_lines:\n",
    "    print(\"Todas las inconsistencias en el modulo 'corpus_etiquetado.txt' que aparecen el  formato (word\\\\tlemma\\\\ttag).\")\n",
    "else:\n",
    "    print(\"Por si se encuentra alguna inconsistencia 'corpus_etiquetado.txt':\")\n",
    "    for i, problem in enumerate(inconsistent_lines[:10]): # Show first 10 examples\n",
    "        print(f\"  {i+1}. {problem}\")\n",
    "    if len(inconsistent_lines) > 10:\n",
    "        print(f\"  ...and {len(inconsistent_lines) - 10} more inconsistencies.\")\n",
    "\n",
    "# Example of a well-formatted line from your corpus for reference\n",
    "print(\"\\nEjemplo como lo pide la guia con (word\\\\tlemma\\\\ttag):\")\n",
    "print(\"Habla\\\\thablar\\\\tVMIP3S0\")\n",
    "print(\"con\\\\tcon\\\\tSP\")\n",
    "print(\"el\\\\tel\\\\tDA\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 68,
   "id": "fbe8b067",
   "metadata": {},
   "outputs": [],
   "source": [
    "def calcular_probabilidades_emision(corpus):\n",
    "    emision = {}\n",
    "    total_etiquetas = {}\n",
    "    for token, etiqueta in corpus:\n",
    "        if etiqueta not in emision:\n",
    "            emision[etiqueta] = {}\n",
    "            total_etiquetas[etiqueta] = 0\n",
    "        emision[etiqueta][token] = emision[etiqueta].get(token, 0) + 1\n",
    "        total_etiquetas[etiqueta] += 1\n",
    "    for etiqueta in emision:\n",
    "        for token in emision[etiqueta]:\n",
    "            emision[etiqueta][token] /= total_etiquetas[etiqueta]\n",
    "    return emision\n",
    "\n",
    "def calcular_probabilidades_transicion(corpus):\n",
    "    transicion = {}\n",
    "    total_transiciones = {}\n",
    "    anterior = None\n",
    "    for _, etiqueta in corpus:\n",
    "        if anterior is not None:\n",
    "            if anterior not in transicion:\n",
    "                transicion[anterior] = {}\n",
    "                total_transiciones[anterior] = 0\n",
    "            transicion[anterior][etiqueta] = transicion[anterior].get(etiqueta, 0) + 1\n",
    "            total_transiciones[anterior] += 1\n",
    "        anterior = etiqueta\n",
    "    for etiqueta in transicion:\n",
    "        for siguiente in transicion[etiqueta]:\n",
    "            transicion[etiqueta][siguiente] /= total_transiciones[etiqueta]\n",
    "    return transicion\n",
    "\n",
    "def calcular_probabilidades_iniciales(corpus):\n",
    "    iniciales = {}\n",
    "    total = 0\n",
    "    if corpus:\n",
    "        iniciales[corpus[0][1]] = 1\n",
    "        total = 1\n",
    "    for etiqueta in iniciales:\n",
    "        iniciales[etiqueta] /= total\n",
    "    return iniciales\n",
    "\n",
    "emision = calcular_probabilidades_emision(corpus)\n",
    "transicion = calcular_probabilidades_transicion(corpus)\n",
    "estado_inicial = calcular_probabilidades_iniciales(corpus)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 69,
   "id": "cbf97199",
   "metadata": {},
   "outputs": [],
   "source": [
    "def viterbi(frase, estados, transicion, emision, estado_inicial):\n",
    "    V = [{}]\n",
    "    path = {}\n",
    "\n",
    "    for estado in estados:\n",
    "        V[0][estado] = estado_inicial.get(estado, 1e-6) * emision.get(estado, {}).get(frase[0], 1e-6)\n",
    "        path[estado] = [estado]\n",
    "\n",
    "    for t in range(1, len(frase)):\n",
    "        V.append({})\n",
    "        new_path = {}\n",
    "        for y in estados:\n",
    "            (prob, estado_max) = max(\n",
    "                [(V[t-1][y0] * transicion.get(y0, {}).get(y, 1e-6) * emision.get(y, {}).get(frase[t], 1e-6), y0)\n",
    "                 for y0 in estados], key=lambda x: x[0])\n",
    "            V[t][y] = prob\n",
    "            new_path[y] = path[estado_max] + [y]\n",
    "        path = new_path\n",
    "\n",
    "    n = len(frase) - 1\n",
    "    (prob, estado_max) = max([(V[n][y], y) for y in estados], key=lambda x: x[0])\n",
    "    return (prob, path[estado_max])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 70,
   "id": "16dc22b6",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Frase: ['Habla', 'con', 'el', 'enfermo', 'grave', 'de', 'trasplantes', '.']\n",
      "Ruta más probable: ['VMIP3S0', 'SP', 'DA', 'NCMS000', 'AQ0CS00', 'SP', 'NCMN000', 'Fp']\n",
      "Probabilidad total: 1.8533917043090838e-08\n"
     ]
    }
   ],
   "source": [
    "frase_prueba = ['Habla', 'con', 'el', 'enfermo', 'grave', 'de', 'trasplantes', '.']\n",
    "estados = list(set([etiqueta for _, etiqueta in corpus]))\n",
    "\n",
    "probabilidad, ruta = viterbi(frase_prueba, estados, transicion, emision, estado_inicial)\n",
    "print(\"Frase:\", frase_prueba)\n",
    "print(\"Ruta más probable:\", ruta)\n",
    "print(\"Probabilidad total:\", probabilidad)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 71,
   "id": "1e2dd194",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "DataFrame de Emisión:\n",
      "             Habla     habla     corre     canta    brilla    avanza  \\\n",
      "VMIP3S0   0.142857  0.142857  0.142857  0.142857  0.142857  0.142857   \n",
      "SP        0.000000  0.000000  0.000000  0.000000  0.000000  0.000000   \n",
      "DA        0.000000  0.000000  0.000000  0.000000  0.000000  0.000000   \n",
      "NCMS000   0.000000  0.000000  0.000000  0.000000  0.000000  0.000000   \n",
      "AQ0CS00   0.000000  0.000000  0.000000  0.000000  0.000000  0.000000   \n",
      "NCMN000   0.000000  0.000000  0.000000  0.000000  0.000000  0.000000   \n",
      "Fp        0.000000  0.000000  0.000000  0.000000  0.000000  0.000000   \n",
      "NCFS000   0.000000  0.000000  0.000000  0.000000  0.000000  0.000000   \n",
      "VAIP3S0   0.000000  0.000000  0.000000  0.000000  0.000000  0.000000   \n",
      "VMP00SF   0.000000  0.000000  0.000000  0.000000  0.000000  0.000000   \n",
      "SP+DA     0.000000  0.000000  0.000000  0.000000  0.000000  0.000000   \n",
      "NP00000   0.000000  0.000000  0.000000  0.000000  0.000000  0.000000   \n",
      "VSIP3S0   0.000000  0.000000  0.000000  0.000000  0.000000  0.000000   \n",
      "AQ0MS00   0.000000  0.000000  0.000000  0.000000  0.000000  0.000000   \n",
      "DI        0.000000  0.000000  0.000000  0.000000  0.000000  0.000000   \n",
      "AQ0FS00   0.000000  0.000000  0.000000  0.000000  0.000000  0.000000   \n",
      "VIIIS0S   0.000000  0.000000  0.000000  0.000000  0.000000  0.000000   \n",
      "PP3FS000  0.000000  0.000000  0.000000  0.000000  0.000000  0.000000   \n",
      "PP3MP000  0.000000  0.000000  0.000000  0.000000  0.000000  0.000000   \n",
      "VMIP3P0   0.000000  0.000000  0.000000  0.000000  0.000000  0.000000   \n",
      "RG        0.000000  0.000000  0.000000  0.000000  0.000000  0.000000   \n",
      "PP1MP000  0.000000  0.000000  0.000000  0.000000  0.000000  0.000000   \n",
      "VMIP1P0   0.000000  0.000000  0.000000  0.000000  0.000000  0.000000   \n",
      "VIIIS3S   0.000000  0.000000  0.000000  0.000000  0.000000  0.000000   \n",
      "VMIS3S0   0.000000  0.000000  0.000000  0.000000  0.000000  0.000000   \n",
      "PP3FP000  0.000000  0.000000  0.000000  0.000000  0.000000  0.000000   \n",
      "NCMP000   0.000000  0.000000  0.000000  0.000000  0.000000  0.000000   \n",
      "AQ0MP00   0.000000  0.000000  0.000000  0.000000  0.000000  0.000000   \n",
      "CC        0.000000  0.000000  0.000000  0.000000  0.000000  0.000000   \n",
      "\n",
      "             pinta       con        de        en  ...  miraba  cocinaba  \\\n",
      "VMIP3S0   0.142857  0.000000  0.000000  0.000000  ...     0.0       0.0   \n",
      "SP        0.000000  0.090909  0.272727  0.363636  ...     0.0       0.0   \n",
      "DA        0.000000  0.000000  0.000000  0.000000  ...     0.0       0.0   \n",
      "NCMS000   0.000000  0.000000  0.000000  0.000000  ...     0.0       0.0   \n",
      "AQ0CS00   0.000000  0.000000  0.000000  0.000000  ...     0.0       0.0   \n",
      "NCMN000   0.000000  0.000000  0.000000  0.000000  ...     0.0       0.0   \n",
      "Fp        0.000000  0.000000  0.000000  0.000000  ...     0.0       0.0   \n",
      "NCFS000   0.000000  0.000000  0.000000  0.000000  ...     0.0       0.0   \n",
      "VAIP3S0   0.000000  0.000000  0.000000  0.000000  ...     0.0       0.0   \n",
      "VMP00SF   0.000000  0.000000  0.000000  0.000000  ...     0.0       0.0   \n",
      "SP+DA     0.000000  0.000000  0.000000  0.000000  ...     0.0       0.0   \n",
      "NP00000   0.000000  0.000000  0.000000  0.000000  ...     0.0       0.0   \n",
      "VSIP3S0   0.000000  0.000000  0.000000  0.000000  ...     0.0       0.0   \n",
      "AQ0MS00   0.000000  0.000000  0.000000  0.000000  ...     0.0       0.0   \n",
      "DI        0.000000  0.000000  0.000000  0.000000  ...     0.0       0.0   \n",
      "AQ0FS00   0.000000  0.000000  0.000000  0.000000  ...     0.0       0.0   \n",
      "VIIIS0S   0.000000  0.000000  0.000000  0.000000  ...     0.0       0.0   \n",
      "PP3FS000  0.000000  0.000000  0.000000  0.000000  ...     0.0       0.0   \n",
      "PP3MP000  0.000000  0.000000  0.000000  0.000000  ...     0.0       0.0   \n",
      "VMIP3P0   0.000000  0.000000  0.000000  0.000000  ...     0.0       0.0   \n",
      "RG        0.000000  0.000000  0.000000  0.000000  ...     0.0       0.0   \n",
      "PP1MP000  0.000000  0.000000  0.000000  0.000000  ...     0.0       0.0   \n",
      "VMIP1P0   0.000000  0.000000  0.000000  0.000000  ...     0.0       0.0   \n",
      "VIIIS3S   0.000000  0.000000  0.000000  0.000000  ...     0.5       0.5   \n",
      "VMIS3S0   0.000000  0.000000  0.000000  0.000000  ...     0.0       0.0   \n",
      "PP3FP000  0.000000  0.000000  0.000000  0.000000  ...     0.0       0.0   \n",
      "NCMP000   0.000000  0.000000  0.000000  0.000000  ...     0.0       0.0   \n",
      "AQ0MP00   0.000000  0.000000  0.000000  0.000000  ...     0.0       0.0   \n",
      "CC        0.000000  0.000000  0.000000  0.000000  ...     0.0       0.0   \n",
      "\n",
      "          explicó  llovió  Ellas  cuadros  países  hermosos  diferentes    y  \n",
      "VMIP3S0       0.0     0.0    0.0      0.0     0.0       0.0         0.0  0.0  \n",
      "SP            0.0     0.0    0.0      0.0     0.0       0.0         0.0  0.0  \n",
      "DA            0.0     0.0    0.0      0.0     0.0       0.0         0.0  0.0  \n",
      "NCMS000       0.0     0.0    0.0      0.0     0.0       0.0         0.0  0.0  \n",
      "AQ0CS00       0.0     0.0    0.0      0.0     0.0       0.0         0.0  0.0  \n",
      "NCMN000       0.0     0.0    0.0      0.0     0.0       0.0         0.0  0.0  \n",
      "Fp            0.0     0.0    0.0      0.0     0.0       0.0         0.0  0.0  \n",
      "NCFS000       0.0     0.0    0.0      0.0     0.0       0.0         0.0  0.0  \n",
      "VAIP3S0       0.0     0.0    0.0      0.0     0.0       0.0         0.0  0.0  \n",
      "VMP00SF       0.0     0.0    0.0      0.0     0.0       0.0         0.0  0.0  \n",
      "SP+DA         0.0     0.0    0.0      0.0     0.0       0.0         0.0  0.0  \n",
      "NP00000       0.0     0.0    0.0      0.0     0.0       0.0         0.0  0.0  \n",
      "VSIP3S0       0.0     0.0    0.0      0.0     0.0       0.0         0.0  0.0  \n",
      "AQ0MS00       0.0     0.0    0.0      0.0     0.0       0.0         0.0  0.0  \n",
      "DI            0.0     0.0    0.0      0.0     0.0       0.0         0.0  0.0  \n",
      "AQ0FS00       0.0     0.0    0.0      0.0     0.0       0.0         0.0  0.0  \n",
      "VIIIS0S       0.0     0.0    0.0      0.0     0.0       0.0         0.0  0.0  \n",
      "PP3FS000      0.0     0.0    0.0      0.0     0.0       0.0         0.0  0.0  \n",
      "PP3MP000      0.0     0.0    0.0      0.0     0.0       0.0         0.0  0.0  \n",
      "VMIP3P0       0.0     0.0    0.0      0.0     0.0       0.0         0.0  0.0  \n",
      "RG            0.0     0.0    0.0      0.0     0.0       0.0         0.0  0.0  \n",
      "PP1MP000      0.0     0.0    0.0      0.0     0.0       0.0         0.0  0.0  \n",
      "VMIP1P0       0.0     0.0    0.0      0.0     0.0       0.0         0.0  0.0  \n",
      "VIIIS3S       0.0     0.0    0.0      0.0     0.0       0.0         0.0  0.0  \n",
      "VMIS3S0       0.5     0.5    0.0      0.0     0.0       0.0         0.0  0.0  \n",
      "PP3FP000      0.0     0.0    1.0      0.0     0.0       0.0         0.0  0.0  \n",
      "NCMP000       0.0     0.0    0.0      0.5     0.5       0.0         0.0  0.0  \n",
      "AQ0MP00       0.0     0.0    0.0      0.0     0.0       0.5         0.5  0.0  \n",
      "CC            0.0     0.0    0.0      0.0     0.0       0.0         0.0  1.0  \n",
      "\n",
      "[29 rows x 91 columns]\n",
      "\n",
      "==================================================\n",
      "\n"
     ]
    }
   ],
   "source": [
    "import pandas as pd\n",
    "\n",
    "# (Assuming 'emision', 'transicion', 'frase_prueba', and 'ruta' are defined as above or loaded elsewhere)\n",
    "\n",
    "# Create DataFrame for Emisión (Emission Probabilities)\n",
    "# Rows will be words, columns will be tags\n",
    "df_emision = pd.DataFrame.from_dict(emision, orient='index').fillna(0)\n",
    "print(\"DataFrame de Emisión:\")\n",
    "print(df_emision)\n",
    "print(\"\\n\" + \"=\"*50 + \"\\n\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 101,
   "id": "1fd08cd0",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "DataFrame de Transición:\n",
      "                SP        DI        RG   NCMP000        DA   NCMN000  \\\n",
      "VMIP3S0   0.428571  0.142857  0.285714  0.142857  0.000000  0.000000   \n",
      "NCMS000   0.105263  0.052632  0.000000  0.000000  0.000000  0.000000   \n",
      "AQ0CS00   0.142857  0.000000  0.000000  0.000000  0.000000  0.000000   \n",
      "VMIP3P0   0.333333  0.000000  0.000000  0.000000  0.000000  0.000000   \n",
      "RG        0.750000  0.000000  0.000000  0.000000  0.000000  0.000000   \n",
      "VIIIS3S   0.500000  0.500000  0.000000  0.000000  0.000000  0.000000   \n",
      "Fp        0.000000  0.157895  0.052632  0.000000  0.315789  0.000000   \n",
      "VSIP3S0   0.000000  0.333333  0.000000  0.000000  0.000000  0.000000   \n",
      "VMIS3S0   0.000000  0.000000  0.500000  0.000000  0.500000  0.000000   \n",
      "AQ0MP00   0.000000  0.000000  0.000000  0.500000  0.000000  0.000000   \n",
      "SP        0.000000  0.000000  0.000000  0.000000  0.636364  0.181818   \n",
      "VIIIS0S   0.000000  0.000000  0.000000  0.000000  1.000000  0.000000   \n",
      "DA        0.000000  0.000000  0.000000  0.000000  0.000000  0.000000   \n",
      "SP+DA     0.000000  0.000000  0.000000  0.000000  0.000000  0.000000   \n",
      "DI        0.000000  0.000000  0.000000  0.000000  0.000000  0.000000   \n",
      "VMIP1P0   0.000000  0.000000  0.000000  0.000000  0.000000  0.000000   \n",
      "NCMP000   0.000000  0.000000  0.000000  0.000000  0.000000  0.000000   \n",
      "NCFS000   0.000000  0.000000  0.000000  0.000000  0.000000  0.000000   \n",
      "CC        0.000000  0.000000  0.000000  0.000000  0.000000  0.000000   \n",
      "AQ0MS00   0.000000  0.000000  0.000000  0.000000  0.000000  0.000000   \n",
      "PP3FS000  0.000000  0.000000  0.000000  0.000000  0.000000  0.000000   \n",
      "NCMN000   0.000000  0.000000  0.000000  0.000000  0.000000  0.000000   \n",
      "NP00000   0.000000  0.000000  0.000000  0.000000  0.000000  0.000000   \n",
      "VMP00SF   0.000000  0.000000  0.000000  0.000000  0.000000  0.000000   \n",
      "VAIP3S0   0.000000  0.000000  0.000000  0.000000  0.000000  0.000000   \n",
      "AQ0FS00   0.000000  0.000000  0.000000  0.000000  0.000000  0.000000   \n",
      "PP3MP000  0.000000  0.000000  0.000000  0.000000  0.000000  0.000000   \n",
      "PP3FP000  0.000000  0.000000  0.000000  0.000000  0.000000  0.000000   \n",
      "PP1MP000  0.000000  0.000000  0.000000  0.000000  0.000000  0.000000   \n",
      "\n",
      "           NCMS000   AQ0MP00   NCFS000   AQ0CS00  ...  PP3MP000  PP1MP000  \\\n",
      "VMIP3S0   0.000000  0.000000  0.000000  0.000000  ...  0.000000  0.000000   \n",
      "NCMS000   0.000000  0.000000  0.000000  0.263158  ...  0.000000  0.000000   \n",
      "AQ0CS00   0.000000  0.000000  0.000000  0.000000  ...  0.000000  0.000000   \n",
      "VMIP3P0   0.666667  0.000000  0.000000  0.000000  ...  0.000000  0.000000   \n",
      "RG        0.000000  0.000000  0.000000  0.000000  ...  0.000000  0.000000   \n",
      "VIIIS3S   0.000000  0.000000  0.000000  0.000000  ...  0.000000  0.000000   \n",
      "Fp        0.052632  0.000000  0.000000  0.000000  ...  0.105263  0.052632   \n",
      "VSIP3S0   0.333333  0.000000  0.000000  0.000000  ...  0.000000  0.000000   \n",
      "VMIS3S0   0.000000  0.000000  0.000000  0.000000  ...  0.000000  0.000000   \n",
      "AQ0MP00   0.000000  0.000000  0.000000  0.000000  ...  0.000000  0.000000   \n",
      "SP        0.090909  0.090909  0.000000  0.000000  ...  0.000000  0.000000   \n",
      "VIIIS0S   0.000000  0.000000  0.000000  0.000000  ...  0.000000  0.000000   \n",
      "DA        0.466667  0.000000  0.533333  0.000000  ...  0.000000  0.000000   \n",
      "SP+DA     0.500000  0.000000  0.000000  0.000000  ...  0.000000  0.000000   \n",
      "DI        0.714286  0.000000  0.285714  0.000000  ...  0.000000  0.000000   \n",
      "VMIP1P0   1.000000  0.000000  0.000000  0.000000  ...  0.000000  0.000000   \n",
      "NCMP000   0.000000  0.500000  0.000000  0.000000  ...  0.000000  0.000000   \n",
      "NCFS000   0.000000  0.000000  0.000000  0.100000  ...  0.000000  0.000000   \n",
      "CC        0.000000  0.000000  0.000000  1.000000  ...  0.000000  0.000000   \n",
      "AQ0MS00   0.000000  0.000000  0.000000  0.000000  ...  0.000000  0.000000   \n",
      "PP3FS000  0.000000  0.000000  0.000000  0.000000  ...  0.000000  0.000000   \n",
      "NCMN000   0.000000  0.000000  0.000000  0.000000  ...  0.000000  0.000000   \n",
      "NP00000   0.000000  0.000000  0.000000  0.000000  ...  0.000000  0.000000   \n",
      "VMP00SF   0.000000  0.000000  0.000000  0.000000  ...  0.000000  0.000000   \n",
      "VAIP3S0   0.000000  0.000000  0.000000  0.000000  ...  0.000000  0.000000   \n",
      "AQ0FS00   0.000000  0.000000  0.000000  0.000000  ...  0.000000  0.000000   \n",
      "PP3MP000  0.000000  0.000000  0.000000  0.000000  ...  0.000000  0.000000   \n",
      "PP3FP000  0.000000  0.000000  0.000000  0.000000  ...  0.000000  0.000000   \n",
      "PP1MP000  0.000000  0.000000  0.000000  0.000000  ...  0.000000  0.000000   \n",
      "\n",
      "          PP3FP000  VAIP3S0   AQ0FS00  VMP00SF  VIIIS0S   CC  VMIP3P0  VMIP1P0  \n",
      "VMIP3S0   0.000000      0.0  0.000000      0.0      0.0  0.0      0.0      0.0  \n",
      "NCMS000   0.000000      0.0  0.000000      0.0      0.0  0.0      0.0      0.0  \n",
      "AQ0CS00   0.000000      0.0  0.000000      0.0      0.0  0.0      0.0      0.0  \n",
      "VMIP3P0   0.000000      0.0  0.000000      0.0      0.0  0.0      0.0      0.0  \n",
      "RG        0.000000      0.0  0.000000      0.0      0.0  0.0      0.0      0.0  \n",
      "VIIIS3S   0.000000      0.0  0.000000      0.0      0.0  0.0      0.0      0.0  \n",
      "Fp        0.052632      0.0  0.000000      0.0      0.0  0.0      0.0      0.0  \n",
      "VSIP3S0   0.000000      0.0  0.333333      0.0      0.0  0.0      0.0      0.0  \n",
      "VMIS3S0   0.000000      0.0  0.000000      0.0      0.0  0.0      0.0      0.0  \n",
      "AQ0MP00   0.000000      0.0  0.000000      0.0      0.0  0.0      0.0      0.0  \n",
      "SP        0.000000      0.0  0.000000      0.0      0.0  0.0      0.0      0.0  \n",
      "VIIIS0S   0.000000      0.0  0.000000      0.0      0.0  0.0      0.0      0.0  \n",
      "DA        0.000000      0.0  0.000000      0.0      0.0  0.0      0.0      0.0  \n",
      "SP+DA     0.000000      0.0  0.000000      0.0      0.0  0.0      0.0      0.0  \n",
      "DI        0.000000      0.0  0.000000      0.0      0.0  0.0      0.0      0.0  \n",
      "VMIP1P0   0.000000      0.0  0.000000      0.0      0.0  0.0      0.0      0.0  \n",
      "NCMP000   0.000000      0.0  0.000000      0.0      0.0  0.0      0.0      0.0  \n",
      "NCFS000   0.000000      0.1  0.100000      0.0      0.0  0.0      0.0      0.0  \n",
      "CC        0.000000      0.0  0.000000      0.0      0.0  0.0      0.0      0.0  \n",
      "AQ0MS00   0.000000      0.0  0.000000      0.0      0.0  0.0      0.0      0.0  \n",
      "PP3FS000  0.000000      0.0  0.000000      0.0      0.0  0.0      0.0      0.0  \n",
      "NCMN000   0.000000      0.0  0.000000      0.0      0.0  0.0      0.0      0.0  \n",
      "NP00000   0.000000      0.0  0.000000      0.0      0.0  0.0      0.0      0.0  \n",
      "VMP00SF   0.000000      0.0  0.000000      0.0      0.0  0.0      0.0      0.0  \n",
      "VAIP3S0   0.000000      0.0  0.000000      1.0      0.0  0.0      0.0      0.0  \n",
      "AQ0FS00   0.000000      0.0  0.000000      0.0      0.5  0.5      0.0      0.0  \n",
      "PP3MP000  0.000000      0.0  0.000000      0.0      0.0  0.0      1.0      0.0  \n",
      "PP3FP000  0.000000      0.0  0.000000      0.0      0.0  0.0      1.0      0.0  \n",
      "PP1MP000  0.000000      0.0  0.000000      0.0      0.0  0.0      0.0      1.0  \n",
      "\n",
      "[29 rows x 29 columns]\n",
      "\n",
      "==================================================\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# Create DataFrame for Transición (Transition Probabilities)\n",
    "# Rows will be current tags, columns will be next tags\n",
    "df_transicion = pd.DataFrame.from_dict(transicion, orient='index').fillna(0)\n",
    "print(\"DataFrame de Transición:\")\n",
    "print(df_transicion)\n",
    "print(\"\\n\" + \"=\"*50 + \"\\n\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 104,
   "id": "7ef67f22",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "DataFrame Viterbi (Palabra y Etiqueta):\n",
      "       Palabra Etiqueta\n",
      "0        Habla  VMIP3S0\n",
      "1          con       SP\n",
      "2           el       DA\n",
      "3      enfermo  NCMS000\n",
      "4        grave  AQ0CS00\n",
      "5           de       SP\n",
      "6  trasplantes  NCMN000\n",
      "7            .       Fp\n"
     ]
    }
   ],
   "source": [
    "\n",
    "# Create DataFrame for Viterbi result\n",
    "# Shows the input phrase words and their predicted tags\n",
    "df_viterbi = pd.DataFrame({\n",
    "    \"Palabra\": frase_prueba,\n",
    "    \"Etiqueta\": ruta\n",
    "})\n",
    "print(\"DataFrame Viterbi (Palabra y Etiqueta):\")\n",
    "print(df_viterbi)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 74,
   "id": "be605a77",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<a href='tabla_emision.xlsx' target='_blank'>tabla_emision.xlsx</a><br>"
      ],
      "text/plain": [
       "i:\\unirIA\\procesamientoLenguaje\\senseval\\tabla_emision.xlsx"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "<a href='tabla_transicion.xlsx' target='_blank'>tabla_transicion.xlsx</a><br>"
      ],
      "text/plain": [
       "i:\\unirIA\\procesamientoLenguaje\\senseval\\tabla_transicion.xlsx"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "<a href='ruta_viterbi.xlsx' target='_blank'>ruta_viterbi.xlsx</a><br>"
      ],
      "text/plain": [
       "i:\\unirIA\\procesamientoLenguaje\\senseval\\ruta_viterbi.xlsx"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "df_emision.to_excel(\"tabla_emision.xlsx\", engine='openpyxl')\n",
    "df_transicion.to_excel(\"tabla_transicion.xlsx\", engine='openpyxl')\n",
    "df_viterbi.to_excel(\"ruta_viterbi.xlsx\", engine='openpyxl')\n",
    "\n",
    "from IPython.display import FileLink, display\n",
    "display(FileLink(\"tabla_emision.xlsx\"))\n",
    "display(FileLink(\"tabla_transicion.xlsx\"))\n",
    "display(FileLink(\"ruta_viterbi.xlsx\"))"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
