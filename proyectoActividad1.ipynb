{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "e9370b61",
   "metadata": {},
   "source": [
    "# Procesamiento del Lenguaje \n",
    "\n",
    "1. Darwin Yusef Gonzalez \n",
    "2. Orlando Silva \n",
    "3. Jose Barrios\n",
    "4. Ana Hernandez\n",
    "June 2025"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "93e021fc",
   "metadata": {},
   "source": [
    "## Laboratorio de Aprendizaje supervisado basado en desambiguar el sentido (senseval) de las palabras\n",
    "En este laboratorio Hemos implementado el uso de diferentes algoritmos basados en aprendizaje autom√°tico supervisado para desambiguar el sentido de las palabras en Python y utilizando la herramienta de software abierto Natural Language Toolkit (`NLTK`)."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "63c282ab",
   "metadata": {},
   "source": [
    "# Instalaci√≥n y configuraci√≥nes iniciales"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 347,
   "id": "90259aa6",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[nltk_data] Downloading package senseval to C:\\Users\\Aqui\n",
      "[nltk_data]     Creamos\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]   Package senseval is already up-to-date!\n",
      "[nltk_data] Downloading package punkt to C:\\Users\\Aqui\n",
      "[nltk_data]     Creamos\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]   Package punkt is already up-to-date!\n",
      "[nltk_data] Downloading package stopwords to C:\\Users\\Aqui\n",
      "[nltk_data]     Creamos\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]   Package stopwords is already up-to-date!\n",
      "[nltk_data] Downloading package wordnet to C:\\Users\\Aqui\n",
      "[nltk_data]     Creamos\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]   Package wordnet is already up-to-date!\n",
      "[nltk_data] Downloading package averaged_perceptron_tagger to\n",
      "[nltk_data]     C:\\Users\\Aqui Creamos\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]   Package averaged_perceptron_tagger is already up-to-\n",
      "[nltk_data]       date!\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "True"
      ]
     },
     "execution_count": 347,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# ü§ì Iniciamos con la configuraci√≥n y extracci√≥n de las librer√≠as senseval, punk stopwords wordnet,\n",
    "import os\n",
    "import nltk\n",
    "from nltk.corpus import senseval\n",
    "from nltk.corpus import stopwords\n",
    "from nltk.probability import FreqDist\n",
    "import string\n",
    "from collections import Counter\n",
    "import re\n",
    "import string\n",
    "\n",
    "nltk.download('senseval') # Para el corpus Senseval 2\n",
    "nltk.download('punkt')    # Para tokenizaci√≥n\n",
    "nltk.download('stopwords') # Para palabras vac√≠as (stop words)\n",
    "nltk.download('wordnet')  # Aunque no se use directamente para los sentidos, es √∫til para entender WordNet.\n",
    "nltk.download('averaged_perceptron_tagger') # Para etiquetado POS si se necesita, aunque Senseval ya viene con ello."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 348,
   "id": "c4bccbf3",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['hard.pos', 'interest.pos', 'line.pos', 'serve.pos']"
      ]
     },
     "execution_count": 348,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# ü§ì Listamos todas las instancias\n",
    "senseval.fileids()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 349,
   "id": "f093af6c",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Total de instancias con sentidos: 4333\n"
     ]
    }
   ],
   "source": [
    "hard_instances = senseval.instances('hard.pos')\n",
    "hard_instances = [instance for instance in hard_instances if instance.senses]  # Filtrar instancias con sentidos\n",
    "print(f\"Total de instancias con sentidos: {len(hard_instances)}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 350,
   "id": "5afa72ae",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Archivo 'corpus_etiquetado.txt' cargado exitosamente.\n"
     ]
    }
   ],
   "source": [
    "file_path = 'corpus_etiquetado.txt'\n",
    "\n",
    "# ü§ì Cargamos el archivo de texto que contiene el corpus\n",
    "try:\n",
    "    with open(file_path, 'r', encoding='utf-8') as f:\n",
    "        corpus_content = f.read()\n",
    "    print(f\"Archivo '{file_path}' cargado exitosamente.\")\n",
    "except FileNotFoundError:\n",
    "    print(f\"Error: El archivo '{file_path}' no se encontr√≥. Aseg√∫rate de que est√° en la misma carpeta que tu script.\")\n",
    "except Exception as e:\n",
    "    print(f\"Ocurri√≥ un error al leer el archivo: {e}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 351,
   "id": "4dd18ee8",
   "metadata": {},
   "outputs": [],
   "source": [
    "## ü§ìProcesa el contenido del corpus para extraer pares (palabra, etiqueta).\n",
    "## ü§ì Maneja etiquetas de documento, l√≠neas vac√≠as y el formato espec√≠fico.\n",
    "def parse_corpus_content(content):\n",
    "    corpus = []\n",
    "    \n",
    "    for linea in content.splitlines():\n",
    "        linea = linea.strip()\n",
    "        \n",
    "        # Saltamos etiquetas de documento y l√≠neas vac√≠as.\n",
    "        if linea.startswith(\"<doc\") or linea.startswith(\"</doc>\") or linea == \"\":\n",
    "            continue\n",
    "        \n",
    "        # Saltar 'Fp' si aparece en una l√≠nea separada (inconsistencia del corpus).\n",
    "        if linea == \"Fp\":\n",
    "            continue\n",
    "        \n",
    "        datos = linea.split(\"\\t\")\n",
    "        \n",
    "        # Formato esperado: palabra \\t lema \\t etiqueta\n",
    "        if len(datos) == 3:\n",
    "            word = datos[0]\n",
    "            tag = datos[2]\n",
    "            corpus.append((word, tag))\n",
    "        # Caso espec√≠fico: . \\t . \\t Fp\n",
    "        elif len(datos) == 2 and datos[0] == '.' and datos[1] == 'Fp':\n",
    "            word = datos[0]\n",
    "            tag = datos[1] \n",
    "            corpus.append((word, tag))\n",
    "        # Las l√≠neas que no encajan se omiten.\n",
    "        else:\n",
    "            pass \n",
    "\n",
    "    return corpus\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1bb28ab3",
   "metadata": {},
   "source": [
    "# Analisis del codigo inicial y uso, levantamiento del corpus"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 352,
   "id": "7f3718ab",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[('Habla', 'VMIP3S0'),\n",
       " ('con', 'SP'),\n",
       " ('el', 'DA'),\n",
       " ('enfermo', 'NCMS000'),\n",
       " ('grave', 'AQ0CS00'),\n",
       " ('de', 'SP'),\n",
       " ('trasplantes', 'NCMN000'),\n",
       " ('.', 'Fp'),\n",
       " ('El', 'DA'),\n",
       " ('enfermo', 'NCMS000'),\n",
       " ('grave', 'AQ0CS00'),\n",
       " ('habla', 'VMIP3S0'),\n",
       " ('de', 'SP'),\n",
       " ('trasplantes', 'NCMN000'),\n",
       " ('.', 'Fp'),\n",
       " ('La', 'DA'),\n",
       " ('pel√≠cula', 'NCFS000'),\n",
       " ('fue', 'VAIP3S0'),\n",
       " ('nominada', 'VMP00SF'),\n",
       " ('al', 'SP+DA'),\n",
       " ('Oscar', 'NP00000'),\n",
       " ('.', 'Fp'),\n",
       " ('Luis', 'NP00000'),\n",
       " ('Bu√±uel', 'NP00000'),\n",
       " ('es', 'VSIP3S0'),\n",
       " ('director', 'NCMS000'),\n",
       " ('espa√±ol', 'AQ0MS00'),\n",
       " ('.', 'Fp'),\n",
       " ('El', 'DA'),\n",
       " ('ni√±o', 'NCMS000'),\n",
       " ('corre', 'VMIP3S0'),\n",
       " ('en', 'SP'),\n",
       " ('el', 'DA'),\n",
       " ('parque', 'NCMS000'),\n",
       " ('.', 'Fp'),\n",
       " ('Una', 'DI'),\n",
       " ('flor', 'NCFS000'),\n",
       " ('roja', 'AQ0FS00'),\n",
       " ('decoraba', 'VIIIS0S'),\n",
       " ('la', 'DA'),\n",
       " ('mesa', 'NCFS000'),\n",
       " ('.', 'Fp'),\n",
       " ('Ella', 'PP3FS000'),\n",
       " ('canta', 'VMIP3S0'),\n",
       " ('una', 'DI'),\n",
       " ('canci√≥n', 'NCFS000'),\n",
       " ('alegre', 'AQ0CS00'),\n",
       " ('.', 'Fp'),\n",
       " ('Ellos', 'PP3MP000'),\n",
       " ('juegan', 'VMIP3P0'),\n",
       " ('f√∫tbol', 'NCMS000'),\n",
       " ('cada', 'DI'),\n",
       " ('d√≠a', 'NCMS000'),\n",
       " ('.', 'Fp'),\n",
       " ('Hoy', 'NCMS000'),\n",
       " ('es', 'VSIP3S0'),\n",
       " ('un', 'DI'),\n",
       " ('d√≠a', 'NCMS000'),\n",
       " ('especial', 'AQ0CS00'),\n",
       " ('.', 'Fp'),\n",
       " ('El', 'DA'),\n",
       " ('sol', 'NCMS000'),\n",
       " ('brilla', 'VMIP3S0'),\n",
       " ('intensamente', 'RG'),\n",
       " ('en', 'SP'),\n",
       " ('el', 'DA'),\n",
       " ('cielo', 'NCMS000'),\n",
       " ('.', 'Fp'),\n",
       " ('Nosotros', 'PP1MP000'),\n",
       " ('estudiamos', 'VMIP1P0'),\n",
       " ('procesamiento', 'NCMS000'),\n",
       " ('del', 'SP+DA'),\n",
       " ('lenguaje', 'NCMS000'),\n",
       " ('natural', 'AQ0CS00'),\n",
       " ('.', 'Fp'),\n",
       " ('Un', 'DI'),\n",
       " ('gato', 'NCMS000'),\n",
       " ('gris', 'AQ0CS00'),\n",
       " ('miraba', 'VIIIS3S'),\n",
       " ('por', 'SP'),\n",
       " ('la', 'DA'),\n",
       " ('ventana', 'NCFS000'),\n",
       " ('.', 'Fp'),\n",
       " ('El', 'DA'),\n",
       " ('profesor', 'NCMS000'),\n",
       " ('explic√≥', 'VMIS3S0'),\n",
       " ('la', 'DA'),\n",
       " ('tarea', 'NCFS000'),\n",
       " ('.', 'Fp'),\n",
       " ('Mar√≠a', 'NP00000'),\n",
       " ('cocinaba', 'VIIIS3S'),\n",
       " ('un', 'DI'),\n",
       " ('pastel', 'NCMS000'),\n",
       " ('de', 'SP'),\n",
       " ('chocolate', 'NCMS000'),\n",
       " ('.', 'Fp'),\n",
       " ('Ayer', 'RG'),\n",
       " ('llovi√≥', 'VMIS3S0'),\n",
       " ('fuertemente', 'RG'),\n",
       " ('en', 'SP'),\n",
       " ('la', 'DA'),\n",
       " ('ciudad', 'NCFS000'),\n",
       " ('.', 'Fp'),\n",
       " ('Ellas', 'PP3FP000'),\n",
       " ('estudian', 'VMIP3P0'),\n",
       " ('arte', 'NCMS000'),\n",
       " ('en', 'SP'),\n",
       " ('la', 'DA'),\n",
       " ('universidad', 'NCFS000'),\n",
       " ('.', 'Fp'),\n",
       " ('Un', 'DI'),\n",
       " ('coche', 'NCMS000'),\n",
       " ('rojo', 'AQ0MS00'),\n",
       " ('avanza', 'VMIP3S0'),\n",
       " ('r√°pido', 'RG'),\n",
       " ('por', 'SP'),\n",
       " ('la', 'DA'),\n",
       " ('carretera', 'NCFS000'),\n",
       " ('.', 'Fp'),\n",
       " ('Ella', 'PP3FS000'),\n",
       " ('pinta', 'VMIP3S0'),\n",
       " ('cuadros', 'NCMP000'),\n",
       " ('hermosos', 'AQ0MP00'),\n",
       " ('.', 'Fp'),\n",
       " ('Ellos', 'PP3MP000'),\n",
       " ('viajan', 'VMIP3P0'),\n",
       " ('a', 'SP'),\n",
       " ('diferentes', 'AQ0MP00'),\n",
       " ('pa√≠ses', 'NCMP000'),\n",
       " ('.', 'Fp'),\n",
       " ('La', 'DA'),\n",
       " ('tarde', 'NCFS000'),\n",
       " ('es', 'VSIP3S0'),\n",
       " ('tranquila', 'AQ0FS00'),\n",
       " ('y', 'CC'),\n",
       " ('agradable', 'AQ0CS00'),\n",
       " ('.', 'Fp')]"
      ]
     },
     "execution_count": 352,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "parse_corpus_content(corpus_content) # Procesamos el contenido del corpus de manera organizada\n",
    "# ü§ì Todo ello para extraer pares (palabra, etiqueta)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 353,
   "id": "9ba99169",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Corpus cargado. N√∫mero de tokens: 137\n",
      "Primeros 5 registros: [('Habla', 'VMIP3S0'), ('con', 'SP'), ('el', 'DA'), ('enfermo', 'NCMS000'), ('grave', 'AQ0CS00')]\n"
     ]
    }
   ],
   "source": [
    "corpus = parse_corpus_content(corpus_content)\n",
    "print(\"Corpus cargado. N√∫mero de tokens:\", len(corpus))\n",
    "print(\"Primeros 5 registros:\", corpus[:5])\n",
    "\n",
    "# ü§ì Contamos las etiquetas y mostramos las m√°s frecuentes, por eso filtramos :5."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 354,
   "id": "44f75ccd",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Cargando instancias de 'hard' desde Senseval...\n",
      "Total Senseval 'hard' instances with senses: 4333\n"
     ]
    }
   ],
   "source": [
    "# ü§ì cargamos esats instancias hay que tener en cuenta que en google colab nos genero bastantes problemas\n",
    "hard_instances = []\n",
    "try:\n",
    "    # Instanciamos a nivel de palabras tipo \"hard\"\n",
    "    print(\"Cargando instancias de 'hard' desde Senseval...\")\n",
    "    hard_instances = senseval.instances('hard.pos')\n",
    "    # Filter instances to ensure they have senses, as some might not\n",
    "    hard_instances = [instance for instance in hard_instances if instance.senses]\n",
    "    print(f\"Total Senseval 'hard' instances with senses: {len(hard_instances)}\")\n",
    "except Exception as e:\n",
    "    print(f\"Could not load 'hard.pos' from senseval, skipping 'hard' analysis for senses: {e}\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 355,
   "id": "02658947",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Total Senseval 'serve' instances with senses: 4378\n"
     ]
    }
   ],
   "source": [
    "# ü§ì Inicializamos las instancias de \"serve\" de Senseval\n",
    "serve_instances = []\n",
    "try:\n",
    "    # ü§ì Cargamos las instancias para \"serve\" desde Senseval\n",
    "    serve_instances = senseval.instances('serve.pos')\n",
    "    # Filtrar instancias para asegurarse de que tienen sentidos, ya que algunas podr√≠an no tenerlos\n",
    "    serve_instances = [instance for instance in serve_instances if instance.senses]\n",
    "    print(f\"Total Senseval 'serve' instances with senses: {len(serve_instances)}\")\n",
    "except Exception as e:\n",
    "    print(f\"Could not load 'serve.pos' from senseval, skipping 'serve' analysis for senses: {e}\")\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "14dfe6b6",
   "metadata": {},
   "source": [
    "1. ¬øCu√°ntos posibles sentidos tienen las palabras ambiguas ¬´hard¬ª y ¬´serve¬ª? ¬øCu√°les son esos sentidos? Para cada sentido indica la etiqueta que aparece en el corpus."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 356,
   "id": "faafb0e3",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "--- 1. Posibles sentidos y etiquetas para 'hard and 'serve' ---\n",
      "'hard' has 3 possible senses: {'HARD2', 'HARD1', 'HARD3'}\n",
      "'serve' has 4 possible senses: {'SERVE6', 'SERVE10', 'SERVE2', 'SERVE12'}\n"
     ]
    }
   ],
   "source": [
    "print(\"--- 1. Posibles sentidos y etiquetas para 'hard and 'serve' ---\")\n",
    "\n",
    "hard_senses = set()\n",
    "for instance in hard_instances:\n",
    "    for sense in instance.senses:\n",
    "        hard_senses.add(sense)\n",
    "print(f\"'hard' has {len(hard_senses)} possible senses: {hard_senses}\")\n",
    "\n",
    "serve_senses = set()\n",
    "for instance in serve_instances:\n",
    "    for sense in instance.senses:\n",
    "        serve_senses.add(sense)\n",
    "print(f\"'serve' has {len(serve_senses)} possible senses: {serve_senses}\")\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6f9b2e14",
   "metadata": {},
   "source": [
    "¬øCu√°ntas instancias hay en el corpus para cada uno de los sentidos de las palabras ambiguas ¬´hard¬ª y ¬´serve¬ª? Es decir, cuantas oraciones hay en el corpus etiquetadas con cada uno de los sentidos."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 357,
   "id": "396358d7",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "--- 2. Numero de instancias para cada sentido ---\n",
      "\n",
      "Conteo de sentidos para 'hard':\n",
      "  Sentido 'HARD1': 3455 instancias\n",
      "  Sentido 'HARD2': 502 instancias\n",
      "  Sentido 'HARD3': 376 instancias\n",
      "\n",
      "Conteo de sentidos para 'serve':\n",
      "  Sentido 'SERVE10': 1814 instancias\n",
      "  Sentido 'SERVE12': 1272 instancias\n",
      "  Sentido 'SERVE2': 853 instancias\n",
      "  Sentido 'SERVE6': 439 instancias\n"
     ]
    }
   ],
   "source": [
    "print(\"\\n--- 2. Numero de instancias para cada sentido ---\")\n",
    "# --- 2. N√∫mero de instancias para cada sentido ---\n",
    "# Contamos las instancias para cada sentido de la palabra ambigua 'hard'\n",
    "hard_sense_counts = {}\n",
    "for instance in hard_instances:\n",
    "    for sense in instance.senses:\n",
    "        hard_sense_counts[sense] = hard_sense_counts.get(sense, 0) + 1\n",
    "print(\"\\nConteo de sentidos para 'hard':\")\n",
    "for sense, count in hard_sense_counts.items():\n",
    "    print(f\"  Sentido '{sense}': {count} instancias\")\n",
    "\n",
    "# Contamos las instancias para cada sentido de la palabra ambigua 'serve'\n",
    "serve_sense_counts = {}\n",
    "for instance in serve_instances:\n",
    "    for sense in instance.senses:\n",
    "        serve_sense_counts[sense] = serve_sense_counts.get(sense, 0) + 1\n",
    "print(\"\\nConteo de sentidos para 'serve':\")\n",
    "for sense, count in serve_sense_counts.items():\n",
    "    print(f\"  Sentido '{sense}': {count} instancias\")\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e778ff86",
   "metadata": {},
   "source": [
    "3. ¬øTienen todas las instancias que forman el corpus el formato que se ha descrito anteriormente? Si hay alguna instancia que no cumpla con ese formato, indica cuales ser√≠an las incongruencias que presenta y muestra algunos ejemplos."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 358,
   "id": "20a2e34b",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "--- 3. Formatos gramaticales para 'hard' y 'serve' ---\n",
      "Formatos gramaticales en 'hard' encontrados: {'hardest', 'harder', 'hard'}\n",
      "Formatos gramaticales en 'serve' encontrados: {'serves', 'server', 'serve', 'served'}\n"
     ]
    }
   ],
   "source": [
    "# ü§ì 3. Formatos gramaticales para \"hard\" y \"serve\"\n",
    "print(\"--- 3. Formatos gramaticales para 'hard' y 'serve' ---\")\n",
    "\n",
    "# ü§ì Para 'hard'\n",
    "hard_forms = set()\n",
    "for instance in hard_instances:\n",
    "    # ü§ì instance.context puede contener tuplas (palabra, etiqueta) o solo palabras.\n",
    "    # ü§ì Intentaremos manejar ambos casos verificando el tipo de 'item'.\n",
    "    for item in instance.context:\n",
    "        word_to_check = ''\n",
    "        if isinstance(item, tuple):\n",
    "            word_to_check = item[0]  # ü§ì Obtener la palabra de la tupla\n",
    "        elif isinstance(item, str):\n",
    "            word_to_check = item  # ü§ì 'item' ya es la palabra como cadena\n",
    "        \n",
    "        if word_to_check:  # ü§ì Asegurarse de que no est√© vac√≠o\n",
    "            if word_to_check.lower() in {'hard', 'harder', 'hardest'}:\n",
    "                hard_forms.add(word_to_check.lower())\n",
    "print(f\"Formatos gramaticales en 'hard' encontrados: {hard_forms}\")\n",
    "\n",
    "# ü§ì Para 'serve' \n",
    "serve_forms = set()\n",
    "for instance in serve_instances:\n",
    "    for item in instance.context:\n",
    "        word_to_check = ''\n",
    "        if isinstance(item, tuple):\n",
    "            word_to_check = item[0]\n",
    "        elif isinstance(item, str):\n",
    "            word_to_check = item\n",
    "            \n",
    "        if word_to_check:\n",
    "            if word_to_check.lower().startswith('serve'):  # ü§ì Capturar serve, serves, served, serving, etc.\n",
    "                serve_forms.add(word_to_check.lower())\n",
    "print(f\"Formatos gramaticales en 'serve' encontrados: {serve_forms}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 359,
   "id": "b46a868a",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "SensevalInstance(word='hard-a', position=10, context=[('clever', 'NNP'), ('white', 'NNP'), ('house', 'NNP'), ('``', '``'), ('spin', 'VB'), ('doctors', 'NNS'), (\"''\", \"''\"), ('are', 'VBP'), ('having', 'VBG'), ('a', 'DT'), ('hard', 'JJ'), ('time', 'NN'), ('helping', 'VBG'), ('president', 'NNP'), ('bush', 'NNP'), ('explain', 'VB'), ('away', 'RB'), ('the', 'DT'), ('economic', 'JJ'), ('bashing', 'NN'), ('that', 'IN'), ('low-and', 'JJ'), ('middle-income', 'JJ'), ('workers', 'NNS'), ('are', 'VBP'), ('taking', 'VBG'), ('these', 'DT'), ('days', 'NNS'), ('.', '.')], senses=('HARD1',))\n"
     ]
    }
   ],
   "source": [
    "inst = hard_instances[1]\n",
    "print(inst)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2229ff0e",
   "metadata": {},
   "source": [
    "4. ¬øTienen todas las instancias que forman el corpus el formato que se ha descrito anteriormente? Si hay alguna instancia que no cumpla con ese formato, indica cuales ser√≠an las incongruencias que presenta y muestra algunos ejemplos.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 360,
   "id": "56c66c20",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "--- 4. Checking corpus_etiquetado.txt format consistency ---\n",
      "Por si se encuentra alguna inconsistencia 'corpus_etiquetado.txt':\n",
      "  1. Invalid tag format: .\t.\tFp (Doc ID: 1)\n",
      "  2. Invalid tag format: .\t.\tFp (Doc ID: 2)\n",
      "  3. Invalid tag format: .\t.\tFp (Doc ID: 3)\n",
      "  4. Invalid tag format: .\t.\tFp (Doc ID: 4)\n",
      "  5. Invalid tag format: .\t.\tFp (Doc ID: 5)\n",
      "  6. Invalid tag format: .\t.\tFp (Doc ID: 6)\n",
      "  7. Invalid tag format: .\t.\tFp (Doc ID: 7)\n",
      "  8. Invalid tag format: .\t.\tFp (Doc ID: 8)\n",
      "  9. Invalid tag format: .\t.\tFp (Doc ID: 9)\n",
      "  10. Invalid tag format: .\t.\tFp (Doc ID: 10)\n",
      "  ...and 10 more inconsistencies.\n",
      "\n",
      "Ejemplo como lo pide la guia con (word\\tlemma\\ttag):\n",
      "Habla\\thablar\\tVMIP3S0\n",
      "con\\tcon\\tSP\n",
      "el\\tel\\tDA\n"
     ]
    }
   ],
   "source": [
    "\n",
    "# Check if all instances in the provided corpus follow the format\n",
    "print(\"\\n--- 4. Checking corpus_etiquetado.txt format consistency ---\")\n",
    "\n",
    "if not inconsistent_lines:\n",
    "    print(\"Todas las inconsistencias en el modulo 'corpus_etiquetado.txt' que aparecen el  formato (word\\\\tlemma\\\\ttag).\")\n",
    "else:\n",
    "    print(\"Por si se encuentra alguna inconsistencia 'corpus_etiquetado.txt':\")\n",
    "    for i, problem in enumerate(inconsistent_lines[:10]): # Show first 10 examples\n",
    "        print(f\"  {i+1}. {problem}\")\n",
    "    if len(inconsistent_lines) > 10:\n",
    "        print(f\"  ...and {len(inconsistent_lines) - 10} more inconsistencies.\")\n",
    "\n",
    "# Example of a well-formatted line from your corpus for reference\n",
    "print(\"\\nEjemplo como lo pide la guia con (word\\\\tlemma\\\\ttag):\")\n",
    "print(\"Habla\\\\thablar\\\\tVMIP3S0\")\n",
    "print(\"con\\\\tcon\\\\tSP\")\n",
    "print(\"el\\\\tel\\\\tDA\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 361,
   "id": "d7b11ce7",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "--- Verifying format of 'corpus_etiquetado.txt' ---\n",
      "\n",
      "--- Summary for 'corpus_etiquetado.txt' ---\n",
      "Total documents processed: 20\n",
      "\n",
      "All processed lines conform to the 'Word\\tLemma\\tPOS_Tag' format within documents.\n",
      "\n",
      "--- Examples of Valid Lines (Word\\tLemma\\tPOS_Tag) ---\n",
      "  Doc 1, Line 3: 'Habla\thablar\tVMIP3S0' (Valid)\n",
      "  Doc 1, Line 4: 'con\tcon\tSP' (Valid)\n",
      "  Doc 1, Line 5: 'el\tel\tDA' (Valid)\n",
      "\n",
      "--- Additional Notes ---\n",
      "In this specific corpus format, an 'inconsistency' means a line that doesn't have exactly 3 tab-separated components (Word, Lemma, POS Tag).\n",
      "Missing blank lines between documents or malformed <doc id> tags would also be inconsistencies.\n"
     ]
    }
   ],
   "source": [
    "def verify_corpus_format(file_path=corpus_content):\n",
    "    \n",
    "    if not os.path.exists(file_path):\n",
    "        print(f\"Error: The file '{file_path}' was not found.\")\n",
    "        return\n",
    "\n",
    "    print(f\"--- Verifying format of '{file_path}' ---\")\n",
    "\n",
    "    current_doc_id = None\n",
    "    line_number = 0\n",
    "    inconsistencies = []\n",
    "    example_lines = []\n",
    "    doc_count = 0\n",
    "\n",
    "    with open(file_path, 'r', encoding='utf-8') as f:\n",
    "        for line in f:\n",
    "            line_number += 1\n",
    "            line = line.strip() # Remove leading/trailing whitespace including newline\n",
    "\n",
    "            if not line: # Empty line, often signifies end of a doc or just whitespace\n",
    "                continue\n",
    "\n",
    "            if line.startswith('<doc id=\"') and line.endswith('>'):\n",
    "                doc_count += 1\n",
    "                try:\n",
    "                    # Extract doc ID\n",
    "                    current_doc_id = line.split('\"')[1]\n",
    "                    # print(f\"\\n--- Processing Document ID: {current_doc_id} ---\")\n",
    "                except IndexError:\n",
    "                    inconsistencies.append(f\"Line {line_number}: Malformed document ID tag: '{line}'\")\n",
    "                continue\n",
    "\n",
    "            # If it's not a doc ID line and not empty, it should be a data line\n",
    "            parts = line.split('\\t')\n",
    "\n",
    "            if len(parts) == 3:\n",
    "                # Valid format: Word<TAB>Lemma<TAB>POS_Tag\n",
    "                if len(example_lines) < 3 and current_doc_id is not None: # Capture up to 3 valid examples\n",
    "                    example_lines.append(f\"Doc {current_doc_id}, Line {line_number}: '{line}' (Valid)\")\n",
    "            else:\n",
    "                # Inconsistent format\n",
    "                inconsistencies.append(f\"Doc {current_doc_id if current_doc_id else 'N/A'}, Line {line_number}: Expected 3 tab-separated fields, found {len(parts)} in '{line}'\")\n",
    "\n",
    "    print(f\"\\n--- Summary for '{file_path}' ---\")\n",
    "    print(f\"Total documents processed: {doc_count}\")\n",
    "\n",
    "    if inconsistencies:\n",
    "        print(f\"\\nFound {len(inconsistencies)} inconsistencies:\")\n",
    "        for i, issue in enumerate(inconsistencies):\n",
    "            print(f\"  - {issue}\")\n",
    "            if i >= 4: # Show first 5 inconsistencies only for brevity\n",
    "                print(\"  ... (more inconsistencies hidden)\")\n",
    "                break\n",
    "    else:\n",
    "        print(\"\\nAll processed lines conform to the 'Word\\\\tLemma\\\\tPOS_Tag' format within documents.\")\n",
    "\n",
    "    if example_lines:\n",
    "        print(\"\\n--- Examples of Valid Lines (Word\\\\tLemma\\\\tPOS_Tag) ---\")\n",
    "        for example in example_lines:\n",
    "            print(f\"  {example}\")\n",
    "    else:\n",
    "        print(\"\\nNo valid example lines were found (corpus might be empty or entirely inconsistent).\")\n",
    "\n",
    "    print(\"\\n--- Additional Notes ---\")\n",
    "    print(\"In this specific corpus format, an 'inconsistency' means a line that doesn't have exactly 3 tab-separated components (Word, Lemma, POS Tag).\")\n",
    "    print(\"Missing blank lines between documents or malformed <doc id> tags would also be inconsistencies.\")\n",
    "\n",
    "# --- Run the verification ---\n",
    "verify_corpus_format(\"corpus_etiquetado.txt\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "db784e42",
   "metadata": {},
   "source": [
    "# Parte 2: Extracci√≥n de caracter√≠sticas \n",
    "## Vector de caracter√≠sticas binario basado en la presencia de palabras del vocabulario\n",
    "    en el contexto dado.\n",
    "\n",
    "    + Args:\n",
    "        contexto (list): Una lista de palabras que forman el contexto de la palabra ambigua.\n",
    "                         Puede contener tuplas (palabra, tag) o solo palabras.\n",
    "        vocabulario (list): Una lista de palabras que conforman el vocabulario de caracter√≠sticas.\n",
    "\n",
    "    + Returns:\n",
    "        list: Un vector binario donde cada elemento indica la presencia (1) o ausencia (0)\n",
    "              de la palabra correspondiente del vocabulario en el contexto.\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 362,
   "id": "d8b46dac",
   "metadata": {},
   "outputs": [],
   "source": [
    "class Instance:\n",
    "    def __init__(self, context, ambiguous_word_index):\n",
    "        self.context = context\n",
    "        self.ambiguous_word_index = ambiguous_word_index # √çndice de la palabra ambigua en el contexto\n",
    "\n",
    "# Ejemplo de contexto para 'hard'\n",
    "# Asumimos que 'hard' es la palabra ambigua y est√° en alguna posici√≥n\n",
    "# Para simplificar, aqu√≠ el contexto es una lista de palabras.\n",
    "hard_instance_context = vocab"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "88796876",
   "metadata": {},
   "source": [
    "Extracci√≥n de caracter√≠sticas basada en las palabras vecinas"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 363,
   "id": "ea503a91",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[('clever', 'NNP'),\n",
       " ('white', 'NNP'),\n",
       " ('house', 'NNP'),\n",
       " ('``', '``'),\n",
       " ('spin', 'VB'),\n",
       " ('doctors', 'NNS'),\n",
       " (\"''\", \"''\"),\n",
       " ('are', 'VBP'),\n",
       " ('having', 'VBG'),\n",
       " ('a', 'DT'),\n",
       " ('hard', 'JJ'),\n",
       " ('time', 'NN'),\n",
       " ('helping', 'VBG'),\n",
       " ('president', 'NNP'),\n",
       " ('bush', 'NNP'),\n",
       " ('explain', 'VB'),\n",
       " ('away', 'RB'),\n",
       " ('the', 'DT'),\n",
       " ('economic', 'JJ'),\n",
       " ('bashing', 'NN'),\n",
       " ('that', 'IN'),\n",
       " ('low-and', 'JJ'),\n",
       " ('middle-income', 'JJ'),\n",
       " ('workers', 'NNS'),\n",
       " ('are', 'VBP'),\n",
       " ('taking', 'VBG'),\n",
       " ('these', 'DT'),\n",
       " ('days', 'NNS'),\n",
       " ('.', '.')]"
      ]
     },
     "execution_count": 363,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "inst.context"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 364,
   "id": "dcf67e20",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'find', 'get', 'make', 'time', 'work', 'would'}"
      ]
     },
     "execution_count": 364,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "vocab = ['time', 'would', 'get', 'work', 'find', 'make']\n",
    "vocab = set(vocab) \n",
    "vocab# Convert to set for faster lookup"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 365,
   "id": "82045073",
   "metadata": {},
   "outputs": [],
   "source": [
    "def extraer_caracteristicas_palabras_vecinas(contexto, vocabulario):\n",
    "\n",
    "    vector_caracteristicas = []\n",
    "    contexto_palabras_lower = []\n",
    "\n",
    "    # Normalizar el contexto a solo palabras en min√∫sculas\n",
    "    for item in contexto:\n",
    "        word_to_add = ''\n",
    "        if isinstance(item, tuple):\n",
    "            word_to_add = item[0].lower()\n",
    "        elif isinstance(item, str):\n",
    "            word_to_add = item.lower()\n",
    "        if word_to_add:\n",
    "            contexto_palabras_lower.append(word_to_add)\n",
    "\n",
    "    # Construir el vector de caracter√≠sticas\n",
    "    for palabra_vocabulario in vocabulario:\n",
    "        if palabra_vocabulario.lower() in contexto_palabras_lower:\n",
    "            vector_caracteristicas.append(1)\n",
    "        else:\n",
    "            vector_caracteristicas.append(0)\n",
    "            \n",
    "    return vector_caracteristicas"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 366,
   "id": "7aa5ed2f",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Vocabulario: {'get', 'make', 'time', 'would', 'find', 'work'}\n",
      "Contexto de 'hard': get make time would find work\n",
      "Vector de caracter√≠sticas para 'hard': [1, 1, 1, 1, 1, 1]\n",
      "Contexto de 'hard' (ejemplo solicitado): este es un ejemplo con time pero sin would get work find make\n",
      "Vector de caracter√≠sticas para 'hard' (ejemplo solicitado): [1, 1, 1, 1, 1, 1]\n"
     ]
    }
   ],
   "source": [
    "# Contexto de la instancia de \"hard\"\n",
    "# Simulamos un contexto donde \"time\" aparece y el resto no.\n",
    "contexto_hard = vocab\n",
    "\n",
    "# Extraer el vector de caracter√≠sticas\n",
    "vector_caracteristicas_hard = extraer_caracteristicas_palabras_vecinas(contexto_hard, vocab)\n",
    "\n",
    "print(f\"Vocabulario: {vocab}\")\n",
    "print(f\"Contexto de 'hard': {' '.join(contexto_hard)}\")\n",
    "print(f\"Vector de caracter√≠sticas para 'hard': {vector_caracteristicas_hard}\")\n",
    "\n",
    "\n",
    "\n",
    "# Si el contexto_hard fuera el que describiste:\n",
    "contexto_hard_solicitado = [\"este\", \"es\", \"un\", \"ejemplo\", \"con\", \"time\", \"pero\", \"sin\", \"would\", \"get\", \"work\", \"find\", \"make\"]\n",
    "vector_caracteristicas_hard_solicitado = extraer_caracteristicas_palabras_vecinas(contexto_hard_solicitado, vocab)\n",
    "print(f\"Contexto de 'hard' (ejemplo solicitado): {' '.join(contexto_hard_solicitado)}\")\n",
    "print(f\"Vector de caracter√≠sticas para 'hard' (ejemplo solicitado): {vector_caracteristicas_hard_solicitado}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 367,
   "id": "97926c98",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Vocabulario: {'get', 'make', 'time', 'would', 'find', 'work'}\n",
      "Contexto de 'hard': este vector de caracter√≠sticas indica que en el contexto de la palabra ambigua aparece la palabra time y no aparecen las palabras would get work find y make\n",
      "Vector de caracter√≠sticas para 'hard': [1, 1, 1, 1, 1, 1]\n",
      "Contexto de 'hard' (ejemplo solicitado): este es un ejemplo con time pero sin would get work find make\n",
      "Vector de caracter√≠sticas para 'hard' (ejemplo solicitado): [1, 1, 1, 1, 1, 1]\n"
     ]
    }
   ],
   "source": [
    "    # Contexto de la instancia de \"hard\"\n",
    "# Simulamos un contexto donde \"time\" aparece y el resto no.\n",
    "contexto_hard = [\"este\", \"vector\", \"de\", \"caracter√≠sticas\", \"indica\", \"que\", \"en\", \"el\", \"contexto\", \"de\", \"la\", \"palabra\",\n",
    "                 \"ambigua\", \"aparece\", \"la\", \"palabra\", \"time\", \"y\", \"no\", \"aparecen\", \"las\", \"palabras\",\n",
    "                 \"would\", \"get\", \"work\", \"find\", \"y\", \"make\"]\n",
    "\n",
    "# Extraer el vector de caracter√≠sticas\n",
    "vector_caracteristicas_hard = extraer_caracteristicas_palabras_vecinas(contexto_hard, vocab)\n",
    "\n",
    "print(f\"Vocabulario: {vocab}\")\n",
    "print(f\"Contexto de 'hard': {' '.join(contexto_hard)}\")\n",
    "print(f\"Vector de caracter√≠sticas para 'hard': {vector_caracteristicas_hard}\")\n",
    "\n",
    "\n",
    "\n",
    "# Si el contexto_hard fuera el que describiste:\n",
    "contexto_hard_solicitado = [\"este\", \"es\", \"un\", \"ejemplo\", \"con\", \"time\", \"pero\", \"sin\", \"would\", \"get\", \"work\", \"find\", \"make\"]\n",
    "vector_caracteristicas_hard_solicitado = extraer_caracteristicas_palabras_vecinas(contexto_hard_solicitado, vocab)\n",
    "print(f\"Contexto de 'hard' (ejemplo solicitado): {' '.join(contexto_hard_solicitado)}\")\n",
    "print(f\"Vector de caracter√≠sticas para 'hard' (ejemplo solicitado): {vector_caracteristicas_hard_solicitado}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a4179bf7",
   "metadata": {},
   "source": [
    "# Construcci√≥n del vocabulario o bags of words.\n",
    "\"Este vector de caracter√≠sticas indica que en el contexto de la palabra\n",
    "ambigua aparece la palabra ¬´time¬ª y no aparecen las palabras\n",
    "¬´would¬ª, ¬´get¬ª, ¬´work¬ª, ¬´find¬ª y ¬´make¬ª.\"\n",
    "Esto se traducir√≠a a [1, 0, 0, 0, 0, 0] para el vocabulario dado.\n",
    "Si en el contexto_hard las palabras \"would\", \"get\", \"work\", \"find\", \"make\" no estuvieran presentes,\n",
    "el resultado ser√≠a exactamente [1, 0, 0, 0, 0, 0].\n",
    "En el contexto_hard de ejemplo, las palabras \"would\", \"get\", \"work\", \"find\", \"make\" s√≠ aparecen.\n",
    "Por lo tanto, el vector resultante ser√≠a:\n",
    "Palabra 'time': 1 (aparece)\n",
    "Palabra 'would': 1 (aparece)\n",
    "Palabra 'get': 1 (aparece)\n",
    "Palabra 'work': 1 (aparece)\n",
    "Palabra 'find': 1 (aparece)\n",
    "Palabra 'make': 1 (aparece)\n",
    "--> [1, 1, 1, 1, 1, 1]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 387,
   "id": "a21ecfb9",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Aseg√∫rate de haber descargado los recursos necesarios de NLTK\n",
    "try:\n",
    "    nltk.data.find('corpora/stopwords')\n",
    "except nltk.downloader.DownloadError:\n",
    "    nltk.download('stopwords')\n",
    "try:\n",
    "    nltk.data.find('tokenizers/punkt')\n",
    "except nltk.downloader.DownloadError:\n",
    "    nltk.download('punkt')\n",
    "\n",
    "\n",
    "# Simulaci√≥n de las instancias del corpus\n",
    "# En un escenario real, 'hard_instances' y 'serve_instances'\n",
    "# vendr√≠an de tu corpus cargado.\n",
    "class Instance:\n",
    "    def __init__(self, context, ambiguous_word):\n",
    "        self.context = context # Lista de palabras o (palabra, tag) tuplas\n",
    "        self.ambiguous_word = ambiguous_word # La palabra ambigua asociada a esta instancia\n",
    "\n",
    "        \n",
    "# Ejemplos de instancias (simuladas para 'hard' y 'serve')\n",
    "# Aqu√≠ usaremos solo strings en el contexto para simplificar, pero el c√≥digo manejar√° tuplas tambi√©n.\n",
    "hard_instances = [\n",
    "    Instance([\"It\", \"was\", \"a\", \"hard\", \"time\", \"for\", \"everyone\", \"involved\", \".\"], \"hard\"),\n",
    "    Instance([\"He\", \"worked\", \"hard\", \"to\", \"get\", \"the\", \"job\", \"done\", \".\"], \"hard\"),\n",
    "    Instance([\"The\", \"decision\", \"was\", \"harder\", \"than\", \"I\", \"thought\", \".\"], \"hard\"),\n",
    "    Instance([\"This\", \"is\", \"the\", \"hardest\", \"challenge\", \"I've\", \"faced\", \".\"], \"hard\"),\n",
    "    Instance([\"You\", \"would\", \"find\", \"it\", \"hard\", \"to\", \"make\", \"a\", \"living\", \".\"], \"hard\"),\n",
    "    Instance([\"It's\", \"hard\", \"work\", \"but\", \"it's\", \"rewarding\", \".\"], \"hard\"),\n",
    "    Instance([\"They\", \"get\", \"through\", \"hard\", \"times\", \".\"], \"hard\"),\n",
    "    Instance([\"It's\", \"hard\", \"to\", \"get\", \"enough\", \"sleep\", \".\"], \"hard\"),\n",
    "]\n",
    "\n",
    "serve_instances = [\n",
    "    Instance([\"He\", \"will\", \"serve\", \"his\", \"country\", \".\"], \"serve\"),\n",
    "    Instance([\"The\", \"waiter\", \"serves\", \"food\", \".\"], \"serve\"),\n",
    "    Instance([\"She\", \"served\", \"in\", \"the\", \"military\", \".\"], \"serve\"),\n",
    "    Instance([\"Serving\", \"the\", \"community\", \"is\", \"important\", \".\"], \"serve\"),\n",
    "]\n",
    "\n",
    "# Recopilar todas las instancias en una sola lista para procesar todo el corpus\n",
    "all_instances = hard_instances + serve_instances\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c2a028f2",
   "metadata": {},
   "source": [
    "Extracci√≥n de caracter√≠sticas basada en caracter√≠sticas de colocaci√≥n\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 388,
   "id": "bdf14f46",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Total de palabras a excluir: 271\n"
     ]
    }
   ],
   "source": [
    "# Stopwords y otros signos a excluir\n",
    "OTHER_WORDS = [\"''\",\"'d\",\"'ll\",\"'m\",\"'re\",\"'s\",\"'t\",\"'ve\",'--','000','1','2','3','4','5','6','8','10','15','30','I','F','``',\n",
    "               'also',\"don'\",'n','one','said','say','says','u','us']\n",
    "\n",
    "# Combinar stopwords de NLTK, puntuaci√≥n y OTHER_WORDS\n",
    "STOPWORDS_SET = set(stopwords.words('english')).union(set(string.punctuation), set(OTHER_WORDS))\n",
    "\n",
    "# Identificar las formas gramaticales de tus palabras ambiguas\n",
    "# Aqu√≠ es donde debes a√±adir las formas que identificaste en tu laboratorio.\n",
    "\n",
    "hard_forms = vocab\n",
    "serve_forms = set()\n",
    "for instance in serve_instances:\n",
    "    for item in instance.context:\n",
    "        word_to_check = ''\n",
    "        if isinstance(item, tuple):\n",
    "            word_to_check = item[0]\n",
    "        elif isinstance(item, str):\n",
    "            word_to_check = item\n",
    "        if word_to_check and word_to_check.lower().startswith('serve'):\n",
    "            serve_forms.add(word_to_check.lower())\n",
    "\n",
    "# Combinar todas las palabras a excluir\n",
    "WORDS_TO_EXCLUDE = STOPWORDS_SET.union(hard_forms).union(serve_forms)\n",
    "\n",
    "print(f\"Total de palabras a excluir: {len(WORDS_TO_EXCLUDE)}\")\n",
    "# print(f\"Ejemplo de palabras a excluir: {list(WORDS_TO_EXCLUDE)[:20]}\") # Descomenta para ver algunas"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 389,
   "id": "f66a8e03",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[('``', '``'),\n",
       " ('it', 'PRP'),\n",
       " (\"'s\", 'VBZ'),\n",
       " ('a', 'DT'),\n",
       " ('very', 'RB'),\n",
       " ('interesting', 'JJ'),\n",
       " ('place', 'NN'),\n",
       " ('to', 'TO'),\n",
       " ('work', 'VB'),\n",
       " (',', ','),\n",
       " ('but', 'CC'),\n",
       " ('i', 'PRP'),\n",
       " ('can', 'MD'),\n",
       " ('see', 'VB'),\n",
       " ('why', 'WRB'),\n",
       " ('some', 'DT'),\n",
       " ('people', 'NNS'),\n",
       " ('have', 'VBP'),\n",
       " ('a', 'DT'),\n",
       " ('hard', 'JJ'),\n",
       " ('time', 'NN'),\n",
       " ('imagining', 'VBG'),\n",
       " ('what', 'WP'),\n",
       " ('it', 'PRP'),\n",
       " (\"'s\", 'VBZ'),\n",
       " ('like', 'IN'),\n",
       " (',', ','),\n",
       " ('\"', '\"'),\n",
       " ('says', 'VBZ'),\n",
       " ('nate', 'NNP'),\n",
       " ('gossett', 'NNP'),\n",
       " ('.', '.')]"
      ]
     },
     "execution_count": 389,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "senseval.instances('hard.pos')[2737].context"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f49f7adc",
   "metadata": {},
   "source": [
    "C√≥mo construyo mi conjunto y que n utilizo? usando la librer√≠a ngrams\n",
    "\n",
    "Extrae caracter√≠sticas contextuales (palabras anteriores y siguientes) de longitud 'n'\n",
    " alrededor de la palabra ambigua objetivo en una instancia de Senseval.\n",
    " Maneja condiciones de l√≠mite donde la palabra ambigua est√° al principio o al final de la oraci√≥n.\n",
    "\n",
    " Argumentos:\n",
    "     instancia (nltk.corpus.reader.senseval.SensevalInstance): La instancia a procesar.\n",
    "    n (int): El n√∫mero de palabras a considerar antes y despu√©s de la palabra objetivo.\n",
    "\n",
    " Retorna:\n",
    "     dict: Un diccionario de caracter√≠sticas. Las claves ser√°n como 'anterior(palabra1 palabra2)' o 'siguiente(palabra1 palabra2)'."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 390,
   "id": "2d870534",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "The 'senseval' corpus is already downloaded and accessible.\n",
      "\n",
      "Loaded 4333 instances from 'hard.pos'.\n",
      "\n",
      "--- Feature Extraction Examples (n=2) ---\n",
      "\n",
      "Instance 1:\n",
      "  Target Word: 'hard-a'\n",
      "  Context: `` he may lose all popular support , but someone has to kill him to defeat him and that 's **hard-a** to do . ''\n",
      "  Position: 20\n",
      "  Extracted Features: {\"previous(that 's)\": True, 'next(to do)': True}\n",
      "\n",
      "Instance 2:\n",
      "  Target Word: 'hard-a'\n",
      "  Context: clever white house `` spin doctors '' are having a **hard-a** time helping president bush explain away the economic bashing that low-and middle-income workers are taking these days .\n",
      "  Position: 10\n",
      "  Extracted Features: {'previous(having a)': True, 'next(time helping)': True}\n",
      "\n",
      "Instance 3:\n",
      "  Target Word: 'hard-a'\n",
      "  Context: i find it **hard-a** to believe that the sacramento river will ever be quite the same , although i certainly wish that i 'm wrong .\n",
      "  Position: 3\n",
      "  Extracted Features: {'previous(find it)': True, 'next(to believe)': True}\n",
      "\n",
      "Instance 4:\n",
      "  Target Word: 'hard-a'\n",
      "  Context: now when you get bad credit data or are confused with another person , the **hard-a** part in correcting the mistake is not even knowing where it is recorded , let alone having access .\n",
      "  Position: 15\n",
      "  Extracted Features: {'previous(, the)': True, 'next(part in)': True}\n",
      "\n",
      "Instance 5:\n",
      "  Target Word: 'hard-a'\n",
      "  Context: 'a great share of responsibility for this national tragedy unquestionably lies with the president of the country . ' -- eduard shevardnadze , former foreign minister ; 'we are so deep in this crisis that all this business about leaving the party , not leaving the party -- that will never get us out . ' -- natasha , a moscow bookkeeper ; 'our life is **hard-a** now , yes , but it is better to be hungry and free . ' -- lena sedykh , a moscow street sweeper ; 'if you judge by astrology , gorbachev is cancer , and yeltsin and russia are aquarius .\n",
      "  Position: 66\n",
      "  Extracted Features: {'previous(life is)': True, 'next(now ,)': True}\n",
      "\n",
      "--- Example of boundary case simulation (target at start/end) ---\n",
      "\n",
      "Simulated (Start): Context: hard work pays off, Position: 0\n",
      "  Features: {'next(work pays)': True}\n",
      "\n",
      "Simulated (End): Context: It was very hard, Position: 3\n",
      "  Features: {'previous(was very)': True}\n",
      "\n",
      "Simulated (Short Context): Context: A hard choice, Position: 1\n",
      "  Features: {'previous(A)': True, 'next(choice)': True}\n"
     ]
    }
   ],
   "source": [
    "from nltk import ngrams # Import ngrams as suggested in the note \n",
    "\n",
    "def extract_contextual_features(instance, n=2):\n",
    " \n",
    "    features = {}\n",
    "    \n",
    "    # Extract only the words from the context (list of (word, pos_tag) tuples)\n",
    "    context_words = [word for word, _pos in instance.context]\n",
    "    target_position = instance.position\n",
    "\n",
    "    # --- Extract Previous Words ---\n",
    "    # Calculate start index for previous words\n",
    "    # It ensures we don't go below 0\n",
    "    previous_start = max(0, target_position - n)\n",
    "    previous_words = context_words[previous_start:target_position]\n",
    "\n",
    "    if previous_words:\n",
    "        # Join words to form a string for the feature key\n",
    "        feature_key = f\"previous({' '.join(previous_words)})\"\n",
    "        features[feature_key] = True\n",
    "    else:\n",
    "        # If there are no words before (e.g., target word is at the beginning)\n",
    "        # You might choose to add a feature indicating lack of previous context,\n",
    "        # or just omit the 'previous' feature for this instance.\n",
    "        # For this example, we'll just not add the feature if no words exist.\n",
    "        pass\n",
    "\n",
    "    # --- Extract Next Words ---\n",
    "    # Calculate end index for next words\n",
    "    # It ensures we don't go beyond the end of the context\n",
    "    next_end = min(len(context_words), target_position + n + 1)\n",
    "    next_words = context_words[target_position + 1:next_end]\n",
    "\n",
    "    if next_words:\n",
    "        # Join words to form a string for the feature key\n",
    "        feature_key = f\"next({' '.join(next_words)})\"\n",
    "        features[feature_key] = True\n",
    "    else:\n",
    "        # If there are no words after (e.g., target word is at the end)\n",
    "        pass\n",
    "\n",
    "    return features\n",
    "\n",
    "# --- Main script to demonstrate feature extraction ---\n",
    "\n",
    "# 1. Ensure the 'senseval' corpus is downloaded\n",
    "try:\n",
    "    # Attempt to load a part of the corpus to check if it's available\n",
    "    _ = senseval.instances('hard.pos')\n",
    "    print(\"The 'senseval' corpus is already downloaded and accessible.\")\n",
    "except (OSError, LookupError):\n",
    "    print(\"The 'senseval' corpus was not found. Downloading now...\")\n",
    "    nltk.download('senseval')\n",
    "    print(\"Download complete!\")\n",
    "    from nltk.corpus import senseval # Re-import after download\n",
    "\n",
    "# 2. Load instances for 'hard.pos' to test\n",
    "try:\n",
    "    hard_instances = senseval.instances('hard.pos')\n",
    "    print(f\"\\nLoaded {len(hard_instances)} instances from 'hard.pos'.\")\n",
    "except OSError as e:\n",
    "    print(f\"Error loading instances from 'hard.pos': {e}\")\n",
    "    print(\"Please ensure your NLTK data path is correctly configured or restart your environment.\")\n",
    "    exit()\n",
    "\n",
    "# 3. Process and display features for a few instances, including boundary cases\n",
    "print(\"\\n--- Feature Extraction Examples (n=2) ---\")\n",
    "processed_count = 0\n",
    "\n",
    "for i, instance in enumerate(hard_instances):\n",
    "    if processed_count >= 5: # Limit examples for brevity\n",
    "        break\n",
    "\n",
    "    features = extract_contextual_features(instance, n=2)\n",
    "\n",
    "    # Print relevant instance info for context\n",
    "    context_words_only = [word for word, _pos in instance.context]\n",
    "    target_word_display = instance.word\n",
    "    \n",
    "    # Highlight the target word in the context for better visualization\n",
    "    # Handle cases where position might be invalid for some reason, though unlikely in Senseval\n",
    "    if 0 <= instance.position < len(context_words_only):\n",
    "        context_display = context_words_only[:instance.position] + [f'**{target_word_display}**'] + context_words_only[instance.position+1:]\n",
    "    else:\n",
    "        context_display = context_words_only + [f'(Target word \"{target_word_display}\" at invalid position {instance.position})']\n",
    "\n",
    "    print(f\"\\nInstance {i+1}:\")\n",
    "    print(f\"  Target Word: '{instance.word}'\")\n",
    "    print(f\"  Context: {' '.join(context_display)}\")\n",
    "    print(f\"  Position: {instance.position}\")\n",
    "    print(f\"  Extracted Features: {features}\")\n",
    "    processed_count += 1\n",
    "\n",
    "print(\"\\n--- Example of boundary case simulation (target at start/end) ---\")\n",
    "\n",
    "# Simulate an instance where 'hard' is at the very beginning\n",
    "simulated_instance_start = nltk.corpus.reader.senseval.SensevalInstance(\n",
    "    word='hard',\n",
    "    senses=('hard%a%01',),\n",
    "    context=[('hard', 'JJ'), ('work', 'NN'), ('pays', 'VBZ'), ('off', 'RP')],\n",
    "    position=0\n",
    ")\n",
    "features_start = extract_contextual_features(simulated_instance_start, n=2)\n",
    "print(f\"\\nSimulated (Start): Context: {' '.join([w for w,_ in simulated_instance_start.context])}, Position: {simulated_instance_start.position}\")\n",
    "print(f\"  Features: {features_start}\")\n",
    "# Expected: {'next(work pays)': True}\n",
    "\n",
    "# Simulate an instance where 'hard' is at the very end\n",
    "simulated_instance_end = nltk.corpus.reader.senseval.SensevalInstance(\n",
    "    word='hard',\n",
    "    senses=('hard%a%01',),\n",
    "    context=[('It', 'PRP'), ('was', 'VBD'), ('very', 'RB'), ('hard', 'JJ')],\n",
    "    position=3\n",
    ")\n",
    "features_end = extract_contextual_features(simulated_instance_end, n=2)\n",
    "print(f\"\\nSimulated (End): Context: {' '.join([w for w,_ in simulated_instance_end.context])}, Position: {simulated_instance_end.position}\")\n",
    "print(f\"  Features: {features_end}\")\n",
    "# Expected: {'previous(was very)': True}\n",
    "\n",
    "# Simulate an instance with less than n words for previous/next\n",
    "simulated_instance_short_context = nltk.corpus.reader.senseval.SensevalInstance(\n",
    "    word='hard',\n",
    "    senses=('hard%a%01',),\n",
    "    context=[('A', 'DT'), ('hard', 'JJ'), ('choice', 'NN')],\n",
    "    position=1\n",
    ")\n",
    "features_short = extract_contextual_features(simulated_instance_short_context, n=2)\n",
    "print(f\"\\nSimulated (Short Context): Context: {' '.join([w for w,_ in simulated_instance_short_context.context])}, Position: {simulated_instance_short_context.position}\")\n",
    "print(f\"  Features: {features_short}\")\n",
    "# Expected: {'previous(A)': True, 'next(choice)': True}\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 391,
   "id": "8081eeed",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Distribuci√≥n de frecuencias inicial (top 10): [('.', 12), ('hard', 6), ('the', 6), ('to', 3), ('get', 3), (\"it's\", 3), ('it', 2), ('was', 2), ('a', 2), ('he', 2)]\n"
     ]
    }
   ],
   "source": [
    "all_words = []\n",
    "for instance in all_instances:\n",
    "    for item in instance.context:\n",
    "        word = ''\n",
    "        if isinstance(item, tuple):\n",
    "            word = item[0]\n",
    "        elif isinstance(item, str):\n",
    "            word = item\n",
    "        \n",
    "        # Procesar solo si la palabra no est√° vac√≠a y no es una forma gramatical de la palabra ambigua (inicialmente)\n",
    "        # La exclusi√≥n de otras palabras se har√° despu√©s de FreqDist\n",
    "        if word:\n",
    "            all_words.append(word.lower())\n",
    "\n",
    "# Calcular la distribuci√≥n de frecuencias de todas las palabras\n",
    "freq_dist = FreqDist(all_words)\n",
    "\n",
    "print(f\"\\nDistribuci√≥n de frecuencias inicial (top 10): {freq_dist.most_common(10)}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 392,
   "id": "d008f6be",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "--- Vocabulario Construido ---\n",
      "N√∫mero de palabras m√°s frecuentes (m): 6\n",
      "Vocabulario final: ['hard', 'everyone', 'involved', 'worked', 'job', 'done']\n"
     ]
    }
   ],
   "source": [
    "def construir_vocabulario_frecuente(freq_dist, words_to_exclude, m):\n",
    "  \n",
    "    filtered_vocab = []\n",
    "    \n",
    "    # Itera sobre las palabras m√°s comunes en orden descendente de frecuencia\n",
    "    for word, frequency in freq_dist.most_common():\n",
    "        if word not in words_to_exclude:\n",
    "            filtered_vocab.append(word)\n",
    "        \n",
    "        # Detener una vez que hemos alcanzado el tama√±o de vocabulario deseado (m)\n",
    "        if len(filtered_vocab) >= m:\n",
    "            break\n",
    "            \n",
    "    return filtered_vocab\n",
    "\n",
    "# Definir el tama√±o del vocabulario deseado (m)\n",
    "m_palabras = 6 # Para el ejemplo de 'hard'\n",
    "\n",
    "# Construir el vocabulario final\n",
    "vocabulario_final = construir_vocabulario_frecuente(freq_dist, WORDS_TO_EXCLUDE, m_palabras)\n",
    "\n",
    "print(f\"\\n--- Vocabulario Construido ---\")\n",
    "print(f\"N√∫mero de palabras m√°s frecuentes (m): {m_palabras}\")\n",
    "print(f\"Vocabulario final: {vocabulario_final}\")\n",
    "\n",
    "# Comprobaci√≥n con el ejemplo de 'hard'\n",
    "# Si las instancias de 'hard' contienen las palabras 'time', 'would', 'get', 'work', 'find', 'make'\n",
    "# y estas son las 6 m√°s frecuentes despu√©s del filtrado, entonces coincidir√°n.\n",
    "# Con mis datos simulados, el resultado ser√° diferente al ejemplo dado inicialmente,\n",
    "# ya que el ejemplo de 'hard' ten√≠a un vocabulario predefinido.\n",
    "# Para que coincida con ['time', 'would', 'get', 'work', 'find', 'make'],\n",
    "# esas palabras deber√≠an ser las 6 m√°s frecuentes en TU corpus real despu√©s de la limpieza."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "edc05163",
   "metadata": {},
   "source": [
    "Conjunto combinado de stopwords de NLTK (ingl√©s), puntuaci√≥n y tus palabras adicionales\n",
    " Nota: NLTK stopwords por defecto son en ingl√©s. Si tu corpus est√° en espa√±ol,\n",
    " necesitar√°s cambiar 'english' a 'spanish' en 'stopwords.words()'.\n",
    " Para este ejemplo, mantendr√© 'english' como en tu referencia, pero tenlo en cuenta.\n",
    "\n",
    "Construye un vocabulario de las 'm' palabras m√°s frecuentes del corpus,\n",
    "    excluyendo puntuaci√≥n, stopwords y formas gramaticales de las palabras ambiguas.\n",
    "\n",
    "    Args:\n",
    "        instances (list): Lista de objetos Instance, cada uno con un contexto.\n",
    "        m (int): El n√∫mero de palabras m√°s frecuentes a incluir en el vocabulario.\n",
    "        palabras_ambiguas_forms (dict): Un diccionario donde la clave es la palabra ambigua\n",
    "                                        y el valor es un conjunto de sus formas gramaticales\n",
    "                                        a excluir (ej: {'hard': {'hard', 'harder', 'hardest'}}).\n",
    "\n",
    "    Returns:\n",
    "        list: El vocabulario construido (lista de palabras)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 393,
   "id": "7eb83562",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "N√∫mero de palabras en el vocabulario deseado (m): 6\n",
      "Vocabulario construido: ['time', 'get', 'find', 'work', 'make', 'everyone']\n"
     ]
    }
   ],
   "source": [
    "# --- Definici√≥n de Stop Words y Palabras Adicionales a Eliminar ---\n",
    "# Tu conjunto de palabras a eliminar\n",
    "OTHER_WORDS = [\"''\", \"'d\", \"'ll\", \"'m\", \"'re\", \"'s\", \"'t\", \"'ve\", '--', '000', '1', '2', '3', '4', '5', '6', '8', '10', '15', '30', 'I', 'F', '``',\n",
    "               'also', \"don'\", 'n', 'one', 'said', 'say', 'says', 'u', 'us']\n",
    "\n",
    "\n",
    "STOPWORDS_SET = set(stopwords.words('english')).union(set(string.punctuation), set(OTHER_WORDS))\n",
    "\n",
    "# Clase dummy para simular las instancias del corpus\n",
    "class Instance:\n",
    "    def __init__(self, context, ambiguous_word):\n",
    "        # context puede ser una lista de palabras o de tuplas (palabra, tag)\n",
    "        self.context = context\n",
    "        self.ambiguous_word = ambiguous_word\n",
    "\n",
    "def construir_vocabulario(instances, m, palabras_ambiguas_forms):\n",
    "\n",
    "    all_words = []\n",
    "    \n",
    "    # Recopilar todas las palabras de todos los contextos\n",
    "    for instance in instances:\n",
    "        for item in instance.context:\n",
    "            word = ''\n",
    "            if isinstance(item, tuple):\n",
    "                word = item[0] # Extraer la palabra de la tupla (si hay POS tags)\n",
    "            elif isinstance(item, str):\n",
    "                word = item # La palabra ya es un string\n",
    "            \n",
    "            # Normalizar a min√∫sculas\n",
    "            word_lower = word.lower()\n",
    "            all_words.append(word_lower)\n",
    "            \n",
    "    # Usar FreqDist para calcular las frecuencias\n",
    "    fdist = nltk.FreqDist(all_words)\n",
    "    \n",
    "    # Filtrar palabras: eliminar puntuaci√≥n, stopwords y formas de palabras ambiguas\n",
    "    # Creamos un conjunto de todas las palabras a excluir\n",
    "    words_to_exclude = set()\n",
    "    words_to_exclude.update(STOPWORDS_SET)\n",
    "    \n",
    "    for amb_word_forms in palabras_ambiguas_forms.values():\n",
    "        words_to_exclude.update(amb_word_forms)\n",
    "\n",
    "    filtered_words_freq = Counter()\n",
    "    for word, freq in fdist.items():\n",
    "        if word not in words_to_exclude:\n",
    "            filtered_words_freq[word] = freq\n",
    "            \n",
    "    # Obtener las 'm' palabras m√°s frecuentes\n",
    "    vocabulario = [word for word, freq in filtered_words_freq.most_common(m)]\n",
    "    \n",
    "    return vocabulario\n",
    "\n",
    "# --- Ejemplo de Uso ---\n",
    "\n",
    "# 1. Simular datos del corpus (tus instancias de \"hard\" y \"serve\")\n",
    "# Debes reemplazar esto con tus datos reales (hard_instances, serve_instances)\n",
    "# Ejemplo de contextos\n",
    "corpus_instances = [\n",
    "    Instance(context=[\"this\", \"is\", \"a\", \"hard\", \"time\", \"for\", \"everyone\", \"and\", \"we\", \"will\", \"get\", \"through\", \"it\"], ambiguous_word=\"hard\"),\n",
    "    Instance(context=[\"it's\", \"hard\", \"to\", \"find\", \"a\", \"job\", \"these\", \"days\"], ambiguous_word=\"hard\"),\n",
    "    Instance(context=[\"the\", \"machine\", \"will\", \"serve\", \"you\", \"well\"], ambiguous_word=\"serve\"),\n",
    "    Instance(context=[\"they\", \"serve\", \"delicious\", \"food\", \"at\", \"that\", \"workplace\"], ambiguous_word=\"serve\"),\n",
    "    Instance(context=[\"a\", \"time\", \"to\", \"work\", \"harder\", \"than\", \"ever\"], ambiguous_word=\"hard\"),\n",
    "    Instance(context=[\"he\", \"would\", \"like\", \"to\", \"make\", \"a\", \"difference\"], ambiguous_word=\"would\"), # Una instancia extra para 'would'\n",
    "    Instance(context=[\"you\", \"should\", \"get\", \"up\", \"and\", \"make\", \"it\", \"happen\"], ambiguous_word=\"get\"), # Instancia para 'get', 'make'\n",
    "    Instance(context=[\"find\", \"your\", \"own\", \"way\", \"to\", \"work\", \"it\", \"out\"], ambiguous_word=\"find\") # Instancia para 'find', 'work'\n",
    "]\n",
    "\n",
    "# 2. Definir las formas gramaticales de las palabras ambiguas\n",
    "# Aqu√≠ es donde a√±adir√≠as las formas que identificaste en la Parte 1\n",
    "palabras_ambiguas_forms = {\n",
    "    'hard': {'hard', 'harder', 'hardest'},\n",
    "    'serve': {'serve', 'serves', 'served', 'serving'} # Ejemplos para 'serve'\n",
    "}\n",
    "\n",
    "# 3. Construir el vocabulario con un tama√±o 'm' (por ejemplo, m=6)\n",
    "m_size = 6\n",
    "vocab_construido = construir_vocabulario(corpus_instances, m_size, palabras_ambiguas_forms)\n",
    "\n",
    "print(f\"N√∫mero de palabras en el vocabulario deseado (m): {m_size}\")\n",
    "print(f\"Vocabulario construido: {vocab_construido}\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "87681088",
   "metadata": {},
   "source": [
    "El c√≥digo construye un vocabulario a partir de textos, filtrando palabras irrelevantes. Utiliza nltk para calcular la frecuencia de palabras y un conjunto de stopwords (que deben ser en espa√±ol), puntuaci√≥n y formas gramaticales de palabras ambiguas para excluirlas. La funci√≥n construir_vocabulario procesa las instancias, recopila todas las palabras, calcula sus frecuencias y luego filtra las no deseadas. Finalmente, selecciona las m palabras m√°s comunes y relevantes para formar el vocabulario. Este proceso es crucial para la creaci√≥n de vectores de caracter√≠sticas en el procesamiento de lenguaje natural.\n",
    "\n",
    "\n",
    "Construye un vocabulario de las 'm' palabras m√°s frecuentes del corpus,\n",
    "    excluyendo puntuaci√≥n, stopwords y formas gramaticales de las palabras ambiguas.\n",
    "    (Esta funci√≥n es la misma que la proporcionada anteriormente)\n",
    "\n",
    "\n",
    "Como se observa se generan dos instancias similares y se realiza la prueba para obtener el procedimiento finalizado\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 394,
   "id": "dc97d71c",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Vocabulario generado para m=6: ['time', 'get', 'find', 'work', 'make', 'everyone']\n",
      "---\n",
      "Instancia de ejemplo - Palabra ambigua: 'hard'\n",
      "Contexto (oraci√≥n completa): this is a hard time for everyone and we will get through it\n",
      "Vector de caracter√≠sticas para la instancia (diccionario):\n",
      "{'contains(time)': True, 'contains(get)': True, 'contains(find)': False, 'contains(work)': False, 'contains(make)': False, 'contains(everyone)': True}\n",
      "---\n",
      "--- Ejemplo espec√≠fico del enunciado ---\n",
      "Vocabulario de ejemplo del enunciado: ['time', 'would', 'get', 'work', 'find', 'make']\n",
      "Contexto del enunciado: este vector de caracter√≠sticas indica que en el contexto de la palabra ambigua aparece la palabra time y no aparecen las palabras would get work find y make\n",
      "Vector de caracter√≠sticas (si solo 'time' aparece en el contexto): {'contains(time)': True, 'contains(would)': False, 'contains(get)': False, 'contains(work)': False, 'contains(find)': False, 'contains(make)': False}\n"
     ]
    }
   ],
   "source": [
    "\n",
    "# Tu conjunto de palabras a eliminar (igual al anterior)\n",
    "OTHER_WORDS = [\"''\", \"'d\", \"'ll\", \"'m\", \"'re\", \"'s\", \"'t\", \"'ve\", '--', '000', '1', '2', '3', '4', '5', '6', '8', '10', '15', '30', 'I', 'F', '``',\n",
    "               'also', \"don'\", 'n', 'one', 'said', 'say', 'says', 'u', 'us']\n",
    "\n",
    "# Conjunto combinado de stopwords de NLTK (ingl√©s), puntuaci√≥n y tus palabras adicionales\n",
    "STOPWORDS_SET = set(stopwords.words('english')).union(set(string.punctuation), set(OTHER_WORDS))\n",
    "\n",
    "def construir_vocabulario(instances, m, palabras_ambiguas_forms):\n",
    "   \n",
    "    all_words = []\n",
    "    for instance in instances:\n",
    "        for item in instance.context:\n",
    "            word = ''\n",
    "            if isinstance(item, tuple):\n",
    "                word = item[0]\n",
    "            elif isinstance(item, str):\n",
    "                word = item\n",
    "            word_lower = word.lower()\n",
    "            all_words.append(word_lower)\n",
    "            \n",
    "    fdist = nltk.FreqDist(all_words)\n",
    "    \n",
    "    words_to_exclude = set()\n",
    "    words_to_exclude.update(STOPWORDS_SET)\n",
    "    \n",
    "    for amb_word_forms in palabras_ambiguas_forms.values():\n",
    "        words_to_exclude.update(amb_word_forms)\n",
    "\n",
    "    filtered_words_freq = Counter()\n",
    "    for word, freq in fdist.items():\n",
    "        if word not in words_to_exclude:\n",
    "            filtered_words_freq[word] = freq\n",
    "            \n",
    "    vocabulario = [word for word, freq in filtered_words_freq.most_common(m)]\n",
    "    \n",
    "    return vocabulario\n",
    "\n",
    "# --- Nueva funci√≥n para extraer el vector de caracter√≠sticas como diccionario ---\n",
    "\n",
    "def extraer_caracteristicas_diccionario(instance_context, vocabulario):\n",
    "    vector_caracteristicas = {}\n",
    "    contexto_palabras_lower = []\n",
    "\n",
    "    # Normalizar el contexto a solo palabras en min√∫sculas\n",
    "    for item in instance_context:\n",
    "        word_to_add = ''\n",
    "        if isinstance(item, tuple):\n",
    "            word_to_add = item[0]\n",
    "        elif isinstance(item, str):\n",
    "            word_to_add = item\n",
    "        \n",
    "        if word_to_add:\n",
    "            contexto_palabras_lower.append(word_to_add.lower())\n",
    "\n",
    "    # Construir el diccionario de caracter√≠sticas\n",
    "    for palabra_vocabulario in vocabulario:\n",
    "        clave = f'contains({palabra_vocabulario})'\n",
    "        if palabra_vocabulario.lower() in contexto_palabras_lower:\n",
    "            vector_caracteristicas[clave] = True\n",
    "        else:\n",
    "            vector_caracteristicas[clave] = False\n",
    "            \n",
    "    return vector_caracteristicas\n",
    "\n",
    "# --- Ejemplo Completo ---\n",
    "\n",
    "# 1. Simular datos del corpus (usando los mismos datos de ejemplo que antes)\n",
    "corpus_instances = [\n",
    "    Instance(context=[\"this\", \"is\", \"a\", \"hard\", \"time\", \"for\", \"everyone\", \"and\", \"we\", \"will\", \"get\", \"through\", \"it\"], ambiguous_word=\"hard\"),\n",
    "    Instance(context=[\"it's\", \"hard\", \"to\", \"find\", \"a\", \"job\", \"these\", \"days\"], ambiguous_word=\"hard\"),\n",
    "    Instance(context=[\"the\", \"machine\", \"will\", \"serve\", \"you\", \"well\"], ambiguous_word=\"serve\"),\n",
    "    Instance(context=[\"they\", \"serve\", \"delicious\", \"food\", \"at\", \"that\", \"workplace\"], ambiguous_word=\"serve\"),\n",
    "    Instance(context=[\"a\", \"time\", \"to\", \"work\", \"harder\", \"than\", \"ever\"], ambiguous_word=\"hard\"),\n",
    "    Instance(context=[\"he\", \"would\", \"like\", \"to\", \"make\", \"a\", \"difference\"], ambiguous_word=\"would\"),\n",
    "    Instance(context=[\"you\", \"should\", \"get\", \"up\", \"and\", \"make\", \"it\", \"happen\"], ambiguous_word=\"get\"),\n",
    "    Instance(context=[\"find\", \"your\", \"own\", \"way\", \"to\", \"work\", \"it\", \"out\"], ambiguous_word=\"find\")\n",
    "]\n",
    "\n",
    "# 2. Definir las formas gramaticales de las palabras ambiguas\n",
    "palabras_ambiguas_forms = {\n",
    "    'hard': {'hard', 'harder', 'hardest'},\n",
    "    'serve': {'serve', 'serves', 'served', 'serving'}\n",
    "}\n",
    "\n",
    "# 3. Construir el vocabulario (ejemplo con m=6)\n",
    "m_size = 6\n",
    "vocabulario_generado = construir_vocabulario(corpus_instances, m_size, palabras_ambiguas_forms)\n",
    "\n",
    "print(f\"Vocabulario generado para m={m_size}: {vocabulario_generado}\")\n",
    "print(\"---\")\n",
    "\n",
    "# 4. Mostrar el vector de caracter√≠sticas para una instancia del corpus\n",
    "# Tomemos la primera instancia como ejemplo:\n",
    "# \"this is a hard time for everyone and we will get through it\"\n",
    "\n",
    "instancia_ejemplo = corpus_instances[0]\n",
    "print(f\"Instancia de ejemplo - Palabra ambigua: '{instancia_ejemplo.ambiguous_word}'\")\n",
    "print(f\"Contexto (oraci√≥n completa): {' '.join(instancia_ejemplo.context)}\")\n",
    "\n",
    "# Extraer el vector de caracter√≠sticas para esta instancia\n",
    "vector_caracteristicas_instancia = extraer_caracteristicas_diccionario(instancia_ejemplo.context, vocabulario_generado)\n",
    "\n",
    "print(f\"Vector de caracter√≠sticas para la instancia (diccionario):\\n{vector_caracteristicas_instancia}\")\n",
    "\n",
    "print(\"---\")\n",
    "\n",
    "# Para el ejemplo solicitado: {'contains(time)': True, 'contains(would)': False, ...}\n",
    "# Usaremos un vocabulario fijo para ilustrar ese caso espec√≠fico.\n",
    "vocabulario_fijo_ejemplo = ['time', 'would', 'get', 'work', 'find', 'make']\n",
    "contexto_ejemplo_solicitado = [\"este\", \"vector\", \"de\", \"caracter√≠sticas\", \"indica\", \"que\", \"en\", \"el\", \"contexto\", \"de\", \"la\", \"palabra\",\n",
    "                               \"ambigua\", \"aparece\", \"la\", \"palabra\", \"time\", \"y\", \"no\", \"aparecen\", \"las\", \"palabras\",\n",
    "                               \"would\", \"get\", \"work\", \"find\", \"y\", \"make\"] # Aqu√≠, todas aparecen en el ejemplo de texto.\n",
    "\n",
    "print(\"--- Ejemplo espec√≠fico del enunciado ---\")\n",
    "print(f\"Vocabulario de ejemplo del enunciado: {vocabulario_fijo_ejemplo}\")\n",
    "print(f\"Contexto del enunciado: {' '.join(contexto_ejemplo_solicitado)}\")\n",
    "\n",
    "# Si el contexto del ejemplo del enunciado solo contuviera 'time' y NO el resto:\n",
    "contexto_solo_time = [\"este\", \"vector\", \"de\", \"caracter√≠sticas\", \"indica\", \"que\", \"en\", \"el\", \"contexto\", \"de\", \"la\", \"palabra\",\n",
    "                      \"ambigua\", \"aparece\", \"la\", \"palabra\", \"time\"]\n",
    "\n",
    "vector_caracteristicas_solicitado = extraer_caracteristicas_diccionario(contexto_solo_time, vocabulario_fijo_ejemplo)\n",
    "print(f\"Vector de caracter√≠sticas (si solo 'time' aparece en el contexto): {vector_caracteristicas_solicitado}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 395,
   "id": "fbe8b067",
   "metadata": {},
   "outputs": [],
   "source": [
    "def calcular_probabilidades_emision(corpus):\n",
    "    emision = {}\n",
    "    total_etiquetas = {}\n",
    "    for token, etiqueta in corpus:\n",
    "        if etiqueta not in emision:\n",
    "            emision[etiqueta] = {}\n",
    "            total_etiquetas[etiqueta] = 0\n",
    "        emision[etiqueta][token] = emision[etiqueta].get(token, 0) + 1\n",
    "        total_etiquetas[etiqueta] += 1\n",
    "    for etiqueta in emision:\n",
    "        for token in emision[etiqueta]:\n",
    "            emision[etiqueta][token] /= total_etiquetas[etiqueta]\n",
    "    return emision\n",
    "\n",
    "def calcular_probabilidades_transicion(corpus):\n",
    "    transicion = {}\n",
    "    total_transiciones = {}\n",
    "    anterior = None\n",
    "    for _, etiqueta in corpus:\n",
    "        if anterior is not None:\n",
    "            if anterior not in transicion:\n",
    "                transicion[anterior] = {}\n",
    "                total_transiciones[anterior] = 0\n",
    "            transicion[anterior][etiqueta] = transicion[anterior].get(etiqueta, 0) + 1\n",
    "            total_transiciones[anterior] += 1\n",
    "        anterior = etiqueta\n",
    "    for etiqueta in transicion:\n",
    "        for siguiente in transicion[etiqueta]:\n",
    "            transicion[etiqueta][siguiente] /= total_transiciones[etiqueta]\n",
    "    return transicion\n",
    "\n",
    "def calcular_probabilidades_iniciales(corpus):\n",
    "    iniciales = {}\n",
    "    total = 0\n",
    "    if corpus:\n",
    "        iniciales[corpus[0][1]] = 1\n",
    "        total = 1\n",
    "    for etiqueta in iniciales:\n",
    "        iniciales[etiqueta] /= total\n",
    "    return iniciales\n",
    "\n",
    "emision = calcular_probabilidades_emision(corpus)\n",
    "transicion = calcular_probabilidades_transicion(corpus)\n",
    "estado_inicial = calcular_probabilidades_iniciales(corpus)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 396,
   "id": "cbf97199",
   "metadata": {},
   "outputs": [],
   "source": [
    "def viterbi(frase, estados, transicion, emision, estado_inicial):\n",
    "    V = [{}]\n",
    "    path = {}\n",
    "\n",
    "    for estado in estados:\n",
    "        V[0][estado] = estado_inicial.get(estado, 1e-6) * emision.get(estado, {}).get(frase[0], 1e-6)\n",
    "        path[estado] = [estado]\n",
    "\n",
    "    for t in range(1, len(frase)):\n",
    "        V.append({})\n",
    "        new_path = {}\n",
    "        for y in estados:\n",
    "            (prob, estado_max) = max(\n",
    "                [(V[t-1][y0] * transicion.get(y0, {}).get(y, 1e-6) * emision.get(y, {}).get(frase[t], 1e-6), y0)\n",
    "                 for y0 in estados], key=lambda x: x[0])\n",
    "            V[t][y] = prob\n",
    "            new_path[y] = path[estado_max] + [y]\n",
    "        path = new_path\n",
    "\n",
    "    n = len(frase) - 1\n",
    "    (prob, estado_max) = max([(V[n][y], y) for y in estados], key=lambda x: x[0])\n",
    "    return (prob, path[estado_max])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 397,
   "id": "16dc22b6",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Frase: ['Habla', 'con', 'el', 'enfermo', 'grave', 'de', 'trasplantes', '.']\n",
      "Ruta m√°s probable: ['VMIP3S0', 'SP', 'DA', 'NCMS000', 'AQ0CS00', 'SP', 'NCMN000', 'Fp']\n",
      "Probabilidad total: 1.8533917043090838e-08\n"
     ]
    }
   ],
   "source": [
    "frase_prueba = ['Habla', 'con', 'el', 'enfermo', 'grave', 'de', 'trasplantes', '.']\n",
    "estados = list(set([etiqueta for _, etiqueta in corpus]))\n",
    "\n",
    "probabilidad, ruta = viterbi(frase_prueba, estados, transicion, emision, estado_inicial)\n",
    "print(\"Frase:\", frase_prueba)\n",
    "print(\"Ruta m√°s probable:\", ruta)\n",
    "print(\"Probabilidad total:\", probabilidad)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 398,
   "id": "1e2dd194",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "DataFrame de Emisi√≥n:\n",
      "             Habla     habla     corre     canta    brilla    avanza  \\\n",
      "VMIP3S0   0.142857  0.142857  0.142857  0.142857  0.142857  0.142857   \n",
      "SP        0.000000  0.000000  0.000000  0.000000  0.000000  0.000000   \n",
      "DA        0.000000  0.000000  0.000000  0.000000  0.000000  0.000000   \n",
      "NCMS000   0.000000  0.000000  0.000000  0.000000  0.000000  0.000000   \n",
      "AQ0CS00   0.000000  0.000000  0.000000  0.000000  0.000000  0.000000   \n",
      "NCMN000   0.000000  0.000000  0.000000  0.000000  0.000000  0.000000   \n",
      "Fp        0.000000  0.000000  0.000000  0.000000  0.000000  0.000000   \n",
      "NCFS000   0.000000  0.000000  0.000000  0.000000  0.000000  0.000000   \n",
      "VAIP3S0   0.000000  0.000000  0.000000  0.000000  0.000000  0.000000   \n",
      "VMP00SF   0.000000  0.000000  0.000000  0.000000  0.000000  0.000000   \n",
      "SP+DA     0.000000  0.000000  0.000000  0.000000  0.000000  0.000000   \n",
      "NP00000   0.000000  0.000000  0.000000  0.000000  0.000000  0.000000   \n",
      "VSIP3S0   0.000000  0.000000  0.000000  0.000000  0.000000  0.000000   \n",
      "AQ0MS00   0.000000  0.000000  0.000000  0.000000  0.000000  0.000000   \n",
      "DI        0.000000  0.000000  0.000000  0.000000  0.000000  0.000000   \n",
      "AQ0FS00   0.000000  0.000000  0.000000  0.000000  0.000000  0.000000   \n",
      "VIIIS0S   0.000000  0.000000  0.000000  0.000000  0.000000  0.000000   \n",
      "PP3FS000  0.000000  0.000000  0.000000  0.000000  0.000000  0.000000   \n",
      "PP3MP000  0.000000  0.000000  0.000000  0.000000  0.000000  0.000000   \n",
      "VMIP3P0   0.000000  0.000000  0.000000  0.000000  0.000000  0.000000   \n",
      "RG        0.000000  0.000000  0.000000  0.000000  0.000000  0.000000   \n",
      "PP1MP000  0.000000  0.000000  0.000000  0.000000  0.000000  0.000000   \n",
      "VMIP1P0   0.000000  0.000000  0.000000  0.000000  0.000000  0.000000   \n",
      "VIIIS3S   0.000000  0.000000  0.000000  0.000000  0.000000  0.000000   \n",
      "VMIS3S0   0.000000  0.000000  0.000000  0.000000  0.000000  0.000000   \n",
      "PP3FP000  0.000000  0.000000  0.000000  0.000000  0.000000  0.000000   \n",
      "NCMP000   0.000000  0.000000  0.000000  0.000000  0.000000  0.000000   \n",
      "AQ0MP00   0.000000  0.000000  0.000000  0.000000  0.000000  0.000000   \n",
      "CC        0.000000  0.000000  0.000000  0.000000  0.000000  0.000000   \n",
      "\n",
      "             pinta       con        de        en  ...  miraba  cocinaba  \\\n",
      "VMIP3S0   0.142857  0.000000  0.000000  0.000000  ...     0.0       0.0   \n",
      "SP        0.000000  0.090909  0.272727  0.363636  ...     0.0       0.0   \n",
      "DA        0.000000  0.000000  0.000000  0.000000  ...     0.0       0.0   \n",
      "NCMS000   0.000000  0.000000  0.000000  0.000000  ...     0.0       0.0   \n",
      "AQ0CS00   0.000000  0.000000  0.000000  0.000000  ...     0.0       0.0   \n",
      "NCMN000   0.000000  0.000000  0.000000  0.000000  ...     0.0       0.0   \n",
      "Fp        0.000000  0.000000  0.000000  0.000000  ...     0.0       0.0   \n",
      "NCFS000   0.000000  0.000000  0.000000  0.000000  ...     0.0       0.0   \n",
      "VAIP3S0   0.000000  0.000000  0.000000  0.000000  ...     0.0       0.0   \n",
      "VMP00SF   0.000000  0.000000  0.000000  0.000000  ...     0.0       0.0   \n",
      "SP+DA     0.000000  0.000000  0.000000  0.000000  ...     0.0       0.0   \n",
      "NP00000   0.000000  0.000000  0.000000  0.000000  ...     0.0       0.0   \n",
      "VSIP3S0   0.000000  0.000000  0.000000  0.000000  ...     0.0       0.0   \n",
      "AQ0MS00   0.000000  0.000000  0.000000  0.000000  ...     0.0       0.0   \n",
      "DI        0.000000  0.000000  0.000000  0.000000  ...     0.0       0.0   \n",
      "AQ0FS00   0.000000  0.000000  0.000000  0.000000  ...     0.0       0.0   \n",
      "VIIIS0S   0.000000  0.000000  0.000000  0.000000  ...     0.0       0.0   \n",
      "PP3FS000  0.000000  0.000000  0.000000  0.000000  ...     0.0       0.0   \n",
      "PP3MP000  0.000000  0.000000  0.000000  0.000000  ...     0.0       0.0   \n",
      "VMIP3P0   0.000000  0.000000  0.000000  0.000000  ...     0.0       0.0   \n",
      "RG        0.000000  0.000000  0.000000  0.000000  ...     0.0       0.0   \n",
      "PP1MP000  0.000000  0.000000  0.000000  0.000000  ...     0.0       0.0   \n",
      "VMIP1P0   0.000000  0.000000  0.000000  0.000000  ...     0.0       0.0   \n",
      "VIIIS3S   0.000000  0.000000  0.000000  0.000000  ...     0.5       0.5   \n",
      "VMIS3S0   0.000000  0.000000  0.000000  0.000000  ...     0.0       0.0   \n",
      "PP3FP000  0.000000  0.000000  0.000000  0.000000  ...     0.0       0.0   \n",
      "NCMP000   0.000000  0.000000  0.000000  0.000000  ...     0.0       0.0   \n",
      "AQ0MP00   0.000000  0.000000  0.000000  0.000000  ...     0.0       0.0   \n",
      "CC        0.000000  0.000000  0.000000  0.000000  ...     0.0       0.0   \n",
      "\n",
      "          explic√≥  llovi√≥  Ellas  cuadros  pa√≠ses  hermosos  diferentes    y  \n",
      "VMIP3S0       0.0     0.0    0.0      0.0     0.0       0.0         0.0  0.0  \n",
      "SP            0.0     0.0    0.0      0.0     0.0       0.0         0.0  0.0  \n",
      "DA            0.0     0.0    0.0      0.0     0.0       0.0         0.0  0.0  \n",
      "NCMS000       0.0     0.0    0.0      0.0     0.0       0.0         0.0  0.0  \n",
      "AQ0CS00       0.0     0.0    0.0      0.0     0.0       0.0         0.0  0.0  \n",
      "NCMN000       0.0     0.0    0.0      0.0     0.0       0.0         0.0  0.0  \n",
      "Fp            0.0     0.0    0.0      0.0     0.0       0.0         0.0  0.0  \n",
      "NCFS000       0.0     0.0    0.0      0.0     0.0       0.0         0.0  0.0  \n",
      "VAIP3S0       0.0     0.0    0.0      0.0     0.0       0.0         0.0  0.0  \n",
      "VMP00SF       0.0     0.0    0.0      0.0     0.0       0.0         0.0  0.0  \n",
      "SP+DA         0.0     0.0    0.0      0.0     0.0       0.0         0.0  0.0  \n",
      "NP00000       0.0     0.0    0.0      0.0     0.0       0.0         0.0  0.0  \n",
      "VSIP3S0       0.0     0.0    0.0      0.0     0.0       0.0         0.0  0.0  \n",
      "AQ0MS00       0.0     0.0    0.0      0.0     0.0       0.0         0.0  0.0  \n",
      "DI            0.0     0.0    0.0      0.0     0.0       0.0         0.0  0.0  \n",
      "AQ0FS00       0.0     0.0    0.0      0.0     0.0       0.0         0.0  0.0  \n",
      "VIIIS0S       0.0     0.0    0.0      0.0     0.0       0.0         0.0  0.0  \n",
      "PP3FS000      0.0     0.0    0.0      0.0     0.0       0.0         0.0  0.0  \n",
      "PP3MP000      0.0     0.0    0.0      0.0     0.0       0.0         0.0  0.0  \n",
      "VMIP3P0       0.0     0.0    0.0      0.0     0.0       0.0         0.0  0.0  \n",
      "RG            0.0     0.0    0.0      0.0     0.0       0.0         0.0  0.0  \n",
      "PP1MP000      0.0     0.0    0.0      0.0     0.0       0.0         0.0  0.0  \n",
      "VMIP1P0       0.0     0.0    0.0      0.0     0.0       0.0         0.0  0.0  \n",
      "VIIIS3S       0.0     0.0    0.0      0.0     0.0       0.0         0.0  0.0  \n",
      "VMIS3S0       0.5     0.5    0.0      0.0     0.0       0.0         0.0  0.0  \n",
      "PP3FP000      0.0     0.0    1.0      0.0     0.0       0.0         0.0  0.0  \n",
      "NCMP000       0.0     0.0    0.0      0.5     0.5       0.0         0.0  0.0  \n",
      "AQ0MP00       0.0     0.0    0.0      0.0     0.0       0.5         0.5  0.0  \n",
      "CC            0.0     0.0    0.0      0.0     0.0       0.0         0.0  1.0  \n",
      "\n",
      "[29 rows x 91 columns]\n",
      "\n",
      "==================================================\n",
      "\n"
     ]
    }
   ],
   "source": [
    "import pandas as pd\n",
    "\n",
    "# (Assuming 'emision', 'transicion', 'frase_prueba', and 'ruta' are defined as above or loaded elsewhere)\n",
    "\n",
    "# Create DataFrame for Emisi√≥n (Emission Probabilities)\n",
    "# Rows will be words, columns will be tags\n",
    "df_emision = pd.DataFrame.from_dict(emision, orient='index').fillna(0)\n",
    "print(\"DataFrame de Emisi√≥n:\")\n",
    "print(df_emision)\n",
    "print(\"\\n\" + \"=\"*50 + \"\\n\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 399,
   "id": "1fd08cd0",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "DataFrame de Transici√≥n:\n",
      "                SP        DI        RG   NCMP000        DA   NCMN000  \\\n",
      "VMIP3S0   0.428571  0.142857  0.285714  0.142857  0.000000  0.000000   \n",
      "NCMS000   0.105263  0.052632  0.000000  0.000000  0.000000  0.000000   \n",
      "AQ0CS00   0.142857  0.000000  0.000000  0.000000  0.000000  0.000000   \n",
      "VMIP3P0   0.333333  0.000000  0.000000  0.000000  0.000000  0.000000   \n",
      "RG        0.750000  0.000000  0.000000  0.000000  0.000000  0.000000   \n",
      "VIIIS3S   0.500000  0.500000  0.000000  0.000000  0.000000  0.000000   \n",
      "Fp        0.000000  0.157895  0.052632  0.000000  0.315789  0.000000   \n",
      "VSIP3S0   0.000000  0.333333  0.000000  0.000000  0.000000  0.000000   \n",
      "VMIS3S0   0.000000  0.000000  0.500000  0.000000  0.500000  0.000000   \n",
      "AQ0MP00   0.000000  0.000000  0.000000  0.500000  0.000000  0.000000   \n",
      "SP        0.000000  0.000000  0.000000  0.000000  0.636364  0.181818   \n",
      "VIIIS0S   0.000000  0.000000  0.000000  0.000000  1.000000  0.000000   \n",
      "DA        0.000000  0.000000  0.000000  0.000000  0.000000  0.000000   \n",
      "SP+DA     0.000000  0.000000  0.000000  0.000000  0.000000  0.000000   \n",
      "DI        0.000000  0.000000  0.000000  0.000000  0.000000  0.000000   \n",
      "VMIP1P0   0.000000  0.000000  0.000000  0.000000  0.000000  0.000000   \n",
      "NCMP000   0.000000  0.000000  0.000000  0.000000  0.000000  0.000000   \n",
      "NCFS000   0.000000  0.000000  0.000000  0.000000  0.000000  0.000000   \n",
      "CC        0.000000  0.000000  0.000000  0.000000  0.000000  0.000000   \n",
      "AQ0MS00   0.000000  0.000000  0.000000  0.000000  0.000000  0.000000   \n",
      "PP3FS000  0.000000  0.000000  0.000000  0.000000  0.000000  0.000000   \n",
      "NCMN000   0.000000  0.000000  0.000000  0.000000  0.000000  0.000000   \n",
      "NP00000   0.000000  0.000000  0.000000  0.000000  0.000000  0.000000   \n",
      "VMP00SF   0.000000  0.000000  0.000000  0.000000  0.000000  0.000000   \n",
      "VAIP3S0   0.000000  0.000000  0.000000  0.000000  0.000000  0.000000   \n",
      "AQ0FS00   0.000000  0.000000  0.000000  0.000000  0.000000  0.000000   \n",
      "PP3MP000  0.000000  0.000000  0.000000  0.000000  0.000000  0.000000   \n",
      "PP3FP000  0.000000  0.000000  0.000000  0.000000  0.000000  0.000000   \n",
      "PP1MP000  0.000000  0.000000  0.000000  0.000000  0.000000  0.000000   \n",
      "\n",
      "           NCMS000   AQ0MP00   NCFS000   AQ0CS00  ...  PP3MP000  PP1MP000  \\\n",
      "VMIP3S0   0.000000  0.000000  0.000000  0.000000  ...  0.000000  0.000000   \n",
      "NCMS000   0.000000  0.000000  0.000000  0.263158  ...  0.000000  0.000000   \n",
      "AQ0CS00   0.000000  0.000000  0.000000  0.000000  ...  0.000000  0.000000   \n",
      "VMIP3P0   0.666667  0.000000  0.000000  0.000000  ...  0.000000  0.000000   \n",
      "RG        0.000000  0.000000  0.000000  0.000000  ...  0.000000  0.000000   \n",
      "VIIIS3S   0.000000  0.000000  0.000000  0.000000  ...  0.000000  0.000000   \n",
      "Fp        0.052632  0.000000  0.000000  0.000000  ...  0.105263  0.052632   \n",
      "VSIP3S0   0.333333  0.000000  0.000000  0.000000  ...  0.000000  0.000000   \n",
      "VMIS3S0   0.000000  0.000000  0.000000  0.000000  ...  0.000000  0.000000   \n",
      "AQ0MP00   0.000000  0.000000  0.000000  0.000000  ...  0.000000  0.000000   \n",
      "SP        0.090909  0.090909  0.000000  0.000000  ...  0.000000  0.000000   \n",
      "VIIIS0S   0.000000  0.000000  0.000000  0.000000  ...  0.000000  0.000000   \n",
      "DA        0.466667  0.000000  0.533333  0.000000  ...  0.000000  0.000000   \n",
      "SP+DA     0.500000  0.000000  0.000000  0.000000  ...  0.000000  0.000000   \n",
      "DI        0.714286  0.000000  0.285714  0.000000  ...  0.000000  0.000000   \n",
      "VMIP1P0   1.000000  0.000000  0.000000  0.000000  ...  0.000000  0.000000   \n",
      "NCMP000   0.000000  0.500000  0.000000  0.000000  ...  0.000000  0.000000   \n",
      "NCFS000   0.000000  0.000000  0.000000  0.100000  ...  0.000000  0.000000   \n",
      "CC        0.000000  0.000000  0.000000  1.000000  ...  0.000000  0.000000   \n",
      "AQ0MS00   0.000000  0.000000  0.000000  0.000000  ...  0.000000  0.000000   \n",
      "PP3FS000  0.000000  0.000000  0.000000  0.000000  ...  0.000000  0.000000   \n",
      "NCMN000   0.000000  0.000000  0.000000  0.000000  ...  0.000000  0.000000   \n",
      "NP00000   0.000000  0.000000  0.000000  0.000000  ...  0.000000  0.000000   \n",
      "VMP00SF   0.000000  0.000000  0.000000  0.000000  ...  0.000000  0.000000   \n",
      "VAIP3S0   0.000000  0.000000  0.000000  0.000000  ...  0.000000  0.000000   \n",
      "AQ0FS00   0.000000  0.000000  0.000000  0.000000  ...  0.000000  0.000000   \n",
      "PP3MP000  0.000000  0.000000  0.000000  0.000000  ...  0.000000  0.000000   \n",
      "PP3FP000  0.000000  0.000000  0.000000  0.000000  ...  0.000000  0.000000   \n",
      "PP1MP000  0.000000  0.000000  0.000000  0.000000  ...  0.000000  0.000000   \n",
      "\n",
      "          PP3FP000  VAIP3S0   AQ0FS00  VMP00SF  VIIIS0S   CC  VMIP3P0  VMIP1P0  \n",
      "VMIP3S0   0.000000      0.0  0.000000      0.0      0.0  0.0      0.0      0.0  \n",
      "NCMS000   0.000000      0.0  0.000000      0.0      0.0  0.0      0.0      0.0  \n",
      "AQ0CS00   0.000000      0.0  0.000000      0.0      0.0  0.0      0.0      0.0  \n",
      "VMIP3P0   0.000000      0.0  0.000000      0.0      0.0  0.0      0.0      0.0  \n",
      "RG        0.000000      0.0  0.000000      0.0      0.0  0.0      0.0      0.0  \n",
      "VIIIS3S   0.000000      0.0  0.000000      0.0      0.0  0.0      0.0      0.0  \n",
      "Fp        0.052632      0.0  0.000000      0.0      0.0  0.0      0.0      0.0  \n",
      "VSIP3S0   0.000000      0.0  0.333333      0.0      0.0  0.0      0.0      0.0  \n",
      "VMIS3S0   0.000000      0.0  0.000000      0.0      0.0  0.0      0.0      0.0  \n",
      "AQ0MP00   0.000000      0.0  0.000000      0.0      0.0  0.0      0.0      0.0  \n",
      "SP        0.000000      0.0  0.000000      0.0      0.0  0.0      0.0      0.0  \n",
      "VIIIS0S   0.000000      0.0  0.000000      0.0      0.0  0.0      0.0      0.0  \n",
      "DA        0.000000      0.0  0.000000      0.0      0.0  0.0      0.0      0.0  \n",
      "SP+DA     0.000000      0.0  0.000000      0.0      0.0  0.0      0.0      0.0  \n",
      "DI        0.000000      0.0  0.000000      0.0      0.0  0.0      0.0      0.0  \n",
      "VMIP1P0   0.000000      0.0  0.000000      0.0      0.0  0.0      0.0      0.0  \n",
      "NCMP000   0.000000      0.0  0.000000      0.0      0.0  0.0      0.0      0.0  \n",
      "NCFS000   0.000000      0.1  0.100000      0.0      0.0  0.0      0.0      0.0  \n",
      "CC        0.000000      0.0  0.000000      0.0      0.0  0.0      0.0      0.0  \n",
      "AQ0MS00   0.000000      0.0  0.000000      0.0      0.0  0.0      0.0      0.0  \n",
      "PP3FS000  0.000000      0.0  0.000000      0.0      0.0  0.0      0.0      0.0  \n",
      "NCMN000   0.000000      0.0  0.000000      0.0      0.0  0.0      0.0      0.0  \n",
      "NP00000   0.000000      0.0  0.000000      0.0      0.0  0.0      0.0      0.0  \n",
      "VMP00SF   0.000000      0.0  0.000000      0.0      0.0  0.0      0.0      0.0  \n",
      "VAIP3S0   0.000000      0.0  0.000000      1.0      0.0  0.0      0.0      0.0  \n",
      "AQ0FS00   0.000000      0.0  0.000000      0.0      0.5  0.5      0.0      0.0  \n",
      "PP3MP000  0.000000      0.0  0.000000      0.0      0.0  0.0      1.0      0.0  \n",
      "PP3FP000  0.000000      0.0  0.000000      0.0      0.0  0.0      1.0      0.0  \n",
      "PP1MP000  0.000000      0.0  0.000000      0.0      0.0  0.0      0.0      1.0  \n",
      "\n",
      "[29 rows x 29 columns]\n",
      "\n",
      "==================================================\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# Create DataFrame for Transici√≥n (Transition Probabilities)\n",
    "# Rows will be current tags, columns will be next tags\n",
    "df_transicion = pd.DataFrame.from_dict(transicion, orient='index').fillna(0)\n",
    "print(\"DataFrame de Transici√≥n:\")\n",
    "print(df_transicion)\n",
    "print(\"\\n\" + \"=\"*50 + \"\\n\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 400,
   "id": "7ef67f22",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "DataFrame Viterbi (Palabra y Etiqueta):\n",
      "       Palabra Etiqueta\n",
      "0        Habla  VMIP3S0\n",
      "1          con       SP\n",
      "2           el       DA\n",
      "3      enfermo  NCMS000\n",
      "4        grave  AQ0CS00\n",
      "5           de       SP\n",
      "6  trasplantes  NCMN000\n",
      "7            .       Fp\n"
     ]
    }
   ],
   "source": [
    "\n",
    "# Create DataFrame for Viterbi result\n",
    "# Shows the input phrase words and their predicted tags\n",
    "df_viterbi = pd.DataFrame({\n",
    "    \"Palabra\": frase_prueba,\n",
    "    \"Etiqueta\": ruta\n",
    "})\n",
    "print(\"DataFrame Viterbi (Palabra y Etiqueta):\")\n",
    "print(df_viterbi)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 401,
   "id": "be605a77",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<a href='tabla_emision.xlsx' target='_blank'>tabla_emision.xlsx</a><br>"
      ],
      "text/plain": [
       "i:\\unirIA\\procesamientoLenguaje\\senseval\\tabla_emision.xlsx"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "<a href='tabla_transicion.xlsx' target='_blank'>tabla_transicion.xlsx</a><br>"
      ],
      "text/plain": [
       "i:\\unirIA\\procesamientoLenguaje\\senseval\\tabla_transicion.xlsx"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "<a href='ruta_viterbi.xlsx' target='_blank'>ruta_viterbi.xlsx</a><br>"
      ],
      "text/plain": [
       "i:\\unirIA\\procesamientoLenguaje\\senseval\\ruta_viterbi.xlsx"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "df_emision.to_excel(\"tabla_emision.xlsx\", engine='openpyxl')\n",
    "df_transicion.to_excel(\"tabla_transicion.xlsx\", engine='openpyxl')\n",
    "df_viterbi.to_excel(\"ruta_viterbi.xlsx\", engine='openpyxl')\n",
    "\n",
    "from IPython.display import FileLink, display\n",
    "display(FileLink(\"tabla_emision.xlsx\"))\n",
    "display(FileLink(\"tabla_transicion.xlsx\"))\n",
    "display(FileLink(\"ruta_viterbi.xlsx\"))"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
